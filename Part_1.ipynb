{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87ecbbee",
      "metadata": {
        "id": "87ecbbee"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aff9e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aff9e45",
        "outputId": "0aff7736-4802-49f8-ad37-7f867f0733b1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8bff70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8bff70a",
        "outputId": "d1546ec3-aad5-4e32-c462-3a8ddd2b6701",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyldavis in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.10.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyldavis) (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pyldavis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyldavis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyldavis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pyldavis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyldavis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pyldavis) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyldavis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfaf826b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaf826b",
        "outputId": "8bbab88f-069f-4143-f7f1-7aaa3714866f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824d1d4a",
      "metadata": {
        "id": "824d1d4a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import multiprocessing\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f6d4c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0f6d4c3",
        "outputId": "dab2364a-3809-4499-821b-ec546e39a8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107b5c68",
      "metadata": {
        "id": "107b5c68"
      },
      "outputs": [],
      "source": [
        "# Load spaCy for lemmatization\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79314931",
      "metadata": {
        "id": "79314931"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c360eddf",
      "metadata": {
        "id": "c360eddf"
      },
      "outputs": [],
      "source": [
        "pd.set_option('max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9c9e4c",
      "metadata": {
        "id": "9c9c9e4c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/vjavaly/Baruch-CIS-9665/refs/heads/main/data/NIPS_papers_1500.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfe6caa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dfe6caa",
        "outputId": "07d6c834-e826-412d-b550-1aea569458a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553029d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "553029d9",
        "outputId": "6fb67871-8f77-4e3f-d7bf-a911aee75878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   year  \\\n",
              "0  1997   \n",
              "1  2007   \n",
              "2  2017   \n",
              "\n",
              "                                                                                                  title  \\\n",
              "0  Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings   \n",
              "1                       Near-Maximum Entropy Models for Binary Neural Representations of Natural Images   \n",
              "2                     Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions   \n",
              "\n",
              "  event_type  \\\n",
              "0        NaN   \n",
              "1        NaN   \n",
              "2     Poster   \n",
              "\n",
              "                                                                                                        pdf_name  \\\n",
              "0  1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf   \n",
              "1                       3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf   \n",
              "2                        6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Abstract Missing   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Abstract Missing   \n",
              "2  We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 paper_text  \n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Independent Component Analysis for\\nidentification of artifacts in\\nMagnetoencephalographic recordings\\n\\nRicardo Vigario 1 ; Veikko J ousmiiki2 ,\\nMatti Hiimiiliiinen2, Riitta Hari2, and Erkki Oja 1\\n1 Lab.\\n\\nof Computer & Info. Science\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{Ricardo.Vigario, Erkki.Oja}@hut.fi\\n2 Brain\\n\\nResearch Unit, Low Temperature Lab.\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{veikko, msh, hari}@neuro.hut.fi\\n\\nAbstract\\nWe have studied the application of an independent component analysis\\n(ICA) approach to the identification and possible removal of artifacts\\nfrom a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude\\ndistributions over time, thus distinguishing between strictly periodical\\nsignals, and regularly and irregularly occurring signals. Many artifacts\\nbelong to the last category. In order to assess the effectiveness of the\\nmethod, controlled artifacts were produced, which included saccadic eye\\nmovements and blinks, increased muscular tension due to biting and the\\npresence of a digital watch inside the magnetically shielded room. The\\nresults demonstrate the capability of the method to identify and clearly\\nisolate the produced artifacts.\\n\\n1 Introduction\\nWhen using a magnetoencephalographic (MEG) record, as a research or clinical tool, the\\ninvestigator may face a problem of extracting the essential features of the neuromagnetic\\n? Corresponding author\\n\\n\n",
              "R. Vigario,\\n\\n230\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\nsignals in the presence of artifacts. The amplitude of the disturbance may be higher than\\nthat of the brain signals, and the artifacts may resemble pathological signals in shape. For\\nexample, the heart's electrical activity, captured by the lowest sensors of a whole-scalp\\nmagnetometer array, may resemble epileptic spikes and slow waves (Jousmili and Hari\\n1996).\\nThe identification and eventual removal of artifacts is a common problem in electroencephalography (EEG), but has been very infrequently discussed in context to MEG (Hari\\n1993; Berg and Scherg 1994).\\nThe simplest and eventually most commonly used artifact correction method is rejection,\\nbased on discarding portions of MEG that coincide with those artifacts. Other methods\\ntend to restrict the subject from producing the artifacts (e.g. by asking the subject to fix the\\neyes on a target to avoid eye-related artifacts, or to relax to avoid muscular artifacts). The\\neffectiveness of those methods can be questionable in studies of neurological patients, or\\nother non-co-operative subjects. In eye artifact canceling, other methods are available and\\nhave recently been reviewed by Vigario (I 997b) whose method is close to the one presented\\nhere, and in Jung et aI. (1998).\\nThis paper introduces a new method to separate brain activity from artifacts, based on the\\nassumption that the brain activity and the artifacts are anatomically and physiologically\\nseparate processes, and that their independence is reflected in the statistical relation between the magnetic signals generated by those processes.\\nThe remaining of the paper will include an introduction to the independent component\\nanalysis, with a presentation of the algorithm employed and some justification of this approach. Experimental data are used to illustrate the feasibility of the technique, followed\\nby a discussion on the results.\\n\\n2\\n\\nIndependent Component Analysis\\n\\nIndependent component analysis is a useful extension of the principal component analysis\\n(PC A). It has been developed some years ago in context with blind source separation applications (Jutten and Herault 1991; Comon 1994). In PCA. the eigenvectors of the signal\\ncovariance matrix C = E{xx T } give the directions oflargest variance on the input data\\nx. The principal components found by projecting x onto those perpendicular basis vectors\\nare uncorrelated, and their directions orthogonal.\\nHowever, standard PCA is not suited for dealing with non-Gaussian data. Several authors, from the signal processing to the artificial neural network communities, have shown\\nthat information obtained from a second-order method such as PCA is not enough and\\nhigher-order statistics are needed when dealing with the more demanding restriction of\\nindependence (Jutten and Herault 1991; Comon 1994). A good tutorial on neural ICA implementations is available by Karhunen et al. (1997). The particular algorithm used in this\\nstudy was presented and derived by Hyvarinen and Oja (1997a. 1997b).\\n\\n2.1\\n\\nThe model\\n\\nIn blind source separation, the original independent sources are assumed to be unknown,\\nand we only have access to their weighted sum. In this model, the signals recorded in an\\nMEG study are noted as xk(i) (i ranging from 1 to L, the number of sensors used, and\\nk denoting discrete time); see Fig. 1. Each xk(i) is expressed as the weighted sum of M\\n\\n\n",
              "ICAfor Identification of Artifacts in MEG Recordings\\n\\n231\\n\\nindependent signals Sk(j), following the vector expression:\\nM\\n\\nXk = La(j)sdj) = ASk,\\n\\n(1)\\n\\nj=l\\n\\nwhere Xk = [xk(1), ... , xk(L)]T is an L-dimensional data vector, made up of the L mixtures at discrete time k. The sk(1), ... , sk(M) are the M zero mean independent source\\nsignals, and A = [a(1), . .. , a(M)] is a mixing matrix independent of time whose elements\\nail are th.e unknown coefficients of the mixtures. In order to perform ICA, it is necessary\\nto have at least as many mixtures as there are independent sources (L ~ M). When this\\nrelation is not fully guaranteed, and the dimensionality of the problem is high enough,\\nwe should expect the first independent components to present clearly the most strongly\\nindependent signals, while the last components still consist of mixtures of the remaining\\nsignals. In our study, we did expect that the artifacts, being clearly independent from the\\nbrain activity, should come out in the first independent components. The remaining of the\\nbrain activity (e.g. a and J-L rhythms) may need some further processing.\\nThe mixing matrix A is a function of the geometry of the sources and the electrical conductivities of the brain, cerebrospinal fluid, skull and scalp. Although this matrix is unknown.\\nwe assume it to be constant, or slowly changing (to preserve some local constancy).\\nThe problem is now to estimate the independent signals Sk (j) from their mixtures, or the\\nequivalent problem of finding the separating matrix B that satisfies (see Eq. 1)\\n(2)\\nIn our algorithm, the solution uses the statistical definition of fourth-order cumulant or\\nkurtosis that, for the ith source signal, is defined as\\n\\nkurt(s(i)) = E{s(i)4} - 3[E{s(i)2}]2,\\nwhere E( s) denotes the mathematical expectation of s.\\n\\n2.2 The algorithm\\nThe initial step in source separation, using the method described in this article, is whitening, or sphering. This projection of the data is used to achieve the uncorrelation between\\nthe solutions found, which is a prerequisite of statistical independence (Hyvarinen and Oja\\n1997a). The whitening can as well be seen to ease the separation of the independent signals (Karhunen et al. 1997). It may be accomplished by PCA projection: v = V x, with\\nE{ vv T } = I. The whitening matrix V is given by\\n-=T ,\\nV -- A- 1 / 2 .....\\n\\nwhere A = diag[-\\(1), ... , -\\(M)] is a diagonal matrix with the eigenvalues of the data\\ncovariance matrix E{xxT}, and 8 a matrix with the corresponding eigenvectors as its\\ncolumns.\\nConsider a linear combination y = w T v of a sphered data vector v, with Ilwll = 1. Then\\nE{y2} = .1 andkurt(y) = E{y4}-3, whose gradientwithrespecttow is 4E{v(wTv)3} .\\nBased on this, Hyvarinen and Oja (1997a) introduced a simple and efficient fixed-point\\nalgorithm for computing ICA, calculated over sphered zero-mean vectors v, that is able to\\nfind one of the rows of the separating matrix B (noted w) and so identify one independent\\nsource at a time - the corresponding independent source can then be found using Eq. 2.\\nThis algorithm, a gradient descent over the kurtosis, is defined for a particular k as\\n1. Take a random initial vector Wo of unit norm. Let l = 1.\\n\\n\n",
              "232\\n\\nR. Vigario,\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\n2. Let Wi = E{V(W[.1 v)3} - 3Wl-I. The expectation can be estimated using a\\nlarge sample OfVk vectors (say, 1,000 vectors).\\n3. Divide Wi by its norm (e.g. the Euclidean norm\\n4.\\n\\nIlwll = JLi wI J.\\n\\nlflwT wi-II is not close enough to 1, let I = 1+1 andgo back to step 2.\\n\\nOtherwise,\\n\\noutput the vector Wi.\\n\\nIn order to estimate more than one solution, and up to a maximum of lvI, the algorithm\\nmay be run as many times as required. It is, nevertheless, necessary to remove the infonnation contained in the solutions already found, to estimate each time a different independent\\ncomponent. This can be achieved, after the fourth step of the algorithm, by simply subtracting the estimated solution s = w T v from the unsphered data Xk . As the solution is\\ndefined up to a multiplying constant, the subtracted vector must be multiplied by a vector\\ncontaining the regression coefficients over each vector component of Xk.\\n\\n3\\n\\nMethods\\n\\nThe MEG signals were recorded in a magnetically shielded room with a 122-channel\\nwhole-scalp Neuromag-122 neuromagnetometer. This device collects data at 61 locations\\nover the scalp, using orthogonal double-loop pick-up coils that couple strongly to a local\\nsource just underneath, thus making the measurement \"near-sighted\" (HamaHi.inen et al.\\n1993).\\nOne of the authors served as the subject and was seated under the magnetometer. He kept\\nhis head immobile during the measurement. He was asked to blink and make horizontal\\nsaccades, in order to produce typical ocular artifacts. Moreover, to produce myographic\\nartifacts, the subject was asked to bite his teeth for as long as 20 seconds. Yet another\\nartifact was created by placing a digital watch one meter away from the helmet into the\\nshieded room. Finally, to produce breathing artifacts, a piece of metal was placed next\\nto the navel. Vertical and horizontal electro-oculograms (VEOG and HEOG) and electrocardiogram (ECG) between both wrists were recorded simultaneously with the MEG, in\\norder to guide and ease the identification of the independent components. The bandpassfiltered MEG (0.03-90 Hz), VEOG, HEOG, and ECG (0.1-100 Hz) signals were digitized\\nat 297 Hz, and further digitally low-pass filtered, with a cutoff frequency of 45 Hz and\\ndownsampled by a factor of 2. The total length of the recording was 2 minutes. A second\\nset of recordings was perfonned, to assess the reproducibility of the results.\\nFigure 1 presents a subset of 12 spontaneous MEG signals from the frontal, temporal and\\noccipital areas. Due to the dimension of the data (122 magnetic signals were recorded), it\\nis impractical to plot all MEG signals (the complete set is available on the internet - see\\nreference list for the adress (Vigario 1997a?. Also both EOG channels and the electrocardiogram are presented.\\n\\n4\\n\\nResults\\n\\nFigure 2 shows sections of9 independent components (IC's) found from the recorded data,\\ncorresponding to a I min period, starting 1 min after the beginning of the measurements.\\nThe first two IC's, with a broad band spectrum, are clearly due to the musclular activity\\noriginated from the biting. Their separation into two components seems to correspond, on\\nthe basis of the field patterns, to two different sets of muscles that were activated during\\nthe process. IC3 and IC5 are, respectively showing the horizontal eye movements and the\\neye blinks, respectively. IC4 represents cardiac artifact that is very clearly extracted. In\\nagreement with Jousmaki and Hari (1996), the magnetic field pattern of IC4 shows some\\npredominance on the left.\\n\\n\n",
              "ICA/or Identification 0/ Artifacts in MEG Recordings\\n\\n233\\nMEG [ 1000 fTlcm\\n\\nI--\\n\\n---l\\n\\nsaccades\\n\\nI--\\n\\n---l\\n\\nblinking\\n\\nEOG [\\n\\n500 IlV\\n\\nECG [\\n\\n500 IlV\\n\\nI--\\n\\nbiting\\n\\n---l MEG\\n\\n~=::::::::::::::=::\\n~?'104~\\n\\nrJ. .........\\n\\nM\\n\\n,J.\\.......1iIIiM~..\\n\\n::\\nt...\\n\\n:;::::::;:::~=\\n\\n~::::::::;::=\\n~ ?? \",~Jrt\\n\\n..,.\\n\\nt\\n\\n....\\n\\n~,.~ . ? .J..\\n\\n.\\n\\n.../\\\"\"$\"\"~I\\n\\n2 t\\n\\n:;\\n\\n:;\\n4\\n\\n~\\n\\n5 t\\n\\n., ... ...., ,'fIJ'\\,\\n..........-.\\n\\n,..,d\\n\\n,LIlt ... .,\\n\\nI?\\n\\n.,............. ................. \"\\n\\n....,..,.\"........ .\\n\\n.... Dei ..... \"\\n\\n.'''IIb'''*. rt\\n\\n-1I\\JY. ? ---\\n\\nI p\", . . . . , . . . . . . . . . . . . at ...'....\\n\\nI; rp ..\\n\\n,P....\\n\\n,\\n\\n.,...............' tMn':M.U\\n\\n... ,\\n\\n, ..... '\\n\\nU\\..,.--II..------'-__\\n\\nooII..Jl,,-\\n\\n\".'tIItS\\n\\n5 ~\\n\\n6 t\\n\\nVEOG\\n\\nIt ... 11.1. HEOG\\n\\n~UijuJJJ.LU Wl Uij.lJU.LllU.UUUllUUij,UU~ijJJJ\\n\\nECG\\n\\n10 s\\n\\nFigure 1: Samples of MEG signals, showing artifacts produced by blinking, saccades,\\nbiting and cardiac cycle. For each of the 6 positions shown, the two orthogonal directions\\nof the sensors are plotted.\\nThe breathing artifact was visible in several independent components, e.g. IC6 and IC7. It\\nis possible that, in each breathing the relative position and orientation of the metallic piece\\nwith respect to the magnetometer has changed. Therefore, the breathing artifact would be\\nassociated with more than one column of the mixing matrix A, or to a time varying mixing\\nvector.\\nTo make the analysis less sensible to the breathing artifact, and to find the remaining artifacts, the data were high-pass filtered, with cutoff frequency at 1 Hz. Next, the independent\\ncomponent IC8 was found. It shows clearly the artifact originated at the digital watch,\\nlocated to the right side of the magnetometer.\\nThe last independent component shown, relating to the first minute of the measurement,\\nshows an independent component that is related to a sensor presenting higher RMS (root\\nmean squared) noise than the others.\\n\\n5\\n\\nDiscussion\\n\\nThe present paper introduces a new approach to artifact identification from MEG recordings, based on the statistical technique of Independent Component Analysis. Using this\\nmethod, we were able to isolate both eye movement and eye blinking artifacts, as well as\\n\\n\n",
              "R. Vigario,\\n\\n234\\n\\nv. Jousmiiki, M HtJmlJliiinen, R. Hari and E. Oja\\n\\ncardiac, myographic, and respiratory artifacts.\\nThe basic asswnption made upon the data used in the study is that of independence between brain and artifact waveforms. In most cases this independence can be verified by the\\nknown differences in physiological origins of those signals. Nevertheless, in some eventrelated potential (ERP) studies (e.g. when using infrequent or painful stimuli), both the\\ncerebral and ocular signals can be similarly time-locked to the stimulus. This local time\\ndependence could in principle affect these particular ICA studies. However, as the independence between two signals is a measure of the similarity between their joint amplitude\\ndistribution and the product of each signal's distribution (calculated throughout the entire\\nsignal, and not only close to the stimulus applied), it can be expected that the very local\\nrelation between those two signals, during stimulation, will not affect their global statistical\\nrelation.\\n\\n6\\n\\nAcknowledgment\\n\\nSupported by a grant from Junta Nacional de Investiga~ao Cientifica e Tecnologica, under\\nits 'Programa PRAXIS XXI' (R.Y.) and the Academy of Finland (R.H.).\\n\\nReferences\\nBerg, P. and M. Scherg (1994). A multiple source approach to the correction of eye\\nartifacts. Electroenceph. clin. Neurophysiol. 90, 229-241.\\nComon, P. (1994). Independent component analysis - a new concept? Signal Processing 36,287-314.\\nHamalainen, M., R. Hari, R. Ilmoniemi, 1. Knuutila, and O. Y. Lounasmaa (1993, April).\\nMagnetoencephalography-theory, instrumentation, and applications to noninvasive\\nstudies of the working human brain. Reviews o/Modern Physics 65(2), 413-497.\\nHari, R. (1993). Magnetoencephalography as a tool of clinical neurophysiology. In\\nE. Niedermeyer and F. L. da Silva (Eds.), Electroencephalography. Basic principles, clinical applications, and relatedjields, pp. 1035-1061 . Baltimore: Williams\\n& Wilkins.\\nHyvarinen, A. and E. Oja (l997a). A fast fixed-point algorithm for independent component analysis. Neural Computation (9), 1483-1492.\\nHyvarinen, A. and E. Oja (1997b). One-unit learning rules for independent component\\nanalysis. In Neural Information Processing Systems 9 (Proc. NIPS '96). MIT Press.\\nJousmiiki, Y. and R. Hari (1996). Cardiac artifacts in magnetoencephalogram. Journal\\no/Clinical Neurophysiology 13(2), 172-176.\\nJung, T.-P., C. Hwnphries, T.-W. Lee, S. Makeig, M. J. McKeown, Y. lragui, and\\nT. Sejnowski (1998). Extended ica removes artifacts from electroencephalographic\\nrecordings. In Neural Information Processing Systems 10 (Proc. NIPS '97). MIT\\nPress.\\nJutten, C. and 1. Herault (1991). Blind separation of sources, part i: an adaptive algorithm based on neuromimetic architecture. Signal Processing 24, 1-10.\\nKarhunen, J., E. Oja, L. Wang, R. Vigmo, and J. Joutsensalo (1997). A class of neural\\nnetworks for independent component analysis. IEEE Trans. Neural Networks 8(3),\\n1-19.\\nVigmo, R. (1997a). WWW adress for the MEG data:\\nhttp://nuc1eus.hut.firrvigarioINIPS97_data.html.\\nVigmo, R. (1997b). Extraction of ocular artifacts from eeg using independent component analysis. To appear in Electroenceph. c/in. Neurophysiol.\\n\\n\n",
              "ICAfor Identification ofArtifacts in MEG Recordings\\n\\n235\\n\\n~~~\\n\\nIC1\\n\\n------,--y~-------------------------.-.------~~.. ,.. ~\\nU\\n\\n...\\n\\nIC2\\n\\nIC3\\n.\",\\n\\n''' ... '' .. '\\n\\n<> .\\n).\\~\\n.\\ C:> ?\\n\\\\n\\n~~~}a\\n\\n~~-\"\\n\\n____I4-_. _\\n. . . ._.---_._. . . .-.__\\n. \"\"\"\"\"\"?t;_-\"'' '....\\n~\\n\\n. . . . .-......,.....\\n\\n~_1\\n\\nIC4\\n\\nIC5\\n\\nIC6\\n~\\n...W\"\\n....\\n\"1011\\n...~\"_f....\\n..\".,.\"\"'_\\n\\n/tJ'IfII/'h\\n\\nI' ......\\n\\nd1b ..\\n\\n~*W,.'tJ ......\\n\\nr' .. ns...\\n\\nIC7\\n\\nICB\\n\\nICg\\n~._-~.,.\\n\\n. . . . .t . .\\n\\nWt:n:ePWt.~..,.~I'NJ'~~\\nI\\n\\n10 s\\n\\nI\\n\\nFigure 2: Nine independent components found from the MEG data. For each component the\\nleft, back and right views of the field patterns generated by these components are shown full line stands for magnetic flux coming out from the head, and dotted line the flux inwards.\\n\\n\n",
              "  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Near-Maximum Entropy Models for Binary\\nNeural Representations of Natural Images\\n\\nMatthias Bethge and Philipp Berens\\nMax Planck Institute for Biological Cybernetics\\nSpemannstrasse 41, 72076, T?ubingen, Germany\\nmbethge,berens@tuebingen.mpg.de\\n\\nAbstract\\nMaximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these\\napproaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new\\napproach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data?the model parameters can be derived\\nin closed form and sampling is easy. Therefore, our NearMaxEnt approach can\\nserve as a tool for testing predictions from a pairwise maximum entropy model not\\nonly for low-dimensional marginals, but also for high dimensional measurements\\nof more than thousand units. We demonstrate its usefulness by studying natural\\nimages with dichotomized pixel intensities. Our results indicate that the statistics\\nof such higher-dimensional measurements exhibit additional structure that are not\\npredicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of\\ndimensionality where estimation of the full joint distribution is feasible.\\n\\n1\\n\\nIntroduction\\n\\nA core issue in sensory coding is to seek out and model statistical regularities in high-dimensional\\ndata. In particular, motivated by developments in information theory, it has been hypothesized\\nthat modeling these regularities by means of redundancy reduction constitutes an important goal of\\nearly visual processing [2]. Recent studies conjectured that the binary spike responses of retinal\\nganglion cells may be characterized completely in terms of second-order correlations when using\\na maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual\\ninput, however, this would be very surprising: Natural images are known to exhibit complex higherorder correlations which are extremely difficult to model yet being perceptually relevant. Thus, if\\nwe assume that retinal ganglion cells do not discard the information underlying these higher-order\\ncorrelations altogether, it would be a very difficult signal processing task to remove all of those\\nalready within the retinal network.\\nOftentimes, neurons involved in early visual processing are modeled as rather simple computational\\nunits akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity.\\nFor such simple neuron models, the possibility of removing higher-order correlations present in the\\ninput is very limited [3].\\nHere, we study the role of second-order correlations in the multivariate binary output statistics of\\nsuch linear-nonlinear model neurons with a threshold nonlinearity responding to natural images.\\nThat is, each unit can be described by an affine transformation zk = wkT x + ? followed by a\\npoint-wise signum function sk = sgn(zk ). Our interest in this model is twofold: (A) It can be\\nregarded a parsimonious model for the analysis of population codes of natural images for which the\\n1\\n\\n\n",
              "A\\n\\n?3\\n\\nB\\n\\n3\\n\\n0\\n\\n2\\n\\nC\\n\\n6\\n\\nJS?Divergence (bits)\\n\\n?H (%)\\n\\nx 10\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n1\\n\\nlog? H (%)\\n\\n?H (%)\\n\\n4\\n3\\n2\\n1\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n10\\n\\nD\\n10\\n\\n0.5\\n\\n0\\n\\n?5\\n\\n5\\n\\n0\\n\\n10\\n\\nx 10\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n0\\n\\n10\\n\\n?1\\n\\n10\\n\\n?2\\n\\n20\\n\\n10\\n\\nDimension\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n20\\n\\nlog 2 (Number of Samples)\\n\\nFigure 1: Similarity between the Ising and the DG model. A+C: Entropy difference ?H between the Ising\\nmodel and the Dichotomized Gaussian distribution as a function of dimensionality. A: Up to 10 dimensions\\nwe can compute HDG directly by evaluating Eq. 6. Gray dots correspond to different sets of parameters. For\\nm ? 4, the relatively large scatter and the existence of negative values is due to the limited numerical precision\\nof the Monte-Carlo integration. Errorbars show standard error of the mean. B. JS-divergence DJS between PI\\nand PDG . C. ?H as above, for higher dimensions. Up to 20 dimensions ?H remains very small. The increase\\nfor m ? 20 is most likely due to undersampling of the distributions. D. ?H as function of sample size used\\nto estimate HDG , at seven (black) and ten (grey) dimensions (note log scale on both axes). ?H decreases with\\na power law with increasing sample sizes.\\n\\ncomputational power and the bandwidth of each unit is limited. (B) The same model can also be\\nused more generally to fit multivariate binary data with given pairwise correlations, if x is drawn\\nfrom a Gaussian distribution. In particular, we will show that the resulting distribution closely\\nresembles the binary maximum entropy models known as Ising models or Boltzmann machines\\nwhich have recently become popular for the analysis of spike train recordings from retinal ganglion\\ncell responses [13, 12].\\nMotivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more general level in the following questions: are pairwise interactions enough for understanding the statistical regularities in high-dimensional natural data (given that they provide a good fit in the lowdimensional case)? If we suppose that pairwise interactions are enough, what can we say about the\\namount of redundancies in high-dimensional data? In comparison with neural spike data, natural\\nimages provide two advantages for studying these questions: 1) It is much easier to obtain large\\namounts of data with millions of samples which are less prone to nonstationarities. 2) Often differences in the higher-order statistics such as between pink noise and natural images can be recognized\\nby eye.\\n\\n2\\n\\nSecond order models for binary variables\\n\\nIn order to study whether pairwise interactions are enough to determine the statistical regularities\\nin high-dimensional data, it is necessary to be able to compute the maximum entropy distribution\\nfor large number of dimensions N . Given a set of measured statistics, maximum entropy models\\nyield a full probability distribution that is consistent with these constraints but does not impose any\\n2\\n\\n\n",
              "0.05\\n0\\n0\\n?4\\n\\n1\\n?\\n\\n2\\n\\n1\\n\\n2\\n\\nx 10\\n\\n2\\n1\\n0\\n\\n0\\n\\n?\\n\\nFigure 2: Examples of covariance matrices (A+B.) and their learned approximations (C+D) at m = 10 for\\nclarity. ? is the parameter controlling the steepness of correlation decrease. E+F. Eigenvalue spectra of both\\nmatrices. G. Entropy difference ?H and H. JS-divergence between the distribution of samples obtained from\\nthe two models at m = 7.\\n\\nadditional structure on the distribution [7]. For binary data with given mean activations ?i = hsi i\\nand correlations between neurons ?ij = hsi sj i ? hsi ihsj i, one obtains a quadratic exponential\\nprobability mass function known as the Ising model in physics or as the Boltzmann machine in\\nmachine learning.\\nCurrently all methods used to determine the parameters of such binary maximum entropy models\\nsuffer from the same drawback: since the parameters do not correspond directly to any of the measured statistics, they have to be inferred (or ?learned?) from data. In high dimensions though, this\\nposes a difficult computational problem. Therefore the characterization of complete neural circuits\\nwith possibly hundreds of neurons is still out of reach, even though analysis was recently extended\\nto up to forty neurons [14].\\nTo make the maximum entropy approach feasible in high dimensions, we propose a new strategy:\\nSampling from a ?near-maximum? entropy model that does not require any complicated learning\\nof parameters. In order to justify this approach, we verify empirically that the entropy of the full\\nprobability distributions obtained with the near-maximum entropy model are indistinguishable from\\nthose obtained with classical methods such as Gibbs sampling for up to 20 dimensions.\\n2.1\\n\\nBoltzmann machine learning\\n\\nFor a binary vector of neural activities s ? {?1, 1}m and specified ?i and ?ij the Ising model takes\\nthe form\\n?\\n?\\nm\\nX\\nX\\n1\\n1\\nPI (s) = exp ?\\nhi si +\\nJij si sj ? ,\\n(1)\\nZ\\n2\\ni=1\\ni6=j\\n\\nwhere the local fields hi and the couplings Jij have to be chosen such that hsi i = ?i and hsi sj i ?\\nhsi ihsj i = ?ij . Unfortunately, finding the correct parameters turns out to be a difficult problem\\nwhich cannot be solved in closed form.\\nTherefore, one has to resort to an optimization approach to learn the model parameters hi and Jij\\nfrom data. This problem is called Boltzmann machine learning and is based on maximization of the\\nlog-likelihood L = ln PI ({si }N\\ni=1 |h, J) [1] where N is the number of samples. The gradient of the\\nlikelihood can be computed in terms of the empirical covariance and the covariance of si and sj as\\nproduced by the current model:\\n?L\\n= hsi sj iData ? hsi sj iModel\\n?Jij\\n\\n(2)\\n\\nThe second term on the right hand side is difficult to compute, as it requires sampling from the model.\\nSince the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such\\n3\\n\\n\n",
              "Figure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from\\nthe corresponding dichotomized Gaussian distribution with equal covariance matrix (middle). It is not possible\\nto see any systematic difference between the samples from the two distributions. For comparison, this is not so\\nfor the sample from the independent model (right).\\n\\nas Gibbs sampling are employed [9] in order to approximate the required model average. This is\\ncomputationally demanding as sampling is necessary for each individual update. While efficient\\nsampling algorithms exist for special cases [6], it still remains a hard and time consuming problem\\nin the general case. Additionally, most sampling algorithms do not come with guarantees for the\\nquality of the approximation of the required average. In conclusion, parameter fitting of the Ising\\nmodel is slow and oftentimes painstaking, especially in high dimensions.\\n2.2\\n\\nModeling with the dichotomized Gaussian\\n\\nHere we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model\\nby a ?near-maximum? entropy model, for which both parameter computation and sampling is easy. A\\nvery convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian\\ndistribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated\\nfrom a hidden Gaussian variable\\nz ? N (?, ?) ,\\n\\nsi = sgn(zi ).\\n\\n(3)\\n\\nWithout loss of generality, we can assume unit variances for the Gaussian, i.e. ?ii = 1, the mean ?\\nand the covariance matrix ? of s are given by\\n?i = 2?(?i ) ? 1 ,\\n\\n?ii = 4?(?i )?(??i ) ,\\n\\n?ij = 4?(?i , ?j , ?ij ) for i 6= j\\n\\n(4)\\n\\nwhere ?(x, y, ?) = ?2 (x, y, ?) ? ?(x)?(y) . Here ? is the univariate standardized cumulative\\nGaussian distribution and ?2 its bivariate counterpart. While the computation of the model parameters was hard for the Ising model, these equations can be easily inverted to find the parameters of\\nthe hidden Gaussian distribution:\\n\u0012\\n\u0013\\n?i + 1\\n?i = ??1\\n(5)\\n2\\nDetermining ?ij generally requires to find a suitable value such that ?ij ? 4?(?i , ?j , ?ij ) = 0.\\nThis can be efficently solved by numerical computations, since the function is monotonic in ?ij\\nand has a unique\u0001 zero crossing. We obtain an especially easy case, when ?i = ?j = 0, as then\\n?ij = sin ?2 ?ij .\\nIt is also possible to evaluate the probability mass function of the DG model by numerical integration,\\nZ b1\\nZ bm\\n\u0001\\n1\\nT ?1\\nPDG (s) =\\n.\\n.\\n.\\nexp\\n?(s\\n?\\n?)\\n?\\n(s\\n?\\n?)\\n,\\n(6)\\n(2?)N/2 |?|1/2 a1\\nam\\nwhere the integration limits are chosen as ai = 0 and bi = ?, if si = 1, and ai = ?? and bi = 0,\\notherwise.\\nIn summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling\\nis easy, and (2) finding the model parameters is easy too.\\n4\\n\\n\n",
              "3\\n\\nNear-maximum entropy behavior of the dichotomized Gaussian\\ndistribution\\n\\nIn the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that\\nin many cases it can serve as a convenient approximation to the Ising model. Now, we investigate\\nhow good this approximation is. For a wide range of interaction terms and mean activations we\\nverify that the DG model closely resembles the Ising model. In particular we show that the entropy of\\nthe DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions.\\n3.1\\n\\nRandom Connectivity\\n\\nWe created randomly connected networks of varying size m, where mean activations hi and\\ninteractions\\nterms Jij were drawn from N (0, 0.4). First, we compared the entropy HI =\\nP\\n? s PI (s) log2 PI (s) of the thus specified Ising model obtained by evaluating Eq. 1 with the entropy of the DG distribution HDG computed by numerical integration1 from Eq. 6 (twenty parameter\\nsets). The entropy difference ?H = HI ? HDG was smaller than 0.002 percent of HI (Fig. 1 A,\\nnote scale) and probably within the range of the numerical integration accuracy. In addition, we\\ncomputed the Jensen-Shannon divergence DJS [PI kPDG ] = 12 (DKL [PI kM ] + DKL [PDG kM ]),\\nwhere M = 21 (PI + PDG ) [8]. We find that DJS [PI kPDG ] is extremly small up to 10 dimensions\\n(Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also\\nto have a very similar structure.\\nNext, we extended this analysis to networks of larger size and repeated the same analysis for up to\\ntwenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m ? 20 due\\nto the large number of states, we used a histogram based estimate of PDG (using 3 ? 106 samples\\nfor m < 15 and 15 ? 106 samples for m ? 15). The estimate of ?H is still very small at high\\ndimensions (Fig. 1 C, below 0.5%). We also computed DJS , which scaled similarly to ?H (data\\nnot shown).\\nIn Fig. 1 C, ?H seems to increase with dimensionality. Therefore, we investigated how the estimate\\nof ?H is influenced by the number of samples used. We computed both quantities for varying numbers of samples from the DG distribution (for m = 7, 10). As ?H decreases according to a power\\nlaw with increasing m, the rise of ?H observed in Fig. 1 C is most likely due to undersampling of\\nthe distribution.\\n3.2\\n\\nSpecified covariance structure\\n\\nTo explore the relationship between the two techniques more systematically, we generated covariance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the nth\\ndiagonal is set to a constant value exp(?? ? n) (Fig. 2A and B, m = 7, 10). We varied the decay\\nparameter ?, which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E\\nand F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The covariance matrix of the samples drawn from the Ising model resembles the original very closely (Fig.\\n2C and D). We also computed the entropy of the DG model using the desired covariance structure.\\nWe estimated ?H and DJS [PG kPDG ] averaged over 10 trials with 105 samples obtained by Gibbs\\nsampling from the Ising model. ?H is very close to zero (Fig. 2G, m = 7) except for small ?s\\nand never exceeded 0.05%. Moreover, the structure of both distributions seems to be very similar as\\nwell (Fig. 2H, m = 7). At m = 10, both quantities scaled qualitatively similair (data not shown).\\nWe also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data\\nnot shown).\\nOur experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good\\napproximation to the quadratic exponential distribution for a large parameter range. In the following\\nsection, we will exploit the similarity between the two models to study how the role of second-order\\ncorrelations may change between low-dimensional and high-dimensional statistics in case of natural\\nimages.\\n1\\nFor integration, we used the mvncdf function of Matlab. For m ? 4 this function employs Monte-Carlo\\nintegration.\\n\\n5\\n\\n\n",
              "Figure 4: A: Negative log probabilities of the DG model are plotted against ground truth (red dots). Identical\\ndistributions fall on the diagonal. Data points outside the area enclosed by the dashed lines indicate significant\\ndifferences between the model and ground truth. The DG model matches the true distribution very well. For\\ncomparison the independent model is shown as well (blue crosses). B: The multi-information of the true\\ndistribution (blue dots) accurately agrees with the multi-information of the DG model (red line). Similar to\\nthe analysis in [12], we observe a power law behavior of the entropy of the independent model (black solid\\nline) and the mutli-information. Linear extrapolation (in the log-log plot) to higher dimensions is indicated by\\ndashed lines. C: Different way of presentation of the same data as in B: the joint entropy H = Hindep ? I\\n(blue dots) is plotted instead of I and the axis are in linear scale. The dashed red line represents the same\\nextrapolation as in B.\\n\\n4\\n\\nNatural images: Second order and beyond\\n\\nWe now investigate to which extent the statistics of natural images with dichotomized pixel intensities can be characterized by pairwise correlations only. In particular, we would like to know how\\nthe role of pairwise correlations opposed to higher-order correlations changes depending on the dimensionality. Thanks to the DG model introduced above, we are in the position to study the effect\\nof pairwise correlations for high-dimensional binary random variables (N ? 1000 or even larger).\\nWe use the van Hateren image database in log-intensity scale, from which we sample small image\\npatches at random positions. The threshold for the dichotomization is set to the median of pixel\\nintensities. That is, each binary variable encodes whether the corresponding pixel intensity is above\\nor below the median over the ensemble. Up to patch sizes of 4 ? 4 pixel, the true joint statistics can\\nbe assessed using nonparametric histogram methods. Before we present quantitative comparisons, it\\nis instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model\\nwith same mean and covariance (Fig. 3, middle), and from the corresponding independent model\\n(Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well.\\nIn order to quantify how well the DG model matches the true distribution, we draw two independent\\nsets of samples from each (N = 2 ? 106 for each set) and generate a scatter plot as shown in\\nFig. 4 A for 4 ? 4 image patches. Each dot corresponds to one of the 216 = 65536 possible different\\nbinary patterns. The relative frequencies of these patterns according to the DG model (red dots) and\\naccording to the independent model (blue dots) are plotted against the relative frequencies obtained\\nfrom the natural image patches. The solid diagonal line corresponds to a perfect match between\\nmodel and ground truth. The dashed lines enclose the regions within which deviations are to be\\nexpected due to the finite sampling size. Since most of the red dots fall within this region, the DG\\nmodel fits the data distribution very well.\\nP\\nWe also systematically evaluated the JS-divergence and the multi-information I[S] = k H[Sk ] ?\\nH[S] as a function of dimensionality. That is, we started with the bivariate marginal distribution\\nof two randomly selected pixels. Then we incrementally added more pixels of random location\\nuntil the random vector contains all the 16 pixels of the 4 ? 4 image patches. Independent of the\\ndimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015\\nbits. For comparison, the JS-divergence between the independent model and the true distribution\\nincreases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in\\nthe case of 16 pixels. For two independent sets of samples both drawn from natural image data the\\nJS-divergence ranges between 0.006 and 0.007 bits for 4 ? 4 patches setting the gold standard for\\nthe minimal possible JS-divergence one could achieve with any model due to finite sampling size.\\nCarrying out the same type of analysis as in [12], we make qualitatively the same observations as it\\nwas reported there: as shown above, we find a quite accurate match between the two distributions.\\n6\\n\\n\n",
              "Figure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and\\nfrom the corresponding dichotomized Gaussian distribution with equal covariance matrix (right). For the latter, the percept of typical objects is missing due to the ignorance of higher-order correlations. This striking\\ndifference is not obvious, however, at the level of 4x4 patches, for which we found an excellent match of the\\ndichotomized Gaussian to the ensemble of natural images.\\n\\nFurthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue\\ndots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings\\ncan be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12],\\ntwo claims about the higher-dimensional statistics have been based on these two observations: First,\\nthat pairwise correlations may be sufficient to determine the full statistics of binary responses, and\\nsecondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards\\nstrong order.\\nUsing natural images instead of retinal ganglion cell data, we would like to verify to what extent\\nthe low-dimensional observations can be used to support these claims about the high-dimensional\\nstatistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions\\n(dashed lines) as in [12]. The difference between the entropy of the independent model and the\\nmulti-information yields the joint entropy of the respective distribution. If the extrapolation is taken\\nseriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint\\nentropy of the neural responses approaches zero at this size?say for 7 ? 7 image patches (Fig. 4 C).\\nThough it was not taken literally, this point of ?freezing? has been pointed out in [12] as a critical\\nnetwork size at which a transition to strong order is to be expected. The meaning of this assertion,\\nhowever, is not clear. First of all, the joint entropy of a distribution can never be smaller than the\\njoint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing\\nnumber of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to\\nask more precisely how the growth rate of the joint entropy can be characterized and whether there\\nis a critical number of dimensions at which the growth rate suddenly drops. In our study with natural\\nimages, visual inspection does not indicate anything special to happen at the ?critical patch size? of\\n7 ? 7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5\\n(right) we show a sample from the DG model for 32?32 image patches (i.e. 1024 dimensions) which\\nprovides no indication for a particularly interesting change in the statistics towards strong order. The\\nexact law according to which the multi-information grows with the number of dimensions for large\\nm, however, is not easily assessed and remains to be explored.\\nFinally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions\\ndoes not hold any more in the case of large m: the samples from the true distribution at the left\\nhand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5,\\nright), indicating that pairwise correlations do not suffice to determine the full statistics of large\\nimage patches. Even if the match between the DG model and the Ising model may turn out to be\\nless accurate in high dimensions, this would not affect our conclusion. Any mismatch would only\\nintroduce more order in the DG model than justified by pairwise correlations only.\\n\\n5\\n\\nConclusion and Outlook\\n\\nWe proposed a new approach to maximum entropy modeling of binary variables, extending maximum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa7\\n\\n\n",
              "rameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of\\nMonte-Carlo methods. We verified numerically that the empirical entropy of the DG model is comparable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes,\\nthe DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due\\nto the lack of independence between consecutive samples in the Gibbs sampler.\\nAlthough the Ising model and the DG model are in principle different, the match between the two\\nturns out to be surprisingly good for a large region of the parameter space. Currently, we are trying\\nto determine where the close similarity between the Ising model and the DG model breaks down.\\nIn addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal\\ndensity for Monte-Carlo methods such as importance sampling. As it is a very close approximation\\nto the Ising model, we expect this combination to yield highly efficient sampling behaviour. In\\nsummary, by linking the DG model to the Ising model, we believe that maximum entropy modeling\\nof multivariate binary random variables will become much more practical in the future.\\nWe used the DG model to investigate the role of second-order correlations in the context of sensory coding of natural images. While for small image patches the DG model provided an excellent\\nfit to the true distribution, we were able to show that this agreement breakes down in the case\\nof larger image patches. Thus caution is required when extrapolating from low-dimensional measurements to higher-dimensional distributions because higher-order correlations may be invisible in\\nlow-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be\\na promising tool for the analysis of correlated neural activities, and the DG model can facilitate its\\nuse significantly in practice.\\nAcknowledgments\\nWe thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stimulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute advice. An implementation of the DG model in Matlab and R will be avaible at our website\\nhttp://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling.\\n\\nReferences\\n[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive\\nScience, 9:147?169, 1985.\\n[2] H.B. Barlow. Sensory mechanisms, the reduction of redundancy, and intelligence. In The Mechanisation\\nof Thought Processes, pages 535?539, London: Her Majesty?s Stationery Office, 1959.\\n[3] M. Bethge. Factorial coding of natural images: How effective are linear model in removing higher-order\\ndependencies? J. Opt. Soc. Am. A, 23(6):1253?1268, June 2006.\\n[4] D.R. Cox and N. Wermuth. On some models for multivariate binary variables parallel in complexity with\\nthe multivariate gaussian distribution. Biometrika, 89:462?469, 2002.\\n[5] L.J. Emrich and M.R. Piedmonte. A method for generating high-dimensional multivariate binary variates.\\nThe American Statistician, 45(4):302?304, 1991.\\n[6] M. Huber. A bounding chain for swendsen-wang. Random Structures & Algorithms, 22:53?59, 2002.\\n[7] E.T. Jaynes. Where do we stand on maximum entropy inference. In R.D. Levine and M. Tribus, editors,\\nThe Maximum Entropy Formalism. MIT Press, Cambridge, MA, 1978.\\n[8] J. Linn. Divergence measures based on the shannon entropy. IEEE Trans Inf Theory, 37:145?151, 1991.\\n[9] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press,\\n2003.\\n[10] Sheila H Nirenberg and Jonathan D Victor. Analyzing the activity of large populations of neurons: how\\ntractable is the problem? Current Opinion in Neurobiology, 17:397?400, August 2007.\\n[11] Karl Pearson. On a new method of determining correlation between a measured character a, and a character b, of which only the percentage of cases wherein b exceeds (or falls short of) a given intensity is\\nrecorded for each grade of a. Biometrika, 7:96?105, 1909.\\n[12] Elad Schneidman, Michael J Berry, Ronen Segev, and William Bialek. Weak pairwise correlations imply\\nstrongly correlated network states in a neural population. Nature, 440(7087):1007?1012, Apr 2006.\\n[13] J Shlens, JD Field, JL Gauthier, MI Grivich, D Petrusca, A Sher, AM Litke, and EJ Chichilnisky. The\\nstructure of multi-neuron firing patterns in primate retina. J Neurosci, 26(32):8254?8266, Aug 2006.\\n[14] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek. Ising models for networks of real neurons. arXiv:qbio.NC/0611072, 1:1?4, 2006.\\n\\n8\\n\\n\n",
              "  \n",
              "2  Nearest-Neighbor Sample Compression:\\nEfficiency, Consistency, Infinite Dimensions\\n\\nAryeh Kontorovich\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nkaryeh@cs.bgu.ac.il\\n\\nSivan Sabato\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nsabatos@bgu.ac.il\\n\\nRoi Weiss\\nDepartment of Computer Science and Applied Mathematics\\nWeizmann Institute of Science\\nroiw@weizmann.ac.il\\n\\nAbstract\\nWe examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based\\nmulticlass learning algorithm. This algorithm is derived from sample compression\\nbounds and enjoys the statistical advantages of tight, fully empirical generalization\\nbounds, as well as the algorithmic advantages of a faster runtime and memory\\nsavings. We prove that this algorithm is strongly Bayes-consistent in metric\\nspaces with finite doubling dimension ? the first consistency result for an efficient\\nnearest-neighbor sample compression scheme. Rather surprisingly, we discover\\nthat this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which\\nclassic consistency proofs hinge are violated. This is all the more surprising, since\\nit is known that k-NN is not Bayes-consistent in this setting. We pose several\\nchallenging open problems for future research.\\n\\n1\\n\\nIntroduction\\n\\nThis paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by\\nFix and Hodges in 1951 [16], this seemingly naive learning paradigm remains competitive against\\nmore sophisticated methods [8, 46] and, in its celebrated k-NN version, has been placed on a solid\\ntheoretical foundation [11, 44, 13, 47].\\nAlthough the classic 1-NN is well known to be inconsistent in general, in recent years a series of\\npapers has presented variations on the theme of a regularized 1-NN classifier, as an alternative to the\\nBayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can\\nact as a regularizer, actually improving generalization performance rather than just injecting noise.\\nIn a follow-up work, [27] showed that applying Structural Risk Minimization to (essentially) the\\nmargin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classifier.\\nA further development has seen margin-based regularization analyzed through the lens of sample\\ncompression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later\\nextended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in\\n[27], margin-regularized 1-NN methods enjoy a number of statistical and computational advantages\\nover the traditional k-NN classifier. Salient among these are explicit data-dependent generalization\\nbounds, and considerable runtime and memory savings. Sample compression affords additional\\nadvantages, in the form of tighter generalization bounds and increased efficiency in time and space.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\n",
              "In this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning\\nalgorithm, in both finite-dimensional and infinite-dimensional metric spaces. The algorithm is\\nessentially the passive component of the active learner proposed by Kontorovich, Sabato, and Urner\\nin [25], and we refer to it in the sequel as KSU; for completeness, we present it here in full (Alg. 1).\\nWe show that in finite-dimensional metric spaces, KSU is both computationally efficient and Bayesconsistent. This is the first compression-based multiclass 1-NN algorithm proven to possess both of\\nthese properties. We further exhibit a surprising phenomenon in infinite-dimensional spaces, where\\nwe construct a distribution for which KSU is Bayes-consistent while k-NN is not.\\nMain results. Our main contributions consist of analyzing the performance of KSU in finite and\\ninfinite dimensional settings, and comparing it to the classical k-NN learner. Our key findings are\\nsummarized below.\\n? In Theorem 2, we show that KSU is computationally efficient and strongly Bayes-consistent\\nin metric spaces with a finite doubling dimension. This is the first (strong or otherwise)\\nBayes-consistency result for an efficient sample compression scheme for a multiclass (or\\neven binary)1 1-NN algorithm. This result should be contrasted with the one in [27], where\\nmargin-based regularization was employed, but not compression; the proof techniques\\nfrom [27] do not carry over to the compression-based scheme. Instead, novel arguments\\nare required, as we discuss below. The new sample compression technique provides a\\nBayes-consistency proof for multiple (even countably many) labels; this is contrasted with\\nthe multiclass 1-NN algorithm in [28], which is not compression-based, and requires solving\\na minimum vertex cover problem, thereby imposing a 2-approximation factor whenever\\nthere are more than two labels.\\n? In Theorem 4, we make the surprising discovery that KSU continues to be Bayes-consistent\\nin a certain infinite-dimensional setting, even though this setting violates the basic measuretheoretic conditions on which classic consistency proofs hinge, including Theorem 2. This\\nis all the more surprising, since it is known that k-NN is not Bayes-consistent for this\\nconstruction [9]. We are currently unaware of any separable2 metric probability space on\\nwhich KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.\\nOur results indicate that in finite dimensions, an efficient, compression-based, Bayes-consistent\\nmulticlass 1-NN algorithm exists, and hence can be offered as an alternative to k-NN, which is well\\nknown to be Bayes-consistent in finite dimensions [12, 41]. In contrast, in infinite dimensions, our\\nresults show that the condition characterizing the Bayes-consistency of k-NN does not extend to all\\nNN algorithms. It is an open problem to characterize the necessary and sufficient conditions for the\\nexistence of a Bayes-consistent NN-based algorithm in infinite dimensions.\\nRelated work. Following the pioneering work of [11] on nearest-neighbor classification, it was\\nshown by [13, 47, 14] that the k-NN classifier is strongly Bayes consistent in Rd . These results\\nmade extensive use of the Euclidean structure of Rd , but in [41] a weak Bayes-consistency result was\\nshown for metric spaces with a bounded diameter and a bounded doubling dimension, and additional\\ndistributional smoothness assumptions. More recently, some of the classic results on k-NN risk\\ndecay rates were refined by [10] in an analysis that captures the interplay between the metric and the\\nsampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e.,\\nthe so-called curse of dimensionality), and Pestov [33, 34] examines this phenomenon closely under\\nvarious distributional and structural assumptions.\\nConsistency of NN-type algorithms in more general (and in particular infinite-dimensional) metric\\nspaces was discussed in [1, 5, 6, 9, 30]. In [1, 9], characterizations of Bayes-consistency were\\ngiven in terms of Besicovitch-type conditions (see Eq. (3)). In [1], a generalized ?moving window?\\nclassification rule is used and additional regularity conditions on the regression function are imposed.\\nThe filtering technique (i.e., taking the first d coordinates in some basis representation) was shown to\\nbe universally consistent in [5]. However, that algorithm suffers from the cost of cross-validating\\nover both the dimension d and number of neighbors k. Also, the technique is only applicable in\\n1\\nAn efficient sample compression algorithm was given in [20] for the binary case, but no Bayes-consistency\\nguarantee is known for it.\\n2\\nC?rou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor\\nmethods, including k-NN and KSU, obviously fail.\\n\\n2\\n\\n\n",
              "Hilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency,\\nwithout finite-sample bounds such as those provided by KSU. The insight of [5] is extended to the\\nmore general Banach spaces in [6] under various regularity assumptions.\\nNone of the aforementioned generalization results for NN-based techniques are in the form of\\nfully empirical, explicitly computable sample-dependent error bounds. Rather, they are stated in\\nterms of the unknown Bayes-optimal rate, and some involve additional parameters quantifying the\\nwell-behavedness of the unknown distribution (see [27] for a detailed discussion). As such, these\\nguarantees do not enable a practitioner to compute a numerical generalization error estimate for a\\ngiven training sample, much less allow for a data-dependent selection of k, which must be tuned via\\ncross-validation. The asymptotic expansions in [43, 37, 23, 40] likewise do not provide a computable\\nfinite-sample bound. The quest for such bounds was a key motivation behind the series of works\\n[18, 28, 20], of which KSU [25] is the latest development.\\nThe work of Devroye et al. [14, Theorem 21.2] has implications for 1-NN classifiers in Rd that\\nare defined based on data-dependent majority-vote partitions of the space. It is shown that under\\nsome conditions, a fixed mapping from each sample size to a data-dependent partition rule induces a\\nstrongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC\\ndimension, and since this rule must be fixed in advance, the algorithm is not fully adaptive. Theorem\\n19.3 ibid. proves weak consistency for an inefficient compression-based algorithm, which selects\\namong all the possible compression sets of a certain size, and maintains a certain rate of compression\\nrelative to the sample size. The generalizing power of sample compression was independently\\ndiscovered by [31], and later elaborated upon by [22]. In the context of NN classification, [14] lists\\nvarious condensing heuristics (which have no known performance guarantees) and leaves open the\\nalgorithmic question of how to minimize the empirical risk over all subsets of a given size.\\nThe first compression-based 1-NN algorithm with provable optimality guarantees was given in [20];\\nit was based on constructing ?-nets in spaces with a finite doubling dimension. The compression\\nsize of this construction was shown to be nearly unimprovable by an efficient algorithm unless P=NP.\\nWith ?-nets as its algorithmic engine, KSU inherits this near-optimality. The compression-based\\n1-NN paradigm was later extended to semimetrics in [21], where it was shown to survive violations\\nof the triangle inequality, while the hierarchy-based search methods that have become standard for\\nmetric spaces (such as [4, 18] and related approaches) all break down.\\nIt was shown in [27] that a margin-regularized 1-NN learner (essentially, the one proposed in [18],\\nwhich, unlike [20], did not involve sample compression) becomes strongly Bayes-consistent when the\\nmargin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based\\ntechnique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm\\nrelied on computing a minimum vertex cover, it was not possible to make it both computationally\\nefficient and Bayes-consistent when the number of lables exceeds two. An additional improvement\\nover [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence\\non the number of labels, while our compression scheme extends seamlessly to countable label spaces.\\nPaper outline. After fixing the notation and setup in Sec. 2, in Sec. 3 we present KSU, the\\ncompression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions\\nregarding KSU, together with some open problems. High-level proof sketches are given in Sec. 5 for\\nthe finite-dimensional case, and Sec. 6 for the infinite-dimensional case. Full detailed proofs can be\\nfound in [26].\\n\\n2\\n\\nSetting and Notation\\n\\nOur instance space is the metric space (X , ?), where X is the instance domain and ? is the metric.\\n(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable\\nlabel space Y. The unknown sampling distribution is a probability measure ?\\n? over X ? Y, with\\nmarginal ? over X . Denote by (X, Y ) ? ?\\n? a pair drawn according to ?\\n?. The generalization error of a\\nclassifier f : X ? Y is given by err?? (f ) := P?? (Y 6= f (X)),\\nP and its empirical error with respect to\\na labeled set S 0 ? X ? Y is given by err(f,\\nc S 0 ) := |S10 | (x,y)?S 0 1[y 6= f (x)]. The optimal Bayes\\nrisk of ?\\n? is R??? := inf err?? (f ), where the infimum is taken over all measurable classifiers f : X ? Y.\\nWe say that ?\\n? is realizable when R??? = 0. We omit the overline in ?\\n? in the sequel when there is no\\nambiguity.\\n3\\n\\n\n",
              "For a finite labeled set S ? X ? Y and any x ? X , let Xnn (x, S) be the nearest neighbor of x with\\nrespect to S and let Ynn (x, S) be the nearest neighbor label of x with respect to S:\\n(Xnn (x, S), Ynn (x, S)) := argmin ?(x, x0 ),\\n(x0 ,y 0 )?S\\n\\nwhere ties are broken arbitrarily. The 1-NN classifier induced by S is denoted by hS (x) :=\\nYnn (x, S). The set of points in S, denoted by X = {X1 , . . . , X|S| } ? X , induces\\na Voronoi partition of X , V(X) := {V1 (X), . . . , V|S| (X)}, where each Voronoi cell is\\nVi (X) := {x ? X : argminj?{1,...,|S|} ?(x, Xj ) = i}. By definition, ?x ? Vi (X), hS (x) = Yi .\\nA 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ? ?\\n?n to a labeled set Sn0 ? X ? Y,\\nyielding the 1-NN classifier hSn0 . While the classic 1-NN algorithm sets Sn0 := Sn , in this work we\\nstudy a compression-based algorithm which sets Sn0 adaptively, as discussed further below.\\nA 1-NN algorithm is strongly Bayes-consistent on ?\\n? if err(hSn0 ) converges to R? almost surely,\\nthat is P[limn?? err(hSn0 ) = R? ] = 1. An algorithm is weakly Bayes-consistent on ?\\n? if err(hSn0 )\\nconverges to R? in expectation, limn?? E[err(hSn0 )] = R? . Obviously, the former implies the\\nlatter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all\\ndistributions in the metric space.\\nA convenient property that is used when studying the Bayes-consistency of algorithms in metric\\nspaces is the doubling dimension. Denote the open ball of radius r around x by Br (x) := {x0 ?\\n?r (x) denote the corresponding closed ball. The doubling dimension of a\\nX : ?(x, x0 ) < r} and let B\\nmetric space (X , ?) is defined as follows. Let n be the smallest number such that every ball in X can\\nbe covered by n balls of half its radius, where all balls are centered at points of X . Formally,\\nn := min{n ? N : ?x ? X , r > 0, ?x1 , . . . , xn ? X s.t. Br (x) ? ?ni=1 Br/2 (xi )}.\\nThen the doubling dimension of (X , ?) is defined by ddim(X , ?) := log2 n.\\nFor an integer n, let [n] := {1, . . . , n}. Denote the set of all index vectors of length d by In,d :=\\n[n]d . Given a labeled set Sn = (Xi , Yi )i?[n] and any i = {i1 , . . . , id } ? In,d , denote the subsample of Sn indexed by i by Sn (i) := {(Xi1 , Yi1 ), . . . , (Xid , Yid )}. Similarly, for a vector Y 0 =\\n{Y10 , . . . , Yd0 } ? Y d , denote by Sn (i, Y 0 ) := {(Xi1 , Y10 ), . . . , (Xid , Yd0 )}, namely the sub-sample\\nof Sn as determined by i where the labels are replaced with Y 0 . Lastly, for i, j ? In,d , we denote\\nSn (i; j) := {(Xi1 , Yj1 ), . . . , (Xid , Yjd )}.\\n\\n3\\n\\n1-NN majority-based compression\\n\\nIn this work we consider the 1-NN majority-based compression algorithm proposed in [25], which\\nwe refer to as KSU. This algorithm is based on constructing ?-nets at different scales; for ? > 0\\nand A ? X , a set X ? A is said to be a ?-net of A if ?a ? A, ?x ? X : ?(a, x) ? ? and for all\\nx 6= x0 ? X, ?(x, x0 ) > ?.3\\nThe algorithm (see Alg. 1) operates as follows. Given an input sample Sn , whose set of points is\\ndenoted Xn = {X1 , . . . , Xn }, KSU considers all possible scales ? > 0. For each such scale it\\nconstructs a ?-net of Xn . Denote this ?-net by X(?) := {Xi1 , . . . , Xim }, where m ? m(?) denotes\\nits size and i ? i(?) := {i1 , . . . , im } ? In,m denotes the indices selected from Sn for this ?-net.\\nFor every such ?-net, the algorithm attaches the labels Y 0 ? Y 0 (?) ? Y m , which are the empirical\\nmajority-vote labels in the respective Voronoi cells in the partition V(X(?)) = {V1 , . . . , Vm }.\\nFormally, for i ? [m],\\nYi0 ? argmax |{j ? [n] | Xj ? Vi , Yj = y}|,\\n(1)\\ny?Y\\n\\nwhere ties are broken arbitrarily. This procedure creates a labeled set Sn0 (?) := Sn (i(?), Y 0 (?)) for\\nevery relevant ? ? {?(Xi , Xj ) | i, j ? [n]} \\ {0}. The algorithm then selects a single ?, denoted\\n? ? ? ?n? , and outputs hSn0 (? ? ) . The scale ? ? is selected so as to minimize a generalization error\\nbound, which upper bounds err(Sn0 (?)) with high probability. This error bound, denoted Q in the\\nalgorithm, can be derived using a compression-based analysis, as described below.\\n3\\nFor technical reasons, having to do with the construction in Sec. 6, we depart slightly from the standard\\ndefinition of a ?-net X ? A. The classic definition requires that (i) ?a ? A, ?x ? X : ?(a, x) < ? and (ii)\\n?x 6= x0 ? X : ?(x, x0 ) ? ?. In our definition, the relations < and ? in (i) and (ii) are replaced by ? and >.\\n\\n4\\n\\n\n",
              "Algorithm 1 KSU: 1-NN compression-based algorithm\\nRequire: Sample Sn = (Xi , Yi )i?[n] , confidence ?\\nEnsure: A 1-NN classifier\\n1: Let ? := {?(Xi , Xj ) | i, j ? [n]} \\ {0}\\n2: for ? ? ? do\\n3:\\nLet X(?) be a ?-net of {X1 , . . . , Xn }\\n4:\\nLet m(?) := |X(?)|\\n5:\\nFor each i ? [m(?)], let Yi0 be the majority label in Vi (X(?)) as defined in Eq. (1)\\n6:\\nSet Sn0 (?) := (X(?), Y 0 (?))\\n7: end for\\n8: Set ?(?) := err(h\\nc Sn0 (?) , Sn )\\n9: Find ?n? ? argmin??? Q(n, ?(?), 2m(?), ?), where Q is, e.g., as in Eq. (2)\\n10: Set Sn0 := Sn0 (?n? )\\n11: return hSn0\\n\\nm\\nWe say that a mapping Sn 7? Sn0 is a compression scheme if there is a function C : ??\\nm=0 (X ?Y) ?\\n2X ?Y , from sub-samples to subsets of X ? Y, such that for every Sn there exists an m and a sequence\\ni ? In,m such that Sn0 = C(Sn (i)). Given a compression scheme Sn 7? Sn0 and a matching function\\nC, we say that a specific Sn0 is an (?, m)-compression of a given Sn if Sn0 = C(Sn (i)) for some\\ni ? In,m and err(h\\nc Sn0 , Sn ) ? ?. The generalization power of compression was recognized by [17]\\nand [22]. Specifically, it was shown in [21, Theorem 8] that if the mapping Sn 7? Sn0 is a compression\\nscheme, then with probability at least 1 ? ?, for any Sn0 which is an (?, m)-compression of Sn ? ?\\n?n ,\\nwe have (omitting the constants, explicitly provided therein, which do not affect our analysis)\\ns\\nnm\\n? log(n) + log(1/?)\\nn\\nm log(n) + log(1/?)\\nerr(hSn0 ) ?\\n? + O(\\n) + O( n?m\\n). (2)\\nn?m\\nn?m\\nn?m\\n\\nDefining Q(n, ?, m, ?) as the RHS of Eq. (2) provides KSU with a compression bound. The following\\nproposition shows that KSU is a compression scheme, which enables us to use Eq. (2) with the\\nappropriate substitution.4\\nProposition 1. The mapping Sn 7? Sn0 defined by Alg. 1 is a compression scheme whose output Sn0\\nis a (err(h\\nc Sn0 ), 2|Sn0 |)-compression of Sn .\\n? i , Y?i )i?[2m] ) = (X\\n? i , Y?i+m )i?[m] , and observe that for all\\nProof. Define the function C by C((X\\n0\\nSn , we have Sn = C(Sn (i(?); j(?))), where i(?) is the ?-net index set as defined above, and\\nj(?) = {j1 , . . . , jm(?) } ? In,m(?) is some index vector such that Yi0 = Yji for every i ? [m(?)].\\nSince Yi0 is an empirical majority vote, clearly such a j exists. Under this scheme, the output Sn0 of\\nthis algorithm is a (err(h\\nc Sn0 ), 2|Sn0 |)-compression.\\nKSU is efficient, for any countable Y. Indeed, Alg. 1 has a naive runtime complexity of O(n4 ), since\\nO(n2 ) values of ? are considered and a ?-net is constructed for each one in time O(n2 ) (see [20,\\nAlgorithm 1]). Improved runtimes can be obtained, e.g., using the methods in [29, 18]. In this work\\nwe focus on the Bayes-consistency of KSU, rather than optimize its computational complexity. Our\\nBayes-consistency results below hold for KSU, whenever the generalization bound Q(n, ?, m, ?n )\\nsatisfies the following properties:\\nProperty 1 For any integer n and ? ? (0, 1), with probability 1 ? ? over the i.i.d. random sample\\nSn ? ?\\n?n , for all ? ? [0, 1] and m ? [n]: If Sn0 is an (?, m)-compression of Sn , then\\nerr(hSn0 ) ? Q(n, ?, m, ?).\\nProperty 2 Q is monotonically increasing in ? and in m.\\nProperty 3 There is a sequence {?n }?\\nn=1 , ?n ? (0, 1) such that\\nlim\\n\\nP?\\n\\nn=1 ?n\\n\\n< ? and for all m,\\n\\nsup (Q(n, ?, m, ?n ) ? ?) = 0.\\n\\nn?? ??[0,1]\\n4\\n\\nIn [25] the analysis was based on compression with side information, and does not extend to infinite Y.\\n\\n5\\n\\n\n",
              "The compression bound in Eq. (2) clearly\\nP?satisfies these properties. Note that Property 3 is satisfied\\nby Eq. (2) using any convergent series n=1 ?n < ? such that ?n = e?o(n) ; in particular, the decay\\nof ?n cannot be too rapid.\\n\\n4\\n\\nMain results\\n\\nIn this section we describe our main results. The proofs appear in subsequent sections. First, we show\\nthat KSU is Bayes-consistent if the instance space has a finite doubling dimension. This contrasts\\nwith classical 1-NN, which is only Bayes-consistent if the distribution is realizable.\\nTheorem 2. Let (X , ?) be a metric space with a finite doubling-dimension. Let Q be a generalization\\nbound that satisfies Properties 1-3, and let ?n be as stipulated by Property 3 for Q. If the input\\nconfidence ? for input size n is set to ?n , then the 1-NN classifier hSn0 (?n? ) calculated by KSU is\\nstrongly Bayes consistent on (X , ?): P(limn?? err(hSn0 ) = R? ) = 1.\\nThe proof, provided in Sec. 5, closely follows the line of reasoning in [27], where the strong Bayesconsistency of an adaptive margin-regularized 1-NN algorithm was proved, but with several crucial\\ndifferences. In particular, the generalization bounds used by KSU are purely compression-based, as\\nopposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter\\nin practice and guarantee Bayes-consistency of KSU even for countably many labels. This however\\nrequires novel technical arguments, which are discussed in detail in Appendix B.1 in [26]. Moreover,\\nsince the compression-based bounds do not explicitly depend on ddim, they can be used even when\\nddim is infinite, as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency,\\nwe note that the proof technique given here does not carry to an earlier algorithm, suggested in [20,\\nTheorem 4], which also uses ?-nets. It is an open question whether the latter is Bayes-consistent.\\nNext, we study Bayes-consistency of KSU in infinite dimensions (i.e., with ddim = ?) ? in particular, in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed, a straightforward\\napplication of [9, Lemma A.1] yields the following result.\\nTheorem 3 (C?rou and Guyader [9]). There exists an infinite dimensional separable metric space\\n(X , ?) and a realizable distribution ?\\n? over X ? {0, 1} such that no kn -NN learner satisfying\\nkn /n ? 0 when n ? ? is Bayes-consistent under ?\\n?. In particular, this holds for any space and\\nrealizable distribution ?\\n? that satisfy the following condition: The set C of points labeled 1 by ?\\n?\\nsatisfies\\n?r (x))\\n?(C ? B\\n?(C) > 0\\nand\\n?x ? C, lim\\n= 0.\\n(3)\\n?\\nr?0\\n?(Br (x))\\nSince ?(C) > 0, Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling\\nspaces, the Besicovitch covering theorem precludes such a violation [15]. In contrast, as [35, 36]\\nshow, in infinite-dimensional spaces this violation can in fact occur. Moreover, this is not an isolated\\npathology, as this property is shared by Gaussian Hilbert spaces [45].\\nAt first sight, Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.\\nHowever, the following result shows that this is not the case: KSU is Bayes-consistent on a distribution\\nwith this property.\\nTheorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly\\nBayes-consistent, while any k-NN classifier necessarily is not.\\nThe proof relies on a classic construction of Preiss [35] which satisfies Eq. (3). We show that the\\nstructure of the construction, combined with the packing and covering properties of ?-nets, imply that\\nthe majority-vote classifier induced by any ?-net with a sufficienlty small ? approaches the Bayes\\nerror. To contrast with Theorem 4, we next show that on the same construction, not all majority-vote\\nVoronoi partitions succeed. Indeed, if the packing property of ?-nets is relaxed, partition sequences\\nobstructing Bayes-consistency exist.\\nTheorem 5. For the example constructed in Theorem 4, there exists a sequence of Voronoi partitions\\nwith a vanishing diameter such that the induced true majority-vote classifiers are not Bayes consistent.\\nThe above result also stands in contrast to [14, Theorem 21.2], showing that, unlike in finite dimensions, the partitions? vanishing diameter is insufficient to establish consistency when ddim = ?. We\\nconclude the main results by posing intriguing open problems.\\n6\\n\\n\n",
              "Open problem 1. Does there exist a metric probability space on which some k-NN algorithm is\\nconsistent while KSU is not? Does there exist any separable metric space on which KSU fails?\\nOpen problem 2. C?rou and Guyader [9] distill a certain Besicovitch condition which is necessary\\nand sufficient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the\\nBesicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufficient? What is a\\nnecessary condition?\\n\\n5\\n\\nBayes-consistency of KSU in finite dimensions\\n\\nIn this section we give a high-level proof of Theorem 2, showing that KSU is strongly Bayesconsistent in finite-dimensional metric spaces. A fully detailed proof is given in Appendix B in\\n[26].\\nRecall the optimal empirical error ?n? ? ?(?n? ) and the optimal compression size m?n ? m(?n? ) as\\ncomputed by KSU. As shown in Proposition 1, the sub-sample Sn0 (?n? ) is an (?n? , 2m?n )-compression\\nof Sn . Abbreviate the compression-based generalization bound used in KSU by\\nQn (?, m) := Q(n, ?, 2m, ?n ).\\nTo show Bayes-consistency, we start by a standard decomposition of the excess error over the optimal\\nBayes into two terms:\\n\u0001\\n\u0001\\nerr(hSn0 (?n? ) ) ? R? = err(hSn0 (?n? ) ) ? Qn (?n? , m?n ) + Qn (?n? , m?n ) ? R? =: TI (n) + TII (n),\\nand show that each term decays to zero with probability one. For the first term, Property 1 for Q,\\ntogether with the Borel-Cantelli lemma, readily imply lim supn?? TI (n) ? 0 with probability one.\\nThe main challenge is showing that lim supn?? TII (n) ? 0 with probability one. We do so in\\nseveral stages:\\n1. Loosely speaking, we first show (Lemma 10) that the Bayes error R? can be well approximated using 1-NN classifiers defined by the true (as opposed to empirical) majority-vote\\nlabels over fine partitions of X . In particular, this holds for any partition induced by a ?-net\\nof X with a sufficiently small ? > 0. This approximation guarantee relies on the fact that in\\nfinite-dimensional spaces, the class of continuous functions with compact support is dense\\nin L1 (?) (Lemma 9).\\n2. Fix ?? > 0 sufficiently small such that any true majority-vote classifier induced by a ?? -net\\nhas a true error close to R? , as guaranteed by stage 1. Since for bounded subsets of finitedimensional spaces the size of any ?-net is finite, the empirical error of any majority-vote\\n?-net almost surely converges to its true majority-vote error as the sample size n ? ?. Let\\nn(?\\n? ) sufficiently large such that Qn(?? ) (?(?\\n? ), m(?\\n? )) as computed by KSU for a sample of\\n0\\nsize n(?\\n? ) is a reliable estimate for the true error of hSn(?\\n(?\\n?).\\n?)\\n3. Let ?? and n(?\\n? ) be as in stage 2. Given a sample of size n = n(?\\n? ), recall that KSU\\nselects an optimal ? ? such that Qn (?(?), m(?)) is minimized over all ? > 0. For margins\\n? \n",
              " ?? , which are prone to over-fitting, Qn (?(?), m(?)) is not a reliable estimate for\\nhSn0 (?) since compression may not yet taken place for samples of size n. Nevertheless, these\\nmargins are discarded by KSU due to the penalty term in Q. On the other hand, for ?-nets\\nwith margin ? \n",
              " ?? , which are prone to under-fitting, the true error is well estimated by\\nQn (?(?), m(?)). It follows that KSU selects ?n? ? ?? and Qn (?n? , m?n ) ? R? , implying\\nlim supn?? TII (n) ? 0 with probability one.\\nAs one can see, the assumption that X is finite-dimensional plays a major role in the proof. A simple\\nargument shows that the family of continuous functions with compact support is no longer dense\\nin L1 in infinite-dimensional spaces. In addition, ?-nets of bounded subsets in infinite dimensional\\nspaces need no longer be finite.\\n\\n6\\n\\nOn Bayes-consistency of NN algorithms in infinite dimensions\\n\\nIn this section we study the Bayes-consistency properties of 1-NN algorithms on a classic infinitedimensional construction of Preiss [35], which we describe below in detail. This construction was\\n7\\n\\n\n",
              "z1:k?2\\n?k?1\\nz1:k?1\\n?k\\n\\n?k\\nz1:k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\nz\\n\\nC = Z?\\n\\n?? (z) for some z ? C.\\nFigure 1: Preiss?s construction. Encircled is the closed ball B\\nk?1\\nfirst introduced as a concrete example showing that in infinite-dimensional spaces the Besicovich\\ncovering theorem [15] can be strongly violated, as manifested in Eq. (3).\\nExample 1 (Preiss?s construction). The construction (see Figure 1) defines an infinite-dimensional\\nmetric space (X , ?) and a realizable measure ?\\n? over X ? Y with the binary label set Y = {0, 1}.\\nIt relies on two sequences: a sequence of natural numbers {Nk }k?N and a sequence of positive\\nnumbers {ak }k?N . The two sequences should satisfy the following:\\nP?\\nlimk?? ak N1 . . . Nk+1 = ?; and limk?? Nk = ?. (4)\\nk=1 ak N1 . . . Nk = 1;\\nQ\\nThese properties are satisfied, for instance, by setting Nk := k! and ak := 2?k / i?[k] Ni . Let Z0\\nbe the set of all finite sequences (z1 , . . . , zk )k?N of natural numbers such that zi ? Ni , and let Z?\\nbe the set of all infinite sequences (z1 , z2 , . . . ) of natural numbers such that zi ? Ni .\\nDefine the example space X := Z0 ? Z? and denote ?k := 2?k , where ?? := 0. The metric ? over\\nX is defined as follows: for x, y ? X , denote by x ? y their longest common prefix. Then,\\n?(x, y) = (?|x?y| ? ?|x| ) + (?|x?y| ? ?|y| ).\\nIt can be shown (see [35]) that ?(x, y) is a metric; in fact, it embeds isometrically into the square\\nnorm metric of a Hilbert space.\\nTo define ?, the marginal measure over X , let ?? be the uniform product distribution measure\\nover Z? , that is: for all i ? N, each zi in the sequence z = (z1 , z2 , . . . ) ? Z? is independently\\ndrawn from a uniform distribution over [Ni ]. Let ?0 be an atomic measure on Z0 such that for all\\nz ? Z0 , ?0 (z) = a|z| . Clearly, the first condition in Eq. (4) implies ?0 (Z0 ) = 1. Define the marginal\\nprobability measure ? over X by\\n?A ? Z0 ? Z? ,\\n\\n?(A) := ??? (A) + (1 ? ?)?0 (A).\\n\\nIn words, an infinite sequence is drawn with probability ? (and all such sequences are equally likely),\\nor else a finite sequence is drawn (and all finite sequences of the same length are equally likely).\\nDefine the realizable distribution ?\\n? over X ? Y by setting the marginal over X to ?, and by setting\\nthe label of z ? Z? to be 1 with probability 1 and the label of z ? Z0 to be 0 with probability 1.\\nAs shown in [35], this construction satisfies Eq. (3) with C = Z? and ?(C) = ? > 0. It follows\\nfrom Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast, the following theorem\\nshows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows\\nfrom the this result.\\nTheorem 6. Assume (X , ?), Y and ?\\n? as in Example 1. KSU is weakly Bayes-consistent on ?\\n?.\\nThe proof, provided in Appendix C in [26], first characterizes the Voronoi cells for which the true\\nmajority-vote yields a significant error for the cell (Lemma 15). In finite-dimensional spaces, the total\\nmeasure of all such ?bad? cells can be made arbitrarily close to zero by taking ? to be sufficiently\\nsmall, as shown in Lemma 10 of Theorem 2. However, it is not immediately clear whether this can\\nbe achieved for the infinite dimensional construction above.\\nIndeed, we expect such bad cells, due to the unintuitive property that for any x ? C, we have\\n?? (x) ? C)/?(B\\n?? (x)) ? 0 when ? ? 0, and yet ?(C) > 0. Thus, if for example a significant\\n?(B\\n8\\n\\n\n",
              "?? (x) with\\nportion of the set C (whose label is 1) is covered by Voronoi cells of the form V = B\\nx ? C, then for all sufficiently small ?, each one of these cells will have a true majority-vote 0. Thus\\na significant portion of C would be misclassified. However, we show that by the structure of the\\nconstruction, combined with the packing and covering properties of ?-nets, we have that in any ?-net,\\nthe total measure of all these ?bad? cells goes to 0 when ? ? 0, thus yielding a consistent classifier.\\nLastly, the following theorem shows that on the same construction above, when the Voronoi partitions\\nare allowed to violate the packing property of ?-nets, Bayes-consistency does not necessarily hold.\\nTheorem 5 immediately follows from the following result.\\nTheorem 7. Assume (X , ?), Y and ?\\n? as in Example 1. There exists a sequence of Voronoi partitions\\n(Pk )k?N of X with maxV ?Pk diam(V ) ? ?k such that the sequence of true majority-vote classifiers\\n(hPk )k?N induced by these partitions is not Bayes consistent: lim inf k?? err(hPk ) = ? > 0.\\nThe proof, provided in Appendix D, constructs a sequence of Voronoi partitions, where each partition\\nPk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case, C is\\nincorrectly classified by hPk , yielding a significant error. Thus, in infinite-dimensional metric spaces,\\nthe shape of the Voronoi cells plays a fundamental role in the consistency of the partition.\\nAcknowledgments. We thank Fr?d?ric C?rou for the numerous fruitful discussions and helpful\\nfeedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science\\nFoundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel\\nScience Foundation (grant No. 555/15).\\n\\nReferences\\n[1] Christophe Abraham, G?rard Biau, and Beno?t Cadre. On the kernel rule for function classification. Ann. Inst. Statist. Math., 58(3):619?633, 2006.\\n[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics & Probability\\nLetters, 82(6):1102?1110, 2012.\\n[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic\\nCommunications in Probability, 18(3):1?7, 2013.\\n[4] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In\\nICML ?06: Proceedings of the 23rd international conference on Machine learning, pages\\n97?104, New York, NY, USA, 2006. ACM.\\n[5] G?rard Biau, Florentina Bunea, and Marten H. Wegkamp. Functional classification in Hilbert\\nspaces. IEEE Trans. Inform. Theory, 51(6):2163?2172, 2005.\\n[6] G?rard Biau, Fr?d?ric C?rou, and Arnaud Guyader. Rates of convergence of the functional\\nk-nearest neighbor estimate. IEEE Trans. Inform. Theory, 56(4):2034?2040, 2010.\\n[7] V. I. Bogachev. Measure theory. Vol. I, II. Springer-Verlag, Berlin, 2007.\\n[8] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image\\nclassification. In CVPR, 2008.\\n[9] Fr?d?ric C?rou and Arnaud Guyader. Nearest neighbor classification in infinite dimension.\\nESAIM: Probability and Statistics, 10:340?355, 2006.\\n[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In NIPS, 2014.\\n[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions\\non Information Theory, 13:21?27, 1967.\\n[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE\\nTrans. Pattern Anal. Mach. Intell., 3(1):75?78, 1981.\\n[13] Luc Devroye and L?szl? Gy?rfi. Nonparametric density estimation: the L1 view. Wiley Series\\nin Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &\\nSons, Inc., New York, 1985.\\n9\\n\\n\n",
              "[14] Luc Devroye, L?szl? Gy?rfi, and G?bor Lugosi. A probabilistic theory of pattern recognition,\\nvolume 31. Springer Science & Business Media, 2013.\\n[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wissenschaften, Band 153. Springer-Verlag New York Inc., New York, 1969.\\n[16] Evelyn Fix and Jr. Hodges, J. L. Discriminatory analysis. nonparametric discrimination:\\nConsistency properties. International Statistical Review / Revue Internationale de Statistique,\\n57(3):pp. 238?247, 1989.\\n[17] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine learning, 21(3):269?304, 1995.\\n[18] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient classification for metric\\ndata (extended abstract COLT 2010). IEEE Transactions on Information Theory, 60(9):5750?\\n5759, 2014.\\n[19] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality\\nreduction. Theoretical Computer Science, 620:105?118, 2016.\\n[20] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression\\nfor nearest neighbors. In Neural Information Processing Systems (NIPS), 2014.\\n[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Nearly optimal classification for\\nsemimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research, 2017.\\n[22] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on\\nthe prediction error of learning algorithms for classification. Machine Learning, 59(1):55?76,\\n2005.\\n[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classification. Ann.\\nStatist., 33(1):284?306, 02 2005.\\n[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its\\nApplications. Springer-Verlag, 2002.\\n[25] Aryeh Kontorovich, Sivan Sabato, and Ruth Urner. Active nearest-neighbor learning in metric\\nspaces. In Advances in Neural Information Processing Systems, pages 856?864, 2016.\\n[26] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression:\\nEfficiency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017.\\n[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classifier. In Artificial Intelligence\\nand Statistics (AISTATS 2015), 2014.\\n[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In International Conference on Machine Learning (ICML 2014), 2014.\\n[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity\\nsearch. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 791?801, January\\n2004.\\n[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation\\nunder arbitrary sampling. IEEE Trans. Inform. Theory, 41(4):1028?1039, 1995.\\n[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpublished, 1986.\\n[32] James R. Munkres. Topology: a first course. Prentice-Hall, Inc., Englewood Cliffs, N.J., 1975.\\n[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration\\nof measure. Inform. Process. Lett., 73(1-2):47?51, 2000.\\n[34] Vladimir Pestov. Is the k-NN classifier in high dimensions affected by the curse of dimensionality? Comput. Math. Appl., 65(10):1427?1437, 2013.\\n10\\n\\n\n",
              "[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis, pages\\n58?60, 1979.\\n[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin.,\\n22(1):181?193, 1981.\\n[37] Demetri Psaltis, Robert R. Snapp, and Santosh S. Venkatesh. On the finite sample performance\\nof the nearest neighbor classifier. IEEE Transactions on Information Theory, 40(3):820?837,\\n1994.\\n[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third\\nedition, 1976. International Series in Pure and Applied Mathematics.\\n[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1987.\\n[40] Richard J. Samworth. Optimal weighted nearest neighbour classifiers. Ann. Statist., 40(5):2733?\\n2763, 10 2012.\\n[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\\nAlgorithms. Cambridge University Press, 2014.\\n[42] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural\\nrisk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory,\\n44(5):1926?1940, 1998.\\n[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor\\nrisk. Ann. Statist., 26(3):850?878, 1998.\\n[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595?620,\\n1977.\\n[45] Jaroslav Ti?er. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc., 355(8):3277?\\n3289, 2003.\\n[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest\\nneighbor classification. Journal of Machine Learning Research, 10:207?244, 2009.\\n[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of\\nregression functions. J. Multivariate Anal., 21(1):168?178, 1987.\\n\\n11\\n\\n\n",
              "  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bf2c4e2-274a-4479-ad19-1737aef1f68d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1997</td>\n",
              "      <td>Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Independent Component Analysis for\\nidentification of artifacts in\\nMagnetoencephalographic recordings\\n\\nRicardo Vigario 1 ; Veikko J ousmiiki2 ,\\nMatti Hiimiiliiinen2, Riitta Hari2, and Erkki Oja 1\\n1 Lab.\\n\\nof Computer &amp; Info. Science\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{Ricardo.Vigario, Erkki.Oja}@hut.fi\\n2 Brain\\n\\nResearch Unit, Low Temperature Lab.\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{veikko, msh, hari}@neuro.hut.fi\\n\\nAbstract\\nWe have studied the application of an independent component analysis\\n(ICA) approach to the identification and possible removal of artifacts\\nfrom a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude\\ndistributions over time, thus distinguishing between strictly periodical\\nsignals, and regularly and irregularly occurring signals. Many artifacts\\nbelong to the last category. In order to assess the effectiveness of the\\nmethod, controlled artifacts were produced, which included saccadic eye\\nmovements and blinks, increased muscular tension due to biting and the\\npresence of a digital watch inside the magnetically shielded room. The\\nresults demonstrate the capability of the method to identify and clearly\\nisolate the produced artifacts.\\n\\n1 Introduction\\nWhen using a magnetoencephalographic (MEG) record, as a research or clinical tool, the\\ninvestigator may face a problem of extracting the essential features of the neuromagnetic\\n? Corresponding author\\n\\n\fR. Vigario,\\n\\n230\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\nsignals in the presence of artifacts. The amplitude of the disturbance may be higher than\\nthat of the brain signals, and the artifacts may resemble pathological signals in shape. For\\nexample, the heart's electrical activity, captured by the lowest sensors of a whole-scalp\\nmagnetometer array, may resemble epileptic spikes and slow waves (Jousmili and Hari\\n1996).\\nThe identification and eventual removal of artifacts is a common problem in electroencephalography (EEG), but has been very infrequently discussed in context to MEG (Hari\\n1993; Berg and Scherg 1994).\\nThe simplest and eventually most commonly used artifact correction method is rejection,\\nbased on discarding portions of MEG that coincide with those artifacts. Other methods\\ntend to restrict the subject from producing the artifacts (e.g. by asking the subject to fix the\\neyes on a target to avoid eye-related artifacts, or to relax to avoid muscular artifacts). The\\neffectiveness of those methods can be questionable in studies of neurological patients, or\\nother non-co-operative subjects. In eye artifact canceling, other methods are available and\\nhave recently been reviewed by Vigario (I 997b) whose method is close to the one presented\\nhere, and in Jung et aI. (1998).\\nThis paper introduces a new method to separate brain activity from artifacts, based on the\\nassumption that the brain activity and the artifacts are anatomically and physiologically\\nseparate processes, and that their independence is reflected in the statistical relation between the magnetic signals generated by those processes.\\nThe remaining of the paper will include an introduction to the independent component\\nanalysis, with a presentation of the algorithm employed and some justification of this approach. Experimental data are used to illustrate the feasibility of the technique, followed\\nby a discussion on the results.\\n\\n2\\n\\nIndependent Component Analysis\\n\\nIndependent component analysis is a useful extension of the principal component analysis\\n(PC A). It has been developed some years ago in context with blind source separation applications (Jutten and Herault 1991; Comon 1994). In PCA. the eigenvectors of the signal\\ncovariance matrix C = E{xx T } give the directions oflargest variance on the input data\\nx. The principal components found by projecting x onto those perpendicular basis vectors\\nare uncorrelated, and their directions orthogonal.\\nHowever, standard PCA is not suited for dealing with non-Gaussian data. Several authors, from the signal processing to the artificial neural network communities, have shown\\nthat information obtained from a second-order method such as PCA is not enough and\\nhigher-order statistics are needed when dealing with the more demanding restriction of\\nindependence (Jutten and Herault 1991; Comon 1994). A good tutorial on neural ICA implementations is available by Karhunen et al. (1997). The particular algorithm used in this\\nstudy was presented and derived by Hyvarinen and Oja (1997a. 1997b).\\n\\n2.1\\n\\nThe model\\n\\nIn blind source separation, the original independent sources are assumed to be unknown,\\nand we only have access to their weighted sum. In this model, the signals recorded in an\\nMEG study are noted as xk(i) (i ranging from 1 to L, the number of sensors used, and\\nk denoting discrete time); see Fig. 1. Each xk(i) is expressed as the weighted sum of M\\n\\n\fICAfor Identification of Artifacts in MEG Recordings\\n\\n231\\n\\nindependent signals Sk(j), following the vector expression:\\nM\\n\\nXk = La(j)sdj) = ASk,\\n\\n(1)\\n\\nj=l\\n\\nwhere Xk = [xk(1), ... , xk(L)]T is an L-dimensional data vector, made up of the L mixtures at discrete time k. The sk(1), ... , sk(M) are the M zero mean independent source\\nsignals, and A = [a(1), . .. , a(M)] is a mixing matrix independent of time whose elements\\nail are th.e unknown coefficients of the mixtures. In order to perform ICA, it is necessary\\nto have at least as many mixtures as there are independent sources (L ~ M). When this\\nrelation is not fully guaranteed, and the dimensionality of the problem is high enough,\\nwe should expect the first independent components to present clearly the most strongly\\nindependent signals, while the last components still consist of mixtures of the remaining\\nsignals. In our study, we did expect that the artifacts, being clearly independent from the\\nbrain activity, should come out in the first independent components. The remaining of the\\nbrain activity (e.g. a and J-L rhythms) may need some further processing.\\nThe mixing matrix A is a function of the geometry of the sources and the electrical conductivities of the brain, cerebrospinal fluid, skull and scalp. Although this matrix is unknown.\\nwe assume it to be constant, or slowly changing (to preserve some local constancy).\\nThe problem is now to estimate the independent signals Sk (j) from their mixtures, or the\\nequivalent problem of finding the separating matrix B that satisfies (see Eq. 1)\\n(2)\\nIn our algorithm, the solution uses the statistical definition of fourth-order cumulant or\\nkurtosis that, for the ith source signal, is defined as\\n\\nkurt(s(i)) = E{s(i)4} - 3[E{s(i)2}]2,\\nwhere E( s) denotes the mathematical expectation of s.\\n\\n2.2 The algorithm\\nThe initial step in source separation, using the method described in this article, is whitening, or sphering. This projection of the data is used to achieve the uncorrelation between\\nthe solutions found, which is a prerequisite of statistical independence (Hyvarinen and Oja\\n1997a). The whitening can as well be seen to ease the separation of the independent signals (Karhunen et al. 1997). It may be accomplished by PCA projection: v = V x, with\\nE{ vv T } = I. The whitening matrix V is given by\\n-=T ,\\nV -- A- 1 / 2 .....\\n\\nwhere A = diag[-\\(1), ... , -\\(M)] is a diagonal matrix with the eigenvalues of the data\\ncovariance matrix E{xxT}, and 8 a matrix with the corresponding eigenvectors as its\\ncolumns.\\nConsider a linear combination y = w T v of a sphered data vector v, with Ilwll = 1. Then\\nE{y2} = .1 andkurt(y) = E{y4}-3, whose gradientwithrespecttow is 4E{v(wTv)3} .\\nBased on this, Hyvarinen and Oja (1997a) introduced a simple and efficient fixed-point\\nalgorithm for computing ICA, calculated over sphered zero-mean vectors v, that is able to\\nfind one of the rows of the separating matrix B (noted w) and so identify one independent\\nsource at a time - the corresponding independent source can then be found using Eq. 2.\\nThis algorithm, a gradient descent over the kurtosis, is defined for a particular k as\\n1. Take a random initial vector Wo of unit norm. Let l = 1.\\n\\n\f232\\n\\nR. Vigario,\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\n2. Let Wi = E{V(W[.1 v)3} - 3Wl-I. The expectation can be estimated using a\\nlarge sample OfVk vectors (say, 1,000 vectors).\\n3. Divide Wi by its norm (e.g. the Euclidean norm\\n4.\\n\\nIlwll = JLi wI J.\\n\\nlflwT wi-II is not close enough to 1, let I = 1+1 andgo back to step 2.\\n\\nOtherwise,\\n\\noutput the vector Wi.\\n\\nIn order to estimate more than one solution, and up to a maximum of lvI, the algorithm\\nmay be run as many times as required. It is, nevertheless, necessary to remove the infonnation contained in the solutions already found, to estimate each time a different independent\\ncomponent. This can be achieved, after the fourth step of the algorithm, by simply subtracting the estimated solution s = w T v from the unsphered data Xk . As the solution is\\ndefined up to a multiplying constant, the subtracted vector must be multiplied by a vector\\ncontaining the regression coefficients over each vector component of Xk.\\n\\n3\\n\\nMethods\\n\\nThe MEG signals were recorded in a magnetically shielded room with a 122-channel\\nwhole-scalp Neuromag-122 neuromagnetometer. This device collects data at 61 locations\\nover the scalp, using orthogonal double-loop pick-up coils that couple strongly to a local\\nsource just underneath, thus making the measurement \"near-sighted\" (HamaHi.inen et al.\\n1993).\\nOne of the authors served as the subject and was seated under the magnetometer. He kept\\nhis head immobile during the measurement. He was asked to blink and make horizontal\\nsaccades, in order to produce typical ocular artifacts. Moreover, to produce myographic\\nartifacts, the subject was asked to bite his teeth for as long as 20 seconds. Yet another\\nartifact was created by placing a digital watch one meter away from the helmet into the\\nshieded room. Finally, to produce breathing artifacts, a piece of metal was placed next\\nto the navel. Vertical and horizontal electro-oculograms (VEOG and HEOG) and electrocardiogram (ECG) between both wrists were recorded simultaneously with the MEG, in\\norder to guide and ease the identification of the independent components. The bandpassfiltered MEG (0.03-90 Hz), VEOG, HEOG, and ECG (0.1-100 Hz) signals were digitized\\nat 297 Hz, and further digitally low-pass filtered, with a cutoff frequency of 45 Hz and\\ndownsampled by a factor of 2. The total length of the recording was 2 minutes. A second\\nset of recordings was perfonned, to assess the reproducibility of the results.\\nFigure 1 presents a subset of 12 spontaneous MEG signals from the frontal, temporal and\\noccipital areas. Due to the dimension of the data (122 magnetic signals were recorded), it\\nis impractical to plot all MEG signals (the complete set is available on the internet - see\\nreference list for the adress (Vigario 1997a?. Also both EOG channels and the electrocardiogram are presented.\\n\\n4\\n\\nResults\\n\\nFigure 2 shows sections of9 independent components (IC's) found from the recorded data,\\ncorresponding to a I min period, starting 1 min after the beginning of the measurements.\\nThe first two IC's, with a broad band spectrum, are clearly due to the musclular activity\\noriginated from the biting. Their separation into two components seems to correspond, on\\nthe basis of the field patterns, to two different sets of muscles that were activated during\\nthe process. IC3 and IC5 are, respectively showing the horizontal eye movements and the\\neye blinks, respectively. IC4 represents cardiac artifact that is very clearly extracted. In\\nagreement with Jousmaki and Hari (1996), the magnetic field pattern of IC4 shows some\\npredominance on the left.\\n\\n\fICA/or Identification 0/ Artifacts in MEG Recordings\\n\\n233\\nMEG [ 1000 fTlcm\\n\\nI--\\n\\n---l\\n\\nsaccades\\n\\nI--\\n\\n---l\\n\\nblinking\\n\\nEOG [\\n\\n500 IlV\\n\\nECG [\\n\\n500 IlV\\n\\nI--\\n\\nbiting\\n\\n---l MEG\\n\\n~=::::::::::::::=::\\n~?'104~\\n\\nrJ. .........\\n\\nM\\n\\n,J.\\.......1iIIiM~..\\n\\n::\\nt...\\n\\n:;::::::;:::~=\\n\\n~::::::::;::=\\n~ ?? \",~Jrt\\n\\n..,.\\n\\nt\\n\\n....\\n\\n~,.~ . ? .J..\\n\\n.\\n\\n.../\\\"\"$\"\"~I\\n\\n2 t\\n\\n:;\\n\\n:;\\n4\\n\\n~\\n\\n5 t\\n\\n., ... ...., ,'fIJ'\\,\\n..........-.\\n\\n,..,d\\n\\n,LIlt ... .,\\n\\nI?\\n\\n.,............. ................. \"\\n\\n....,..,.\"........ .\\n\\n.... Dei ..... \"\\n\\n.'''IIb'''*. rt\\n\\n-1I\\JY. ? ---\\n\\nI p\", . . . . , . . . . . . . . . . . . at ...'....\\n\\nI; rp ..\\n\\n,P....\\n\\n,\\n\\n.,...............' tMn':M.U\\n\\n... ,\\n\\n, ..... '\\n\\nU\\..,.--II..------'-__\\n\\nooII..Jl,,-\\n\\n\".'tIItS\\n\\n5 ~\\n\\n6 t\\n\\nVEOG\\n\\nIt ... 11.1. HEOG\\n\\n~UijuJJJ.LU Wl Uij.lJU.LllU.UUUllUUij,UU~ijJJJ\\n\\nECG\\n\\n10 s\\n\\nFigure 1: Samples of MEG signals, showing artifacts produced by blinking, saccades,\\nbiting and cardiac cycle. For each of the 6 positions shown, the two orthogonal directions\\nof the sensors are plotted.\\nThe breathing artifact was visible in several independent components, e.g. IC6 and IC7. It\\nis possible that, in each breathing the relative position and orientation of the metallic piece\\nwith respect to the magnetometer has changed. Therefore, the breathing artifact would be\\nassociated with more than one column of the mixing matrix A, or to a time varying mixing\\nvector.\\nTo make the analysis less sensible to the breathing artifact, and to find the remaining artifacts, the data were high-pass filtered, with cutoff frequency at 1 Hz. Next, the independent\\ncomponent IC8 was found. It shows clearly the artifact originated at the digital watch,\\nlocated to the right side of the magnetometer.\\nThe last independent component shown, relating to the first minute of the measurement,\\nshows an independent component that is related to a sensor presenting higher RMS (root\\nmean squared) noise than the others.\\n\\n5\\n\\nDiscussion\\n\\nThe present paper introduces a new approach to artifact identification from MEG recordings, based on the statistical technique of Independent Component Analysis. Using this\\nmethod, we were able to isolate both eye movement and eye blinking artifacts, as well as\\n\\n\fR. Vigario,\\n\\n234\\n\\nv. Jousmiiki, M HtJmlJliiinen, R. Hari and E. Oja\\n\\ncardiac, myographic, and respiratory artifacts.\\nThe basic asswnption made upon the data used in the study is that of independence between brain and artifact waveforms. In most cases this independence can be verified by the\\nknown differences in physiological origins of those signals. Nevertheless, in some eventrelated potential (ERP) studies (e.g. when using infrequent or painful stimuli), both the\\ncerebral and ocular signals can be similarly time-locked to the stimulus. This local time\\ndependence could in principle affect these particular ICA studies. However, as the independence between two signals is a measure of the similarity between their joint amplitude\\ndistribution and the product of each signal's distribution (calculated throughout the entire\\nsignal, and not only close to the stimulus applied), it can be expected that the very local\\nrelation between those two signals, during stimulation, will not affect their global statistical\\nrelation.\\n\\n6\\n\\nAcknowledgment\\n\\nSupported by a grant from Junta Nacional de Investiga~ao Cientifica e Tecnologica, under\\nits 'Programa PRAXIS XXI' (R.Y.) and the Academy of Finland (R.H.).\\n\\nReferences\\nBerg, P. and M. Scherg (1994). A multiple source approach to the correction of eye\\nartifacts. Electroenceph. clin. Neurophysiol. 90, 229-241.\\nComon, P. (1994). Independent component analysis - a new concept? Signal Processing 36,287-314.\\nHamalainen, M., R. Hari, R. Ilmoniemi, 1. Knuutila, and O. Y. Lounasmaa (1993, April).\\nMagnetoencephalography-theory, instrumentation, and applications to noninvasive\\nstudies of the working human brain. Reviews o/Modern Physics 65(2), 413-497.\\nHari, R. (1993). Magnetoencephalography as a tool of clinical neurophysiology. In\\nE. Niedermeyer and F. L. da Silva (Eds.), Electroencephalography. Basic principles, clinical applications, and relatedjields, pp. 1035-1061 . Baltimore: Williams\\n&amp; Wilkins.\\nHyvarinen, A. and E. Oja (l997a). A fast fixed-point algorithm for independent component analysis. Neural Computation (9), 1483-1492.\\nHyvarinen, A. and E. Oja (1997b). One-unit learning rules for independent component\\nanalysis. In Neural Information Processing Systems 9 (Proc. NIPS '96). MIT Press.\\nJousmiiki, Y. and R. Hari (1996). Cardiac artifacts in magnetoencephalogram. Journal\\no/Clinical Neurophysiology 13(2), 172-176.\\nJung, T.-P., C. Hwnphries, T.-W. Lee, S. Makeig, M. J. McKeown, Y. lragui, and\\nT. Sejnowski (1998). Extended ica removes artifacts from electroencephalographic\\nrecordings. In Neural Information Processing Systems 10 (Proc. NIPS '97). MIT\\nPress.\\nJutten, C. and 1. Herault (1991). Blind separation of sources, part i: an adaptive algorithm based on neuromimetic architecture. Signal Processing 24, 1-10.\\nKarhunen, J., E. Oja, L. Wang, R. Vigmo, and J. Joutsensalo (1997). A class of neural\\nnetworks for independent component analysis. IEEE Trans. Neural Networks 8(3),\\n1-19.\\nVigmo, R. (1997a). WWW adress for the MEG data:\\nhttp://nuc1eus.hut.firrvigarioINIPS97_data.html.\\nVigmo, R. (1997b). Extraction of ocular artifacts from eeg using independent component analysis. To appear in Electroenceph. c/in. Neurophysiol.\\n\\n\fICAfor Identification ofArtifacts in MEG Recordings\\n\\n235\\n\\n~~~\\n\\nIC1\\n\\n------,--y~-------------------------.-.------~~.. ,.. ~\\nU\\n\\n...\\n\\nIC2\\n\\nIC3\\n.\",\\n\\n''' ... '' .. '\\n\\n&lt;&gt; .\\n).\\~\\n.\\ C:&gt; ?\\n\\\\n\\n~~~}a\\n\\n~~-\"\\n\\n____I4-_. _\\n. . . ._.---_._. . . .-.__\\n. \"\"\"\"\"\"?t;_-\"'' '....\\n~\\n\\n. . . . .-......,.....\\n\\n~_1\\n\\nIC4\\n\\nIC5\\n\\nIC6\\n~\\n...W\"\\n....\\n\"1011\\n...~\"_f....\\n..\".,.\"\"'_\\n\\n/tJ'IfII/'h\\n\\nI' ......\\n\\nd1b ..\\n\\n~*W,.'tJ ......\\n\\nr' .. ns...\\n\\nIC7\\n\\nICB\\n\\nICg\\n~._-~.,.\\n\\n. . . . .t . .\\n\\nWt:n:ePWt.~..,.~I'NJ'~~\\nI\\n\\n10 s\\n\\nI\\n\\nFigure 2: Nine independent components found from the MEG data. For each component the\\nleft, back and right views of the field patterns generated by these components are shown full line stands for magnetic flux coming out from the head, and dotted line the flux inwards.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007</td>\n",
              "      <td>Near-Maximum Entropy Models for Binary Neural Representations of Natural Images</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Near-Maximum Entropy Models for Binary\\nNeural Representations of Natural Images\\n\\nMatthias Bethge and Philipp Berens\\nMax Planck Institute for Biological Cybernetics\\nSpemannstrasse 41, 72076, T?ubingen, Germany\\nmbethge,berens@tuebingen.mpg.de\\n\\nAbstract\\nMaximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these\\napproaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new\\napproach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data?the model parameters can be derived\\nin closed form and sampling is easy. Therefore, our NearMaxEnt approach can\\nserve as a tool for testing predictions from a pairwise maximum entropy model not\\nonly for low-dimensional marginals, but also for high dimensional measurements\\nof more than thousand units. We demonstrate its usefulness by studying natural\\nimages with dichotomized pixel intensities. Our results indicate that the statistics\\nof such higher-dimensional measurements exhibit additional structure that are not\\npredicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of\\ndimensionality where estimation of the full joint distribution is feasible.\\n\\n1\\n\\nIntroduction\\n\\nA core issue in sensory coding is to seek out and model statistical regularities in high-dimensional\\ndata. In particular, motivated by developments in information theory, it has been hypothesized\\nthat modeling these regularities by means of redundancy reduction constitutes an important goal of\\nearly visual processing [2]. Recent studies conjectured that the binary spike responses of retinal\\nganglion cells may be characterized completely in terms of second-order correlations when using\\na maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual\\ninput, however, this would be very surprising: Natural images are known to exhibit complex higherorder correlations which are extremely difficult to model yet being perceptually relevant. Thus, if\\nwe assume that retinal ganglion cells do not discard the information underlying these higher-order\\ncorrelations altogether, it would be a very difficult signal processing task to remove all of those\\nalready within the retinal network.\\nOftentimes, neurons involved in early visual processing are modeled as rather simple computational\\nunits akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity.\\nFor such simple neuron models, the possibility of removing higher-order correlations present in the\\ninput is very limited [3].\\nHere, we study the role of second-order correlations in the multivariate binary output statistics of\\nsuch linear-nonlinear model neurons with a threshold nonlinearity responding to natural images.\\nThat is, each unit can be described by an affine transformation zk = wkT x + ? followed by a\\npoint-wise signum function sk = sgn(zk ). Our interest in this model is twofold: (A) It can be\\nregarded a parsimonious model for the analysis of population codes of natural images for which the\\n1\\n\\n\fA\\n\\n?3\\n\\nB\\n\\n3\\n\\n0\\n\\n2\\n\\nC\\n\\n6\\n\\nJS?Divergence (bits)\\n\\n?H (%)\\n\\nx 10\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n1\\n\\nlog? H (%)\\n\\n?H (%)\\n\\n4\\n3\\n2\\n1\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n10\\n\\nD\\n10\\n\\n0.5\\n\\n0\\n\\n?5\\n\\n5\\n\\n0\\n\\n10\\n\\nx 10\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n0\\n\\n10\\n\\n?1\\n\\n10\\n\\n?2\\n\\n20\\n\\n10\\n\\nDimension\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n20\\n\\nlog 2 (Number of Samples)\\n\\nFigure 1: Similarity between the Ising and the DG model. A+C: Entropy difference ?H between the Ising\\nmodel and the Dichotomized Gaussian distribution as a function of dimensionality. A: Up to 10 dimensions\\nwe can compute HDG directly by evaluating Eq. 6. Gray dots correspond to different sets of parameters. For\\nm ? 4, the relatively large scatter and the existence of negative values is due to the limited numerical precision\\nof the Monte-Carlo integration. Errorbars show standard error of the mean. B. JS-divergence DJS between PI\\nand PDG . C. ?H as above, for higher dimensions. Up to 20 dimensions ?H remains very small. The increase\\nfor m ? 20 is most likely due to undersampling of the distributions. D. ?H as function of sample size used\\nto estimate HDG , at seven (black) and ten (grey) dimensions (note log scale on both axes). ?H decreases with\\na power law with increasing sample sizes.\\n\\ncomputational power and the bandwidth of each unit is limited. (B) The same model can also be\\nused more generally to fit multivariate binary data with given pairwise correlations, if x is drawn\\nfrom a Gaussian distribution. In particular, we will show that the resulting distribution closely\\nresembles the binary maximum entropy models known as Ising models or Boltzmann machines\\nwhich have recently become popular for the analysis of spike train recordings from retinal ganglion\\ncell responses [13, 12].\\nMotivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more general level in the following questions: are pairwise interactions enough for understanding the statistical regularities in high-dimensional natural data (given that they provide a good fit in the lowdimensional case)? If we suppose that pairwise interactions are enough, what can we say about the\\namount of redundancies in high-dimensional data? In comparison with neural spike data, natural\\nimages provide two advantages for studying these questions: 1) It is much easier to obtain large\\namounts of data with millions of samples which are less prone to nonstationarities. 2) Often differences in the higher-order statistics such as between pink noise and natural images can be recognized\\nby eye.\\n\\n2\\n\\nSecond order models for binary variables\\n\\nIn order to study whether pairwise interactions are enough to determine the statistical regularities\\nin high-dimensional data, it is necessary to be able to compute the maximum entropy distribution\\nfor large number of dimensions N . Given a set of measured statistics, maximum entropy models\\nyield a full probability distribution that is consistent with these constraints but does not impose any\\n2\\n\\n\f0.05\\n0\\n0\\n?4\\n\\n1\\n?\\n\\n2\\n\\n1\\n\\n2\\n\\nx 10\\n\\n2\\n1\\n0\\n\\n0\\n\\n?\\n\\nFigure 2: Examples of covariance matrices (A+B.) and their learned approximations (C+D) at m = 10 for\\nclarity. ? is the parameter controlling the steepness of correlation decrease. E+F. Eigenvalue spectra of both\\nmatrices. G. Entropy difference ?H and H. JS-divergence between the distribution of samples obtained from\\nthe two models at m = 7.\\n\\nadditional structure on the distribution [7]. For binary data with given mean activations ?i = hsi i\\nand correlations between neurons ?ij = hsi sj i ? hsi ihsj i, one obtains a quadratic exponential\\nprobability mass function known as the Ising model in physics or as the Boltzmann machine in\\nmachine learning.\\nCurrently all methods used to determine the parameters of such binary maximum entropy models\\nsuffer from the same drawback: since the parameters do not correspond directly to any of the measured statistics, they have to be inferred (or ?learned?) from data. In high dimensions though, this\\nposes a difficult computational problem. Therefore the characterization of complete neural circuits\\nwith possibly hundreds of neurons is still out of reach, even though analysis was recently extended\\nto up to forty neurons [14].\\nTo make the maximum entropy approach feasible in high dimensions, we propose a new strategy:\\nSampling from a ?near-maximum? entropy model that does not require any complicated learning\\nof parameters. In order to justify this approach, we verify empirically that the entropy of the full\\nprobability distributions obtained with the near-maximum entropy model are indistinguishable from\\nthose obtained with classical methods such as Gibbs sampling for up to 20 dimensions.\\n2.1\\n\\nBoltzmann machine learning\\n\\nFor a binary vector of neural activities s ? {?1, 1}m and specified ?i and ?ij the Ising model takes\\nthe form\\n?\\n?\\nm\\nX\\nX\\n1\\n1\\nPI (s) = exp ?\\nhi si +\\nJij si sj ? ,\\n(1)\\nZ\\n2\\ni=1\\ni6=j\\n\\nwhere the local fields hi and the couplings Jij have to be chosen such that hsi i = ?i and hsi sj i ?\\nhsi ihsj i = ?ij . Unfortunately, finding the correct parameters turns out to be a difficult problem\\nwhich cannot be solved in closed form.\\nTherefore, one has to resort to an optimization approach to learn the model parameters hi and Jij\\nfrom data. This problem is called Boltzmann machine learning and is based on maximization of the\\nlog-likelihood L = ln PI ({si }N\\ni=1 |h, J) [1] where N is the number of samples. The gradient of the\\nlikelihood can be computed in terms of the empirical covariance and the covariance of si and sj as\\nproduced by the current model:\\n?L\\n= hsi sj iData ? hsi sj iModel\\n?Jij\\n\\n(2)\\n\\nThe second term on the right hand side is difficult to compute, as it requires sampling from the model.\\nSince the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such\\n3\\n\\n\fFigure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from\\nthe corresponding dichotomized Gaussian distribution with equal covariance matrix (middle). It is not possible\\nto see any systematic difference between the samples from the two distributions. For comparison, this is not so\\nfor the sample from the independent model (right).\\n\\nas Gibbs sampling are employed [9] in order to approximate the required model average. This is\\ncomputationally demanding as sampling is necessary for each individual update. While efficient\\nsampling algorithms exist for special cases [6], it still remains a hard and time consuming problem\\nin the general case. Additionally, most sampling algorithms do not come with guarantees for the\\nquality of the approximation of the required average. In conclusion, parameter fitting of the Ising\\nmodel is slow and oftentimes painstaking, especially in high dimensions.\\n2.2\\n\\nModeling with the dichotomized Gaussian\\n\\nHere we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model\\nby a ?near-maximum? entropy model, for which both parameter computation and sampling is easy. A\\nvery convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian\\ndistribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated\\nfrom a hidden Gaussian variable\\nz ? N (?, ?) ,\\n\\nsi = sgn(zi ).\\n\\n(3)\\n\\nWithout loss of generality, we can assume unit variances for the Gaussian, i.e. ?ii = 1, the mean ?\\nand the covariance matrix ? of s are given by\\n?i = 2?(?i ) ? 1 ,\\n\\n?ii = 4?(?i )?(??i ) ,\\n\\n?ij = 4?(?i , ?j , ?ij ) for i 6= j\\n\\n(4)\\n\\nwhere ?(x, y, ?) = ?2 (x, y, ?) ? ?(x)?(y) . Here ? is the univariate standardized cumulative\\nGaussian distribution and ?2 its bivariate counterpart. While the computation of the model parameters was hard for the Ising model, these equations can be easily inverted to find the parameters of\\nthe hidden Gaussian distribution:\\n\u0012\\n\u0013\\n?i + 1\\n?i = ??1\\n(5)\\n2\\nDetermining ?ij generally requires to find a suitable value such that ?ij ? 4?(?i , ?j , ?ij ) = 0.\\nThis can be efficently solved by numerical computations, since the function is monotonic in ?ij\\nand has a unique\u0001 zero crossing. We obtain an especially easy case, when ?i = ?j = 0, as then\\n?ij = sin ?2 ?ij .\\nIt is also possible to evaluate the probability mass function of the DG model by numerical integration,\\nZ b1\\nZ bm\\n\u0001\\n1\\nT ?1\\nPDG (s) =\\n.\\n.\\n.\\nexp\\n?(s\\n?\\n?)\\n?\\n(s\\n?\\n?)\\n,\\n(6)\\n(2?)N/2 |?|1/2 a1\\nam\\nwhere the integration limits are chosen as ai = 0 and bi = ?, if si = 1, and ai = ?? and bi = 0,\\notherwise.\\nIn summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling\\nis easy, and (2) finding the model parameters is easy too.\\n4\\n\\n\f3\\n\\nNear-maximum entropy behavior of the dichotomized Gaussian\\ndistribution\\n\\nIn the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that\\nin many cases it can serve as a convenient approximation to the Ising model. Now, we investigate\\nhow good this approximation is. For a wide range of interaction terms and mean activations we\\nverify that the DG model closely resembles the Ising model. In particular we show that the entropy of\\nthe DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions.\\n3.1\\n\\nRandom Connectivity\\n\\nWe created randomly connected networks of varying size m, where mean activations hi and\\ninteractions\\nterms Jij were drawn from N (0, 0.4). First, we compared the entropy HI =\\nP\\n? s PI (s) log2 PI (s) of the thus specified Ising model obtained by evaluating Eq. 1 with the entropy of the DG distribution HDG computed by numerical integration1 from Eq. 6 (twenty parameter\\nsets). The entropy difference ?H = HI ? HDG was smaller than 0.002 percent of HI (Fig. 1 A,\\nnote scale) and probably within the range of the numerical integration accuracy. In addition, we\\ncomputed the Jensen-Shannon divergence DJS [PI kPDG ] = 12 (DKL [PI kM ] + DKL [PDG kM ]),\\nwhere M = 21 (PI + PDG ) [8]. We find that DJS [PI kPDG ] is extremly small up to 10 dimensions\\n(Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also\\nto have a very similar structure.\\nNext, we extended this analysis to networks of larger size and repeated the same analysis for up to\\ntwenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m ? 20 due\\nto the large number of states, we used a histogram based estimate of PDG (using 3 ? 106 samples\\nfor m &lt; 15 and 15 ? 106 samples for m ? 15). The estimate of ?H is still very small at high\\ndimensions (Fig. 1 C, below 0.5%). We also computed DJS , which scaled similarly to ?H (data\\nnot shown).\\nIn Fig. 1 C, ?H seems to increase with dimensionality. Therefore, we investigated how the estimate\\nof ?H is influenced by the number of samples used. We computed both quantities for varying numbers of samples from the DG distribution (for m = 7, 10). As ?H decreases according to a power\\nlaw with increasing m, the rise of ?H observed in Fig. 1 C is most likely due to undersampling of\\nthe distribution.\\n3.2\\n\\nSpecified covariance structure\\n\\nTo explore the relationship between the two techniques more systematically, we generated covariance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the nth\\ndiagonal is set to a constant value exp(?? ? n) (Fig. 2A and B, m = 7, 10). We varied the decay\\nparameter ?, which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E\\nand F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The covariance matrix of the samples drawn from the Ising model resembles the original very closely (Fig.\\n2C and D). We also computed the entropy of the DG model using the desired covariance structure.\\nWe estimated ?H and DJS [PG kPDG ] averaged over 10 trials with 105 samples obtained by Gibbs\\nsampling from the Ising model. ?H is very close to zero (Fig. 2G, m = 7) except for small ?s\\nand never exceeded 0.05%. Moreover, the structure of both distributions seems to be very similar as\\nwell (Fig. 2H, m = 7). At m = 10, both quantities scaled qualitatively similair (data not shown).\\nWe also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data\\nnot shown).\\nOur experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good\\napproximation to the quadratic exponential distribution for a large parameter range. In the following\\nsection, we will exploit the similarity between the two models to study how the role of second-order\\ncorrelations may change between low-dimensional and high-dimensional statistics in case of natural\\nimages.\\n1\\nFor integration, we used the mvncdf function of Matlab. For m ? 4 this function employs Monte-Carlo\\nintegration.\\n\\n5\\n\\n\fFigure 4: A: Negative log probabilities of the DG model are plotted against ground truth (red dots). Identical\\ndistributions fall on the diagonal. Data points outside the area enclosed by the dashed lines indicate significant\\ndifferences between the model and ground truth. The DG model matches the true distribution very well. For\\ncomparison the independent model is shown as well (blue crosses). B: The multi-information of the true\\ndistribution (blue dots) accurately agrees with the multi-information of the DG model (red line). Similar to\\nthe analysis in [12], we observe a power law behavior of the entropy of the independent model (black solid\\nline) and the mutli-information. Linear extrapolation (in the log-log plot) to higher dimensions is indicated by\\ndashed lines. C: Different way of presentation of the same data as in B: the joint entropy H = Hindep ? I\\n(blue dots) is plotted instead of I and the axis are in linear scale. The dashed red line represents the same\\nextrapolation as in B.\\n\\n4\\n\\nNatural images: Second order and beyond\\n\\nWe now investigate to which extent the statistics of natural images with dichotomized pixel intensities can be characterized by pairwise correlations only. In particular, we would like to know how\\nthe role of pairwise correlations opposed to higher-order correlations changes depending on the dimensionality. Thanks to the DG model introduced above, we are in the position to study the effect\\nof pairwise correlations for high-dimensional binary random variables (N ? 1000 or even larger).\\nWe use the van Hateren image database in log-intensity scale, from which we sample small image\\npatches at random positions. The threshold for the dichotomization is set to the median of pixel\\nintensities. That is, each binary variable encodes whether the corresponding pixel intensity is above\\nor below the median over the ensemble. Up to patch sizes of 4 ? 4 pixel, the true joint statistics can\\nbe assessed using nonparametric histogram methods. Before we present quantitative comparisons, it\\nis instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model\\nwith same mean and covariance (Fig. 3, middle), and from the corresponding independent model\\n(Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well.\\nIn order to quantify how well the DG model matches the true distribution, we draw two independent\\nsets of samples from each (N = 2 ? 106 for each set) and generate a scatter plot as shown in\\nFig. 4 A for 4 ? 4 image patches. Each dot corresponds to one of the 216 = 65536 possible different\\nbinary patterns. The relative frequencies of these patterns according to the DG model (red dots) and\\naccording to the independent model (blue dots) are plotted against the relative frequencies obtained\\nfrom the natural image patches. The solid diagonal line corresponds to a perfect match between\\nmodel and ground truth. The dashed lines enclose the regions within which deviations are to be\\nexpected due to the finite sampling size. Since most of the red dots fall within this region, the DG\\nmodel fits the data distribution very well.\\nP\\nWe also systematically evaluated the JS-divergence and the multi-information I[S] = k H[Sk ] ?\\nH[S] as a function of dimensionality. That is, we started with the bivariate marginal distribution\\nof two randomly selected pixels. Then we incrementally added more pixels of random location\\nuntil the random vector contains all the 16 pixels of the 4 ? 4 image patches. Independent of the\\ndimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015\\nbits. For comparison, the JS-divergence between the independent model and the true distribution\\nincreases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in\\nthe case of 16 pixels. For two independent sets of samples both drawn from natural image data the\\nJS-divergence ranges between 0.006 and 0.007 bits for 4 ? 4 patches setting the gold standard for\\nthe minimal possible JS-divergence one could achieve with any model due to finite sampling size.\\nCarrying out the same type of analysis as in [12], we make qualitatively the same observations as it\\nwas reported there: as shown above, we find a quite accurate match between the two distributions.\\n6\\n\\n\fFigure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and\\nfrom the corresponding dichotomized Gaussian distribution with equal covariance matrix (right). For the latter, the percept of typical objects is missing due to the ignorance of higher-order correlations. This striking\\ndifference is not obvious, however, at the level of 4x4 patches, for which we found an excellent match of the\\ndichotomized Gaussian to the ensemble of natural images.\\n\\nFurthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue\\ndots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings\\ncan be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12],\\ntwo claims about the higher-dimensional statistics have been based on these two observations: First,\\nthat pairwise correlations may be sufficient to determine the full statistics of binary responses, and\\nsecondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards\\nstrong order.\\nUsing natural images instead of retinal ganglion cell data, we would like to verify to what extent\\nthe low-dimensional observations can be used to support these claims about the high-dimensional\\nstatistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions\\n(dashed lines) as in [12]. The difference between the entropy of the independent model and the\\nmulti-information yields the joint entropy of the respective distribution. If the extrapolation is taken\\nseriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint\\nentropy of the neural responses approaches zero at this size?say for 7 ? 7 image patches (Fig. 4 C).\\nThough it was not taken literally, this point of ?freezing? has been pointed out in [12] as a critical\\nnetwork size at which a transition to strong order is to be expected. The meaning of this assertion,\\nhowever, is not clear. First of all, the joint entropy of a distribution can never be smaller than the\\njoint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing\\nnumber of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to\\nask more precisely how the growth rate of the joint entropy can be characterized and whether there\\nis a critical number of dimensions at which the growth rate suddenly drops. In our study with natural\\nimages, visual inspection does not indicate anything special to happen at the ?critical patch size? of\\n7 ? 7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5\\n(right) we show a sample from the DG model for 32?32 image patches (i.e. 1024 dimensions) which\\nprovides no indication for a particularly interesting change in the statistics towards strong order. The\\nexact law according to which the multi-information grows with the number of dimensions for large\\nm, however, is not easily assessed and remains to be explored.\\nFinally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions\\ndoes not hold any more in the case of large m: the samples from the true distribution at the left\\nhand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5,\\nright), indicating that pairwise correlations do not suffice to determine the full statistics of large\\nimage patches. Even if the match between the DG model and the Ising model may turn out to be\\nless accurate in high dimensions, this would not affect our conclusion. Any mismatch would only\\nintroduce more order in the DG model than justified by pairwise correlations only.\\n\\n5\\n\\nConclusion and Outlook\\n\\nWe proposed a new approach to maximum entropy modeling of binary variables, extending maximum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa7\\n\\n\frameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of\\nMonte-Carlo methods. We verified numerically that the empirical entropy of the DG model is comparable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes,\\nthe DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due\\nto the lack of independence between consecutive samples in the Gibbs sampler.\\nAlthough the Ising model and the DG model are in principle different, the match between the two\\nturns out to be surprisingly good for a large region of the parameter space. Currently, we are trying\\nto determine where the close similarity between the Ising model and the DG model breaks down.\\nIn addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal\\ndensity for Monte-Carlo methods such as importance sampling. As it is a very close approximation\\nto the Ising model, we expect this combination to yield highly efficient sampling behaviour. In\\nsummary, by linking the DG model to the Ising model, we believe that maximum entropy modeling\\nof multivariate binary random variables will become much more practical in the future.\\nWe used the DG model to investigate the role of second-order correlations in the context of sensory coding of natural images. While for small image patches the DG model provided an excellent\\nfit to the true distribution, we were able to show that this agreement breakes down in the case\\nof larger image patches. Thus caution is required when extrapolating from low-dimensional measurements to higher-dimensional distributions because higher-order correlations may be invisible in\\nlow-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be\\na promising tool for the analysis of correlated neural activities, and the DG model can facilitate its\\nuse significantly in practice.\\nAcknowledgments\\nWe thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stimulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute advice. An implementation of the DG model in Matlab and R will be avaible at our website\\nhttp://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling.\\n\\nReferences\\n[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive\\nScience, 9:147?169, 1985.\\n[2] H.B. Barlow. Sensory mechanisms, the reduction of redundancy, and intelligence. In The Mechanisation\\nof Thought Processes, pages 535?539, London: Her Majesty?s Stationery Office, 1959.\\n[3] M. Bethge. Factorial coding of natural images: How effective are linear model in removing higher-order\\ndependencies? J. Opt. Soc. Am. A, 23(6):1253?1268, June 2006.\\n[4] D.R. Cox and N. Wermuth. On some models for multivariate binary variables parallel in complexity with\\nthe multivariate gaussian distribution. Biometrika, 89:462?469, 2002.\\n[5] L.J. Emrich and M.R. Piedmonte. A method for generating high-dimensional multivariate binary variates.\\nThe American Statistician, 45(4):302?304, 1991.\\n[6] M. Huber. A bounding chain for swendsen-wang. Random Structures &amp; Algorithms, 22:53?59, 2002.\\n[7] E.T. Jaynes. Where do we stand on maximum entropy inference. In R.D. Levine and M. Tribus, editors,\\nThe Maximum Entropy Formalism. MIT Press, Cambridge, MA, 1978.\\n[8] J. Linn. Divergence measures based on the shannon entropy. IEEE Trans Inf Theory, 37:145?151, 1991.\\n[9] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press,\\n2003.\\n[10] Sheila H Nirenberg and Jonathan D Victor. Analyzing the activity of large populations of neurons: how\\ntractable is the problem? Current Opinion in Neurobiology, 17:397?400, August 2007.\\n[11] Karl Pearson. On a new method of determining correlation between a measured character a, and a character b, of which only the percentage of cases wherein b exceeds (or falls short of) a given intensity is\\nrecorded for each grade of a. Biometrika, 7:96?105, 1909.\\n[12] Elad Schneidman, Michael J Berry, Ronen Segev, and William Bialek. Weak pairwise correlations imply\\nstrongly correlated network states in a neural population. Nature, 440(7087):1007?1012, Apr 2006.\\n[13] J Shlens, JD Field, JL Gauthier, MI Grivich, D Petrusca, A Sher, AM Litke, and EJ Chichilnisky. The\\nstructure of multi-neuron firing patterns in primate retina. J Neurosci, 26(32):8254?8266, Aug 2006.\\n[14] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek. Ising models for networks of real neurons. arXiv:qbio.NC/0611072, 1:1?4, 2006.\\n\\n8\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017</td>\n",
              "      <td>Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions</td>\n",
              "      <td>Poster</td>\n",
              "      <td>6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf</td>\n",
              "      <td>We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.</td>\n",
              "      <td>Nearest-Neighbor Sample Compression:\\nEfficiency, Consistency, Infinite Dimensions\\n\\nAryeh Kontorovich\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nkaryeh@cs.bgu.ac.il\\n\\nSivan Sabato\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nsabatos@bgu.ac.il\\n\\nRoi Weiss\\nDepartment of Computer Science and Applied Mathematics\\nWeizmann Institute of Science\\nroiw@weizmann.ac.il\\n\\nAbstract\\nWe examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based\\nmulticlass learning algorithm. This algorithm is derived from sample compression\\nbounds and enjoys the statistical advantages of tight, fully empirical generalization\\nbounds, as well as the algorithmic advantages of a faster runtime and memory\\nsavings. We prove that this algorithm is strongly Bayes-consistent in metric\\nspaces with finite doubling dimension ? the first consistency result for an efficient\\nnearest-neighbor sample compression scheme. Rather surprisingly, we discover\\nthat this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which\\nclassic consistency proofs hinge are violated. This is all the more surprising, since\\nit is known that k-NN is not Bayes-consistent in this setting. We pose several\\nchallenging open problems for future research.\\n\\n1\\n\\nIntroduction\\n\\nThis paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by\\nFix and Hodges in 1951 [16], this seemingly naive learning paradigm remains competitive against\\nmore sophisticated methods [8, 46] and, in its celebrated k-NN version, has been placed on a solid\\ntheoretical foundation [11, 44, 13, 47].\\nAlthough the classic 1-NN is well known to be inconsistent in general, in recent years a series of\\npapers has presented variations on the theme of a regularized 1-NN classifier, as an alternative to the\\nBayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can\\nact as a regularizer, actually improving generalization performance rather than just injecting noise.\\nIn a follow-up work, [27] showed that applying Structural Risk Minimization to (essentially) the\\nmargin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classifier.\\nA further development has seen margin-based regularization analyzed through the lens of sample\\ncompression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later\\nextended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in\\n[27], margin-regularized 1-NN methods enjoy a number of statistical and computational advantages\\nover the traditional k-NN classifier. Salient among these are explicit data-dependent generalization\\nbounds, and considerable runtime and memory savings. Sample compression affords additional\\nadvantages, in the form of tighter generalization bounds and increased efficiency in time and space.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\fIn this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning\\nalgorithm, in both finite-dimensional and infinite-dimensional metric spaces. The algorithm is\\nessentially the passive component of the active learner proposed by Kontorovich, Sabato, and Urner\\nin [25], and we refer to it in the sequel as KSU; for completeness, we present it here in full (Alg. 1).\\nWe show that in finite-dimensional metric spaces, KSU is both computationally efficient and Bayesconsistent. This is the first compression-based multiclass 1-NN algorithm proven to possess both of\\nthese properties. We further exhibit a surprising phenomenon in infinite-dimensional spaces, where\\nwe construct a distribution for which KSU is Bayes-consistent while k-NN is not.\\nMain results. Our main contributions consist of analyzing the performance of KSU in finite and\\ninfinite dimensional settings, and comparing it to the classical k-NN learner. Our key findings are\\nsummarized below.\\n? In Theorem 2, we show that KSU is computationally efficient and strongly Bayes-consistent\\nin metric spaces with a finite doubling dimension. This is the first (strong or otherwise)\\nBayes-consistency result for an efficient sample compression scheme for a multiclass (or\\neven binary)1 1-NN algorithm. This result should be contrasted with the one in [27], where\\nmargin-based regularization was employed, but not compression; the proof techniques\\nfrom [27] do not carry over to the compression-based scheme. Instead, novel arguments\\nare required, as we discuss below. The new sample compression technique provides a\\nBayes-consistency proof for multiple (even countably many) labels; this is contrasted with\\nthe multiclass 1-NN algorithm in [28], which is not compression-based, and requires solving\\na minimum vertex cover problem, thereby imposing a 2-approximation factor whenever\\nthere are more than two labels.\\n? In Theorem 4, we make the surprising discovery that KSU continues to be Bayes-consistent\\nin a certain infinite-dimensional setting, even though this setting violates the basic measuretheoretic conditions on which classic consistency proofs hinge, including Theorem 2. This\\nis all the more surprising, since it is known that k-NN is not Bayes-consistent for this\\nconstruction [9]. We are currently unaware of any separable2 metric probability space on\\nwhich KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.\\nOur results indicate that in finite dimensions, an efficient, compression-based, Bayes-consistent\\nmulticlass 1-NN algorithm exists, and hence can be offered as an alternative to k-NN, which is well\\nknown to be Bayes-consistent in finite dimensions [12, 41]. In contrast, in infinite dimensions, our\\nresults show that the condition characterizing the Bayes-consistency of k-NN does not extend to all\\nNN algorithms. It is an open problem to characterize the necessary and sufficient conditions for the\\nexistence of a Bayes-consistent NN-based algorithm in infinite dimensions.\\nRelated work. Following the pioneering work of [11] on nearest-neighbor classification, it was\\nshown by [13, 47, 14] that the k-NN classifier is strongly Bayes consistent in Rd . These results\\nmade extensive use of the Euclidean structure of Rd , but in [41] a weak Bayes-consistency result was\\nshown for metric spaces with a bounded diameter and a bounded doubling dimension, and additional\\ndistributional smoothness assumptions. More recently, some of the classic results on k-NN risk\\ndecay rates were refined by [10] in an analysis that captures the interplay between the metric and the\\nsampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e.,\\nthe so-called curse of dimensionality), and Pestov [33, 34] examines this phenomenon closely under\\nvarious distributional and structural assumptions.\\nConsistency of NN-type algorithms in more general (and in particular infinite-dimensional) metric\\nspaces was discussed in [1, 5, 6, 9, 30]. In [1, 9], characterizations of Bayes-consistency were\\ngiven in terms of Besicovitch-type conditions (see Eq. (3)). In [1], a generalized ?moving window?\\nclassification rule is used and additional regularity conditions on the regression function are imposed.\\nThe filtering technique (i.e., taking the first d coordinates in some basis representation) was shown to\\nbe universally consistent in [5]. However, that algorithm suffers from the cost of cross-validating\\nover both the dimension d and number of neighbors k. Also, the technique is only applicable in\\n1\\nAn efficient sample compression algorithm was given in [20] for the binary case, but no Bayes-consistency\\nguarantee is known for it.\\n2\\nC?rou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor\\nmethods, including k-NN and KSU, obviously fail.\\n\\n2\\n\\n\fHilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency,\\nwithout finite-sample bounds such as those provided by KSU. The insight of [5] is extended to the\\nmore general Banach spaces in [6] under various regularity assumptions.\\nNone of the aforementioned generalization results for NN-based techniques are in the form of\\nfully empirical, explicitly computable sample-dependent error bounds. Rather, they are stated in\\nterms of the unknown Bayes-optimal rate, and some involve additional parameters quantifying the\\nwell-behavedness of the unknown distribution (see [27] for a detailed discussion). As such, these\\nguarantees do not enable a practitioner to compute a numerical generalization error estimate for a\\ngiven training sample, much less allow for a data-dependent selection of k, which must be tuned via\\ncross-validation. The asymptotic expansions in [43, 37, 23, 40] likewise do not provide a computable\\nfinite-sample bound. The quest for such bounds was a key motivation behind the series of works\\n[18, 28, 20], of which KSU [25] is the latest development.\\nThe work of Devroye et al. [14, Theorem 21.2] has implications for 1-NN classifiers in Rd that\\nare defined based on data-dependent majority-vote partitions of the space. It is shown that under\\nsome conditions, a fixed mapping from each sample size to a data-dependent partition rule induces a\\nstrongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC\\ndimension, and since this rule must be fixed in advance, the algorithm is not fully adaptive. Theorem\\n19.3 ibid. proves weak consistency for an inefficient compression-based algorithm, which selects\\namong all the possible compression sets of a certain size, and maintains a certain rate of compression\\nrelative to the sample size. The generalizing power of sample compression was independently\\ndiscovered by [31], and later elaborated upon by [22]. In the context of NN classification, [14] lists\\nvarious condensing heuristics (which have no known performance guarantees) and leaves open the\\nalgorithmic question of how to minimize the empirical risk over all subsets of a given size.\\nThe first compression-based 1-NN algorithm with provable optimality guarantees was given in [20];\\nit was based on constructing ?-nets in spaces with a finite doubling dimension. The compression\\nsize of this construction was shown to be nearly unimprovable by an efficient algorithm unless P=NP.\\nWith ?-nets as its algorithmic engine, KSU inherits this near-optimality. The compression-based\\n1-NN paradigm was later extended to semimetrics in [21], where it was shown to survive violations\\nof the triangle inequality, while the hierarchy-based search methods that have become standard for\\nmetric spaces (such as [4, 18] and related approaches) all break down.\\nIt was shown in [27] that a margin-regularized 1-NN learner (essentially, the one proposed in [18],\\nwhich, unlike [20], did not involve sample compression) becomes strongly Bayes-consistent when the\\nmargin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based\\ntechnique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm\\nrelied on computing a minimum vertex cover, it was not possible to make it both computationally\\nefficient and Bayes-consistent when the number of lables exceeds two. An additional improvement\\nover [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence\\non the number of labels, while our compression scheme extends seamlessly to countable label spaces.\\nPaper outline. After fixing the notation and setup in Sec. 2, in Sec. 3 we present KSU, the\\ncompression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions\\nregarding KSU, together with some open problems. High-level proof sketches are given in Sec. 5 for\\nthe finite-dimensional case, and Sec. 6 for the infinite-dimensional case. Full detailed proofs can be\\nfound in [26].\\n\\n2\\n\\nSetting and Notation\\n\\nOur instance space is the metric space (X , ?), where X is the instance domain and ? is the metric.\\n(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable\\nlabel space Y. The unknown sampling distribution is a probability measure ?\\n? over X ? Y, with\\nmarginal ? over X . Denote by (X, Y ) ? ?\\n? a pair drawn according to ?\\n?. The generalization error of a\\nclassifier f : X ? Y is given by err?? (f ) := P?? (Y 6= f (X)),\\nP and its empirical error with respect to\\na labeled set S 0 ? X ? Y is given by err(f,\\nc S 0 ) := |S10 | (x,y)?S 0 1[y 6= f (x)]. The optimal Bayes\\nrisk of ?\\n? is R??? := inf err?? (f ), where the infimum is taken over all measurable classifiers f : X ? Y.\\nWe say that ?\\n? is realizable when R??? = 0. We omit the overline in ?\\n? in the sequel when there is no\\nambiguity.\\n3\\n\\n\fFor a finite labeled set S ? X ? Y and any x ? X , let Xnn (x, S) be the nearest neighbor of x with\\nrespect to S and let Ynn (x, S) be the nearest neighbor label of x with respect to S:\\n(Xnn (x, S), Ynn (x, S)) := argmin ?(x, x0 ),\\n(x0 ,y 0 )?S\\n\\nwhere ties are broken arbitrarily. The 1-NN classifier induced by S is denoted by hS (x) :=\\nYnn (x, S). The set of points in S, denoted by X = {X1 , . . . , X|S| } ? X , induces\\na Voronoi partition of X , V(X) := {V1 (X), . . . , V|S| (X)}, where each Voronoi cell is\\nVi (X) := {x ? X : argminj?{1,...,|S|} ?(x, Xj ) = i}. By definition, ?x ? Vi (X), hS (x) = Yi .\\nA 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ? ?\\n?n to a labeled set Sn0 ? X ? Y,\\nyielding the 1-NN classifier hSn0 . While the classic 1-NN algorithm sets Sn0 := Sn , in this work we\\nstudy a compression-based algorithm which sets Sn0 adaptively, as discussed further below.\\nA 1-NN algorithm is strongly Bayes-consistent on ?\\n? if err(hSn0 ) converges to R? almost surely,\\nthat is P[limn?? err(hSn0 ) = R? ] = 1. An algorithm is weakly Bayes-consistent on ?\\n? if err(hSn0 )\\nconverges to R? in expectation, limn?? E[err(hSn0 )] = R? . Obviously, the former implies the\\nlatter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all\\ndistributions in the metric space.\\nA convenient property that is used when studying the Bayes-consistency of algorithms in metric\\nspaces is the doubling dimension. Denote the open ball of radius r around x by Br (x) := {x0 ?\\n?r (x) denote the corresponding closed ball. The doubling dimension of a\\nX : ?(x, x0 ) &lt; r} and let B\\nmetric space (X , ?) is defined as follows. Let n be the smallest number such that every ball in X can\\nbe covered by n balls of half its radius, where all balls are centered at points of X . Formally,\\nn := min{n ? N : ?x ? X , r &gt; 0, ?x1 , . . . , xn ? X s.t. Br (x) ? ?ni=1 Br/2 (xi )}.\\nThen the doubling dimension of (X , ?) is defined by ddim(X , ?) := log2 n.\\nFor an integer n, let [n] := {1, . . . , n}. Denote the set of all index vectors of length d by In,d :=\\n[n]d . Given a labeled set Sn = (Xi , Yi )i?[n] and any i = {i1 , . . . , id } ? In,d , denote the subsample of Sn indexed by i by Sn (i) := {(Xi1 , Yi1 ), . . . , (Xid , Yid )}. Similarly, for a vector Y 0 =\\n{Y10 , . . . , Yd0 } ? Y d , denote by Sn (i, Y 0 ) := {(Xi1 , Y10 ), . . . , (Xid , Yd0 )}, namely the sub-sample\\nof Sn as determined by i where the labels are replaced with Y 0 . Lastly, for i, j ? In,d , we denote\\nSn (i; j) := {(Xi1 , Yj1 ), . . . , (Xid , Yjd )}.\\n\\n3\\n\\n1-NN majority-based compression\\n\\nIn this work we consider the 1-NN majority-based compression algorithm proposed in [25], which\\nwe refer to as KSU. This algorithm is based on constructing ?-nets at different scales; for ? &gt; 0\\nand A ? X , a set X ? A is said to be a ?-net of A if ?a ? A, ?x ? X : ?(a, x) ? ? and for all\\nx 6= x0 ? X, ?(x, x0 ) &gt; ?.3\\nThe algorithm (see Alg. 1) operates as follows. Given an input sample Sn , whose set of points is\\ndenoted Xn = {X1 , . . . , Xn }, KSU considers all possible scales ? &gt; 0. For each such scale it\\nconstructs a ?-net of Xn . Denote this ?-net by X(?) := {Xi1 , . . . , Xim }, where m ? m(?) denotes\\nits size and i ? i(?) := {i1 , . . . , im } ? In,m denotes the indices selected from Sn for this ?-net.\\nFor every such ?-net, the algorithm attaches the labels Y 0 ? Y 0 (?) ? Y m , which are the empirical\\nmajority-vote labels in the respective Voronoi cells in the partition V(X(?)) = {V1 , . . . , Vm }.\\nFormally, for i ? [m],\\nYi0 ? argmax |{j ? [n] | Xj ? Vi , Yj = y}|,\\n(1)\\ny?Y\\n\\nwhere ties are broken arbitrarily. This procedure creates a labeled set Sn0 (?) := Sn (i(?), Y 0 (?)) for\\nevery relevant ? ? {?(Xi , Xj ) | i, j ? [n]} \\ {0}. The algorithm then selects a single ?, denoted\\n? ? ? ?n? , and outputs hSn0 (? ? ) . The scale ? ? is selected so as to minimize a generalization error\\nbound, which upper bounds err(Sn0 (?)) with high probability. This error bound, denoted Q in the\\nalgorithm, can be derived using a compression-based analysis, as described below.\\n3\\nFor technical reasons, having to do with the construction in Sec. 6, we depart slightly from the standard\\ndefinition of a ?-net X ? A. The classic definition requires that (i) ?a ? A, ?x ? X : ?(a, x) &lt; ? and (ii)\\n?x 6= x0 ? X : ?(x, x0 ) ? ?. In our definition, the relations &lt; and ? in (i) and (ii) are replaced by ? and &gt;.\\n\\n4\\n\\n\fAlgorithm 1 KSU: 1-NN compression-based algorithm\\nRequire: Sample Sn = (Xi , Yi )i?[n] , confidence ?\\nEnsure: A 1-NN classifier\\n1: Let ? := {?(Xi , Xj ) | i, j ? [n]} \\ {0}\\n2: for ? ? ? do\\n3:\\nLet X(?) be a ?-net of {X1 , . . . , Xn }\\n4:\\nLet m(?) := |X(?)|\\n5:\\nFor each i ? [m(?)], let Yi0 be the majority label in Vi (X(?)) as defined in Eq. (1)\\n6:\\nSet Sn0 (?) := (X(?), Y 0 (?))\\n7: end for\\n8: Set ?(?) := err(h\\nc Sn0 (?) , Sn )\\n9: Find ?n? ? argmin??? Q(n, ?(?), 2m(?), ?), where Q is, e.g., as in Eq. (2)\\n10: Set Sn0 := Sn0 (?n? )\\n11: return hSn0\\n\\nm\\nWe say that a mapping Sn 7? Sn0 is a compression scheme if there is a function C : ??\\nm=0 (X ?Y) ?\\n2X ?Y , from sub-samples to subsets of X ? Y, such that for every Sn there exists an m and a sequence\\ni ? In,m such that Sn0 = C(Sn (i)). Given a compression scheme Sn 7? Sn0 and a matching function\\nC, we say that a specific Sn0 is an (?, m)-compression of a given Sn if Sn0 = C(Sn (i)) for some\\ni ? In,m and err(h\\nc Sn0 , Sn ) ? ?. The generalization power of compression was recognized by [17]\\nand [22]. Specifically, it was shown in [21, Theorem 8] that if the mapping Sn 7? Sn0 is a compression\\nscheme, then with probability at least 1 ? ?, for any Sn0 which is an (?, m)-compression of Sn ? ?\\n?n ,\\nwe have (omitting the constants, explicitly provided therein, which do not affect our analysis)\\ns\\nnm\\n? log(n) + log(1/?)\\nn\\nm log(n) + log(1/?)\\nerr(hSn0 ) ?\\n? + O(\\n) + O( n?m\\n). (2)\\nn?m\\nn?m\\nn?m\\n\\nDefining Q(n, ?, m, ?) as the RHS of Eq. (2) provides KSU with a compression bound. The following\\nproposition shows that KSU is a compression scheme, which enables us to use Eq. (2) with the\\nappropriate substitution.4\\nProposition 1. The mapping Sn 7? Sn0 defined by Alg. 1 is a compression scheme whose output Sn0\\nis a (err(h\\nc Sn0 ), 2|Sn0 |)-compression of Sn .\\n? i , Y?i )i?[2m] ) = (X\\n? i , Y?i+m )i?[m] , and observe that for all\\nProof. Define the function C by C((X\\n0\\nSn , we have Sn = C(Sn (i(?); j(?))), where i(?) is the ?-net index set as defined above, and\\nj(?) = {j1 , . . . , jm(?) } ? In,m(?) is some index vector such that Yi0 = Yji for every i ? [m(?)].\\nSince Yi0 is an empirical majority vote, clearly such a j exists. Under this scheme, the output Sn0 of\\nthis algorithm is a (err(h\\nc Sn0 ), 2|Sn0 |)-compression.\\nKSU is efficient, for any countable Y. Indeed, Alg. 1 has a naive runtime complexity of O(n4 ), since\\nO(n2 ) values of ? are considered and a ?-net is constructed for each one in time O(n2 ) (see [20,\\nAlgorithm 1]). Improved runtimes can be obtained, e.g., using the methods in [29, 18]. In this work\\nwe focus on the Bayes-consistency of KSU, rather than optimize its computational complexity. Our\\nBayes-consistency results below hold for KSU, whenever the generalization bound Q(n, ?, m, ?n )\\nsatisfies the following properties:\\nProperty 1 For any integer n and ? ? (0, 1), with probability 1 ? ? over the i.i.d. random sample\\nSn ? ?\\n?n , for all ? ? [0, 1] and m ? [n]: If Sn0 is an (?, m)-compression of Sn , then\\nerr(hSn0 ) ? Q(n, ?, m, ?).\\nProperty 2 Q is monotonically increasing in ? and in m.\\nProperty 3 There is a sequence {?n }?\\nn=1 , ?n ? (0, 1) such that\\nlim\\n\\nP?\\n\\nn=1 ?n\\n\\n&lt; ? and for all m,\\n\\nsup (Q(n, ?, m, ?n ) ? ?) = 0.\\n\\nn?? ??[0,1]\\n4\\n\\nIn [25] the analysis was based on compression with side information, and does not extend to infinite Y.\\n\\n5\\n\\n\fThe compression bound in Eq. (2) clearly\\nP?satisfies these properties. Note that Property 3 is satisfied\\nby Eq. (2) using any convergent series n=1 ?n &lt; ? such that ?n = e?o(n) ; in particular, the decay\\nof ?n cannot be too rapid.\\n\\n4\\n\\nMain results\\n\\nIn this section we describe our main results. The proofs appear in subsequent sections. First, we show\\nthat KSU is Bayes-consistent if the instance space has a finite doubling dimension. This contrasts\\nwith classical 1-NN, which is only Bayes-consistent if the distribution is realizable.\\nTheorem 2. Let (X , ?) be a metric space with a finite doubling-dimension. Let Q be a generalization\\nbound that satisfies Properties 1-3, and let ?n be as stipulated by Property 3 for Q. If the input\\nconfidence ? for input size n is set to ?n , then the 1-NN classifier hSn0 (?n? ) calculated by KSU is\\nstrongly Bayes consistent on (X , ?): P(limn?? err(hSn0 ) = R? ) = 1.\\nThe proof, provided in Sec. 5, closely follows the line of reasoning in [27], where the strong Bayesconsistency of an adaptive margin-regularized 1-NN algorithm was proved, but with several crucial\\ndifferences. In particular, the generalization bounds used by KSU are purely compression-based, as\\nopposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter\\nin practice and guarantee Bayes-consistency of KSU even for countably many labels. This however\\nrequires novel technical arguments, which are discussed in detail in Appendix B.1 in [26]. Moreover,\\nsince the compression-based bounds do not explicitly depend on ddim, they can be used even when\\nddim is infinite, as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency,\\nwe note that the proof technique given here does not carry to an earlier algorithm, suggested in [20,\\nTheorem 4], which also uses ?-nets. It is an open question whether the latter is Bayes-consistent.\\nNext, we study Bayes-consistency of KSU in infinite dimensions (i.e., with ddim = ?) ? in particular, in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed, a straightforward\\napplication of [9, Lemma A.1] yields the following result.\\nTheorem 3 (C?rou and Guyader [9]). There exists an infinite dimensional separable metric space\\n(X , ?) and a realizable distribution ?\\n? over X ? {0, 1} such that no kn -NN learner satisfying\\nkn /n ? 0 when n ? ? is Bayes-consistent under ?\\n?. In particular, this holds for any space and\\nrealizable distribution ?\\n? that satisfy the following condition: The set C of points labeled 1 by ?\\n?\\nsatisfies\\n?r (x))\\n?(C ? B\\n?(C) &gt; 0\\nand\\n?x ? C, lim\\n= 0.\\n(3)\\n?\\nr?0\\n?(Br (x))\\nSince ?(C) &gt; 0, Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling\\nspaces, the Besicovitch covering theorem precludes such a violation [15]. In contrast, as [35, 36]\\nshow, in infinite-dimensional spaces this violation can in fact occur. Moreover, this is not an isolated\\npathology, as this property is shared by Gaussian Hilbert spaces [45].\\nAt first sight, Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.\\nHowever, the following result shows that this is not the case: KSU is Bayes-consistent on a distribution\\nwith this property.\\nTheorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly\\nBayes-consistent, while any k-NN classifier necessarily is not.\\nThe proof relies on a classic construction of Preiss [35] which satisfies Eq. (3). We show that the\\nstructure of the construction, combined with the packing and covering properties of ?-nets, imply that\\nthe majority-vote classifier induced by any ?-net with a sufficienlty small ? approaches the Bayes\\nerror. To contrast with Theorem 4, we next show that on the same construction, not all majority-vote\\nVoronoi partitions succeed. Indeed, if the packing property of ?-nets is relaxed, partition sequences\\nobstructing Bayes-consistency exist.\\nTheorem 5. For the example constructed in Theorem 4, there exists a sequence of Voronoi partitions\\nwith a vanishing diameter such that the induced true majority-vote classifiers are not Bayes consistent.\\nThe above result also stands in contrast to [14, Theorem 21.2], showing that, unlike in finite dimensions, the partitions? vanishing diameter is insufficient to establish consistency when ddim = ?. We\\nconclude the main results by posing intriguing open problems.\\n6\\n\\n\fOpen problem 1. Does there exist a metric probability space on which some k-NN algorithm is\\nconsistent while KSU is not? Does there exist any separable metric space on which KSU fails?\\nOpen problem 2. C?rou and Guyader [9] distill a certain Besicovitch condition which is necessary\\nand sufficient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the\\nBesicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufficient? What is a\\nnecessary condition?\\n\\n5\\n\\nBayes-consistency of KSU in finite dimensions\\n\\nIn this section we give a high-level proof of Theorem 2, showing that KSU is strongly Bayesconsistent in finite-dimensional metric spaces. A fully detailed proof is given in Appendix B in\\n[26].\\nRecall the optimal empirical error ?n? ? ?(?n? ) and the optimal compression size m?n ? m(?n? ) as\\ncomputed by KSU. As shown in Proposition 1, the sub-sample Sn0 (?n? ) is an (?n? , 2m?n )-compression\\nof Sn . Abbreviate the compression-based generalization bound used in KSU by\\nQn (?, m) := Q(n, ?, 2m, ?n ).\\nTo show Bayes-consistency, we start by a standard decomposition of the excess error over the optimal\\nBayes into two terms:\\n\u0001\\n\u0001\\nerr(hSn0 (?n? ) ) ? R? = err(hSn0 (?n? ) ) ? Qn (?n? , m?n ) + Qn (?n? , m?n ) ? R? =: TI (n) + TII (n),\\nand show that each term decays to zero with probability one. For the first term, Property 1 for Q,\\ntogether with the Borel-Cantelli lemma, readily imply lim supn?? TI (n) ? 0 with probability one.\\nThe main challenge is showing that lim supn?? TII (n) ? 0 with probability one. We do so in\\nseveral stages:\\n1. Loosely speaking, we first show (Lemma 10) that the Bayes error R? can be well approximated using 1-NN classifiers defined by the true (as opposed to empirical) majority-vote\\nlabels over fine partitions of X . In particular, this holds for any partition induced by a ?-net\\nof X with a sufficiently small ? &gt; 0. This approximation guarantee relies on the fact that in\\nfinite-dimensional spaces, the class of continuous functions with compact support is dense\\nin L1 (?) (Lemma 9).\\n2. Fix ?? &gt; 0 sufficiently small such that any true majority-vote classifier induced by a ?? -net\\nhas a true error close to R? , as guaranteed by stage 1. Since for bounded subsets of finitedimensional spaces the size of any ?-net is finite, the empirical error of any majority-vote\\n?-net almost surely converges to its true majority-vote error as the sample size n ? ?. Let\\nn(?\\n? ) sufficiently large such that Qn(?? ) (?(?\\n? ), m(?\\n? )) as computed by KSU for a sample of\\n0\\nsize n(?\\n? ) is a reliable estimate for the true error of hSn(?\\n(?\\n?).\\n?)\\n3. Let ?? and n(?\\n? ) be as in stage 2. Given a sample of size n = n(?\\n? ), recall that KSU\\nselects an optimal ? ? such that Qn (?(?), m(?)) is minimized over all ? &gt; 0. For margins\\n? \u001c ?? , which are prone to over-fitting, Qn (?(?), m(?)) is not a reliable estimate for\\nhSn0 (?) since compression may not yet taken place for samples of size n. Nevertheless, these\\nmargins are discarded by KSU due to the penalty term in Q. On the other hand, for ?-nets\\nwith margin ? \u001d ?? , which are prone to under-fitting, the true error is well estimated by\\nQn (?(?), m(?)). It follows that KSU selects ?n? ? ?? and Qn (?n? , m?n ) ? R? , implying\\nlim supn?? TII (n) ? 0 with probability one.\\nAs one can see, the assumption that X is finite-dimensional plays a major role in the proof. A simple\\nargument shows that the family of continuous functions with compact support is no longer dense\\nin L1 in infinite-dimensional spaces. In addition, ?-nets of bounded subsets in infinite dimensional\\nspaces need no longer be finite.\\n\\n6\\n\\nOn Bayes-consistency of NN algorithms in infinite dimensions\\n\\nIn this section we study the Bayes-consistency properties of 1-NN algorithms on a classic infinitedimensional construction of Preiss [35], which we describe below in detail. This construction was\\n7\\n\\n\fz1:k?2\\n?k?1\\nz1:k?1\\n?k\\n\\n?k\\nz1:k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\nz\\n\\nC = Z?\\n\\n?? (z) for some z ? C.\\nFigure 1: Preiss?s construction. Encircled is the closed ball B\\nk?1\\nfirst introduced as a concrete example showing that in infinite-dimensional spaces the Besicovich\\ncovering theorem [15] can be strongly violated, as manifested in Eq. (3).\\nExample 1 (Preiss?s construction). The construction (see Figure 1) defines an infinite-dimensional\\nmetric space (X , ?) and a realizable measure ?\\n? over X ? Y with the binary label set Y = {0, 1}.\\nIt relies on two sequences: a sequence of natural numbers {Nk }k?N and a sequence of positive\\nnumbers {ak }k?N . The two sequences should satisfy the following:\\nP?\\nlimk?? ak N1 . . . Nk+1 = ?; and limk?? Nk = ?. (4)\\nk=1 ak N1 . . . Nk = 1;\\nQ\\nThese properties are satisfied, for instance, by setting Nk := k! and ak := 2?k / i?[k] Ni . Let Z0\\nbe the set of all finite sequences (z1 , . . . , zk )k?N of natural numbers such that zi ? Ni , and let Z?\\nbe the set of all infinite sequences (z1 , z2 , . . . ) of natural numbers such that zi ? Ni .\\nDefine the example space X := Z0 ? Z? and denote ?k := 2?k , where ?? := 0. The metric ? over\\nX is defined as follows: for x, y ? X , denote by x ? y their longest common prefix. Then,\\n?(x, y) = (?|x?y| ? ?|x| ) + (?|x?y| ? ?|y| ).\\nIt can be shown (see [35]) that ?(x, y) is a metric; in fact, it embeds isometrically into the square\\nnorm metric of a Hilbert space.\\nTo define ?, the marginal measure over X , let ?? be the uniform product distribution measure\\nover Z? , that is: for all i ? N, each zi in the sequence z = (z1 , z2 , . . . ) ? Z? is independently\\ndrawn from a uniform distribution over [Ni ]. Let ?0 be an atomic measure on Z0 such that for all\\nz ? Z0 , ?0 (z) = a|z| . Clearly, the first condition in Eq. (4) implies ?0 (Z0 ) = 1. Define the marginal\\nprobability measure ? over X by\\n?A ? Z0 ? Z? ,\\n\\n?(A) := ??? (A) + (1 ? ?)?0 (A).\\n\\nIn words, an infinite sequence is drawn with probability ? (and all such sequences are equally likely),\\nor else a finite sequence is drawn (and all finite sequences of the same length are equally likely).\\nDefine the realizable distribution ?\\n? over X ? Y by setting the marginal over X to ?, and by setting\\nthe label of z ? Z? to be 1 with probability 1 and the label of z ? Z0 to be 0 with probability 1.\\nAs shown in [35], this construction satisfies Eq. (3) with C = Z? and ?(C) = ? &gt; 0. It follows\\nfrom Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast, the following theorem\\nshows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows\\nfrom the this result.\\nTheorem 6. Assume (X , ?), Y and ?\\n? as in Example 1. KSU is weakly Bayes-consistent on ?\\n?.\\nThe proof, provided in Appendix C in [26], first characterizes the Voronoi cells for which the true\\nmajority-vote yields a significant error for the cell (Lemma 15). In finite-dimensional spaces, the total\\nmeasure of all such ?bad? cells can be made arbitrarily close to zero by taking ? to be sufficiently\\nsmall, as shown in Lemma 10 of Theorem 2. However, it is not immediately clear whether this can\\nbe achieved for the infinite dimensional construction above.\\nIndeed, we expect such bad cells, due to the unintuitive property that for any x ? C, we have\\n?? (x) ? C)/?(B\\n?? (x)) ? 0 when ? ? 0, and yet ?(C) &gt; 0. Thus, if for example a significant\\n?(B\\n8\\n\\n\f?? (x) with\\nportion of the set C (whose label is 1) is covered by Voronoi cells of the form V = B\\nx ? C, then for all sufficiently small ?, each one of these cells will have a true majority-vote 0. Thus\\na significant portion of C would be misclassified. However, we show that by the structure of the\\nconstruction, combined with the packing and covering properties of ?-nets, we have that in any ?-net,\\nthe total measure of all these ?bad? cells goes to 0 when ? ? 0, thus yielding a consistent classifier.\\nLastly, the following theorem shows that on the same construction above, when the Voronoi partitions\\nare allowed to violate the packing property of ?-nets, Bayes-consistency does not necessarily hold.\\nTheorem 5 immediately follows from the following result.\\nTheorem 7. Assume (X , ?), Y and ?\\n? as in Example 1. There exists a sequence of Voronoi partitions\\n(Pk )k?N of X with maxV ?Pk diam(V ) ? ?k such that the sequence of true majority-vote classifiers\\n(hPk )k?N induced by these partitions is not Bayes consistent: lim inf k?? err(hPk ) = ? &gt; 0.\\nThe proof, provided in Appendix D, constructs a sequence of Voronoi partitions, where each partition\\nPk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case, C is\\nincorrectly classified by hPk , yielding a significant error. Thus, in infinite-dimensional metric spaces,\\nthe shape of the Voronoi cells plays a fundamental role in the consistency of the partition.\\nAcknowledgments. We thank Fr?d?ric C?rou for the numerous fruitful discussions and helpful\\nfeedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science\\nFoundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel\\nScience Foundation (grant No. 555/15).\\n\\nReferences\\n[1] Christophe Abraham, G?rard Biau, and Beno?t Cadre. On the kernel rule for function classification. Ann. Inst. Statist. Math., 58(3):619?633, 2006.\\n[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics &amp; Probability\\nLetters, 82(6):1102?1110, 2012.\\n[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic\\nCommunications in Probability, 18(3):1?7, 2013.\\n[4] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In\\nICML ?06: Proceedings of the 23rd international conference on Machine learning, pages\\n97?104, New York, NY, USA, 2006. ACM.\\n[5] G?rard Biau, Florentina Bunea, and Marten H. Wegkamp. Functional classification in Hilbert\\nspaces. IEEE Trans. Inform. Theory, 51(6):2163?2172, 2005.\\n[6] G?rard Biau, Fr?d?ric C?rou, and Arnaud Guyader. Rates of convergence of the functional\\nk-nearest neighbor estimate. IEEE Trans. Inform. Theory, 56(4):2034?2040, 2010.\\n[7] V. I. Bogachev. Measure theory. Vol. I, II. Springer-Verlag, Berlin, 2007.\\n[8] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image\\nclassification. In CVPR, 2008.\\n[9] Fr?d?ric C?rou and Arnaud Guyader. Nearest neighbor classification in infinite dimension.\\nESAIM: Probability and Statistics, 10:340?355, 2006.\\n[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In NIPS, 2014.\\n[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions\\non Information Theory, 13:21?27, 1967.\\n[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE\\nTrans. Pattern Anal. Mach. Intell., 3(1):75?78, 1981.\\n[13] Luc Devroye and L?szl? Gy?rfi. Nonparametric density estimation: the L1 view. Wiley Series\\nin Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &amp;\\nSons, Inc., New York, 1985.\\n9\\n\\n\f[14] Luc Devroye, L?szl? Gy?rfi, and G?bor Lugosi. A probabilistic theory of pattern recognition,\\nvolume 31. Springer Science &amp; Business Media, 2013.\\n[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wissenschaften, Band 153. Springer-Verlag New York Inc., New York, 1969.\\n[16] Evelyn Fix and Jr. Hodges, J. L. Discriminatory analysis. nonparametric discrimination:\\nConsistency properties. International Statistical Review / Revue Internationale de Statistique,\\n57(3):pp. 238?247, 1989.\\n[17] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine learning, 21(3):269?304, 1995.\\n[18] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient classification for metric\\ndata (extended abstract COLT 2010). IEEE Transactions on Information Theory, 60(9):5750?\\n5759, 2014.\\n[19] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality\\nreduction. Theoretical Computer Science, 620:105?118, 2016.\\n[20] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression\\nfor nearest neighbors. In Neural Information Processing Systems (NIPS), 2014.\\n[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Nearly optimal classification for\\nsemimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research, 2017.\\n[22] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on\\nthe prediction error of learning algorithms for classification. Machine Learning, 59(1):55?76,\\n2005.\\n[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classification. Ann.\\nStatist., 33(1):284?306, 02 2005.\\n[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its\\nApplications. Springer-Verlag, 2002.\\n[25] Aryeh Kontorovich, Sivan Sabato, and Ruth Urner. Active nearest-neighbor learning in metric\\nspaces. In Advances in Neural Information Processing Systems, pages 856?864, 2016.\\n[26] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression:\\nEfficiency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017.\\n[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classifier. In Artificial Intelligence\\nand Statistics (AISTATS 2015), 2014.\\n[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In International Conference on Machine Learning (ICML 2014), 2014.\\n[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity\\nsearch. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 791?801, January\\n2004.\\n[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation\\nunder arbitrary sampling. IEEE Trans. Inform. Theory, 41(4):1028?1039, 1995.\\n[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpublished, 1986.\\n[32] James R. Munkres. Topology: a first course. Prentice-Hall, Inc., Englewood Cliffs, N.J., 1975.\\n[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration\\nof measure. Inform. Process. Lett., 73(1-2):47?51, 2000.\\n[34] Vladimir Pestov. Is the k-NN classifier in high dimensions affected by the curse of dimensionality? Comput. Math. Appl., 65(10):1427?1437, 2013.\\n10\\n\\n\f[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis, pages\\n58?60, 1979.\\n[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin.,\\n22(1):181?193, 1981.\\n[37] Demetri Psaltis, Robert R. Snapp, and Santosh S. Venkatesh. On the finite sample performance\\nof the nearest neighbor classifier. IEEE Transactions on Information Theory, 40(3):820?837,\\n1994.\\n[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third\\nedition, 1976. International Series in Pure and Applied Mathematics.\\n[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1987.\\n[40] Richard J. Samworth. Optimal weighted nearest neighbour classifiers. Ann. Statist., 40(5):2733?\\n2763, 10 2012.\\n[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\\nAlgorithms. Cambridge University Press, 2014.\\n[42] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural\\nrisk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory,\\n44(5):1926?1940, 1998.\\n[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor\\nrisk. Ann. Statist., 26(3):850?878, 1998.\\n[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595?620,\\n1977.\\n[45] Jaroslav Ti?er. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc., 355(8):3277?\\n3289, 2003.\\n[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest\\nneighbor classification. Journal of Machine Learning Research, 10:207?244, 2009.\\n[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of\\nregression functions. J. Multivariate Anal., 21(1):168?178, 1987.\\n\\n11\\n\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bf2c4e2-274a-4479-ad19-1737aef1f68d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bf2c4e2-274a-4479-ad19-1737aef1f68d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bf2c4e2-274a-4479-ad19-1737aef1f68d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8f334404-c88d-4b93-971d-03993add8342\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f334404-c88d-4b93-971d-03993add8342')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8f334404-c88d-4b93-971d-03993add8342 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1500,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          2000,\n          1994,\n          1998\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          \"Latent Dirichlet Allocation\",\n          \"Potential Boosters?\",\n          \"Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Poster\",\n          \"Spotlight\",\n          \"Oral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          \"2070-latent-dirichlet-allocation.pdf\",\n          \"1737-potential-boosters.pdf\",\n          \"52-teaching-artificial-neural-systems-to-drive-manual-training-techniques-for-autonomous-systems.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 799,\n        \"samples\": [\n          \"In many large economic markets, goods are sold through sequential auctions. Such domains include eBay, online ad auctions, wireless spectrum auctions, and the Dutch flower auctions. Bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions.   In this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization. We define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions. We show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown.\",\n          \"Reliance on computationally expensive algorithms for inference has been limiting the use of Bayesian nonparametric models in large scale applications. To tackle this problem, we propose a Bayesian learning algorithm for DP mixture models. Instead of following the conventional paradigm -- random initialization plus iterative update, we take an progressive approach. Starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation. In this process, new components will be incorporated on the fly when needed. The algorithm can reliably estimate a DP mixture model in one pass, making it particularly suited for applications with massive data. Experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art.\",\n          \"In the online multiple testing problem, p-values corresponding to different null hypotheses are presented one by one, and the decision of whether to reject a hypothesis must be made immediately, after which the next p-value is presented. Alpha-investing algorithms to control the false discovery rate were first formulated by Foster and Stine and have since been generalized and applied to various settings, varying from quality-preserving databases for science to multiple A/B tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways : (a) we show how to uniformly improve the power of the entire class of GAI procedures under independence by awarding more alpha-wealth for each rejection, giving a near win-win resolution to a dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be null or non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more meaningful/important than others, (d) we define a new quantity called the \\\\emph{decaying memory false discovery rate, or $\\\\memfdr$} that may be more meaningful for applications with an explicit time component, using a discount factor to incrementally forget past decisions and alleviate some potential problems that we describe and name ``piggybacking'' and ``alpha-death''. Our GAI++ algorithms incorporate all four generalizations (a, b, c, d) simulatenously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1500,\n        \"samples\": [\n          \"Latent Dirichlet Allocation\\nDavid M. Blei, Andrew Y. Ng and Michael I. Jordan\\nUniversity of California, Berkeley\\nBerkeley, CA 94720\\n\\nAbstract\\nWe propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models\\nincluding naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic\\nindexing (pLSI) [3]. In the context of text modeling, our model\\nposits that each document is generated as a mixture of topics,\\nwhere the continuous-valued mixture proportions are distributed\\nas a latent Dirichlet random variable. Inference and learning are\\ncarried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text\\nmodeling, collaborative filtering, and text classification.\\n\\n1\\n\\nIntroduction\\n\\nRecent years have seen the development and successful application of several latent\\nfactor models for discrete data. One notable example, Hofmann's pLSI/aspect\\nmodel [3], has received the attention of many researchers, and applications have\\nemerged in text modeling [3], collaborative filtering [7], and link analysis [1]. In\\nthe context of text modeling, pLSI is a \\\"bag-of-words\\\" model in that it ignores the\\nordering of the words in a document. It performs dimensionality reduction, relating\\neach document to a position in low-dimensional \\\"topic\\\" space. In this sense, it is\\nanalogous to PCA, except that it is explicitly designed for and works on discrete\\ndata.\\nA sometimes poorly-understood subtlety of pLSI is that, even though it is typically\\ndescribed as a generative model , its documents have no generative probabilistic\\nsemantics and are treated simply as a set of labels for the specific documents seen\\nin the training set. Thus there is no natural way to pose questions such as \\\"what is\\nthe probability of this previously unseen document?\\\". Moreover, since each training\\ndocument is treated as a separate entity, the pLSI model has a large number of\\nparameters and heuristic \\\"tempering\\\" methods are needed to prevent overfitting.\\nIn this paper we describe a new model for collections of discrete data that provides\\nfull generative probabilistic semantics for documents. Documents are modeled via a\\nhidden Dirichlet random variable that specifies a probability distribution on a latent,\\nlow-dimensional topic space. The distribution over words of an unseen document is\\na continuous mixture over document space and a discrete mixture over all possible\\ntopics.\\n\\n\\f2\\n2.1\\n\\nGenerative models for text\\nLatent Dirichlet Allocation (LDA) model\\n\\nTo simplify our discussion, we will use text modeling as a running example throughout this section, though it should be clear that the model is broadly applicable to\\ngeneral collections of discrete data.\\n\\nIn LDA, we assume that there are k underlying latent topics according to which\\ndocuments are generated, and that each topic is represented as a multinomial distribution over the IVI words in the vocabulary. A document is generated by sampling\\na mixture of these topics and then sampling words from that mixture.\\nMore precisely, a document of N words w = (W1,'\\\" ,WN) is generated by the\\nfollowing process. First, B is sampled from a Dirichlet(a1,'\\\" ,ak) distribution.\\nThis means that B lies in the (k - I)-dimensional simplex: Bi 2': 0, 2: i Bi = 1.\\nThen, for each of the N words, a topic Zn E {I , ... , k} is sampled from a Mult(B)\\ndistribution p(zn = ilB) = Bi . Finally, each word Wn is sampled, conditioned on\\nthe znth topic, from the multinomial distribution p(wl zn). Intuitively, Bi can be\\nthought of as the degree to which topic i is referred to in the document . Written\\nout in full, the probability of a document is therefore the following mixture:\\n\\np(w) =\\n\\nIe (11 z~/(wnlzn; ,8)P( Zn IB?) p(B; a)dB,\\n\\n(1)\\n\\nwhere p(B ; a) is Dirichlet , p(znIB) is a multinomial parameterized by B, and\\np( Wn IZn;,8) is a multinomial over the words. This model is parameterized by the kdimensional Dirichlet parameters a = (a1,' .. ,ak) and a k x IVI matrix,8, which are\\nparameters controlling the k multinomial distributions over words. The graphical\\nmodel representation of LDA is shown in Figure 1.\\nAs Figure 1 makes clear, this model is not a simple Dirichlet-multinomial clustering\\nmodel. In such a model the innermost plate would contain only W n ; the topic\\nnode would be sampled only once for each document; and the Dirichlet would be\\nsampled only once for the whole collection. In LDA, the Dirichlet is sampled for\\neach document, and the multinomial topic node is sampled repeatedly within the\\ndocument. The Dirichlet is thus a component in the probability model rather than\\na prior distribution over the model parameters.\\nWe see from Eq. (1) that there is a second interpretation of LDA. Having sampled\\nB, words are drawn iid from the multinomial/unigram model given by p(wIB) =\\n2::=1 p(wl z)p(z IB). Thus, LDA is a mixture model where the unigram models\\np(wIB) are the mixture components, and p(B ; a) gives the mixture weights. Note\\nthat unlike a traditional mixture of unigrams model, this distribution has an infinite\\n\\no\\n\\n1'0 '.\\nZn\\n\\nWn\\n\\nNd I\\nD\\n\\nFigure 1: Graphical model representation of LDA. The boxes are plates representing\\nreplicates. The outer plate represents documents, while the inner plate represents\\nthe repeated choice of topics and words within a document.\\n\\n\\fFigure 2: An example distribution on unigram models p(wIB) under LDA for three\\nwords and four topics. The triangle embedded in the x-y plane is the 2-D simplex\\nover all possible multinomial distributions over three words. (E.g. , each of the\\nvertices of the triangle corresponds to a deterministic distribution that assigns one\\nof the words probability 1; the midpoint of an edge gives two of the words 0.5\\nprobability each; and the centroid of the triangle is the uniform distribution over\\nall 3 words). The four points marked with an x are the locations of the multinomial\\ndistributions p(wlz) for each of the four topics , and the surface shown on top of the\\nsimplex is an example of a resulting density over multinomial distributions given\\nby LDA.\\nnumber of continuously-varying mixture components indexed by B. The example\\nin Figure 2 illustrates this interpretation of LDA as defining a random distribution\\nover unigram models p(wIB).\\n2.2\\n\\nRelated models\\n\\nThe mixture of unigrams model [6] posits that every document is generated by a\\nsingle randomly chosen topic:\\n\\n(2)\\nThis model allows for different documents to come from different topics, but fails to\\ncapture the possibility that a document may express multiple topics. LDA captures\\nthis possibility, and does so with an increase in the parameter count of only one\\nparameter: rather than having k - 1 free parameters for the multinomial p(z) over\\nthe k topics, we have k free parameters for the Dirichlet.\\nA second related model is Hofmann's probabilistic latent semantic indexing\\n(pLSI) [3], which posits that a document label d and a word ware conditionally\\nindependent given the hidden topic z :\\n\\np(d, w) = L~=l p(wlz)p(zld)p(d).\\n\\n(3)\\n\\nThis model does capture the possibility that a document may contain multiple topics\\nsince p(zld) serve as the mixture weights of the topics. However, a subtlety of pLSIand the crucial difference between it and LDA-is that d is a dummy index into\\nthe list of documents in the training set. Thus, d is a multinomial random variable\\nwith as many possible values as there are training documents, and the model learns\\n\\n\\fthe topic mixtures p(zld) only for those documents on which it is trained. For this\\nreason, pLSI is not a fully generative model and there is no clean way to use it\\nto assign probability to a previously unseen document. Furthermore, the number\\nof parameters in pLSI is on the order of klVl + klDI, where IDI is the number of\\ndocuments in the training set. Linear growth in the number of parameters with the\\nsize of the training set suggests that overfitting is likely to be a problem and indeed,\\nin practice, a \\\"tempering\\\" heuristic is used to smooth the parameters of the model.\\n\\n3\\n\\nInference and learning\\n\\nLet us begin our description of inference and learning problems for LDA by examining the contribution to the likelihood made by a single document. To simplify\\nour notation, let w~ = 1 iff Wn is the jth word in the vocabulary and z~ = 1\\niff Zn is the ith topic. Let j3ij denote p(w j = Ilzi = 1), and W = (WI, ... ,WN),\\nZ = (ZI, ... ,ZN). Expanding Eq. (1), we have:\\n\\n(4)\\nThis is a hypergeometric function that is infeasible to compute exactly [4].\\nLarge text collections require fast inference and learning algorithms and thus we\\nhave utilized a variational approach [5] to approximate the likelihood in Eq. (4).\\nWe use the following variational approximation to the log likelihood:\\nlogp(w; a, 13)\\n\\nlog\\n\\nr :Ep(wlz; j3)p(zIB)p(B; a) qq~:,,Z\\\",\\nz:\\\" ~~ dB\\n\\nle\\n\\nz\\n\\n> Eq[logp(wlz;j3) +logp(zIB) +logp(B;a) -logq(B,z; , ,?)],\\nwhere we choose a fully factorized variational distribution q(B, z;\\\" ?)\\nq(B; ,) fIn q(Zn; ?n) parameterized by , and ?n, so that q(B; ,) is Dirichlet({), and\\nq(zn; ?n) is MUlt(?n). Under this distribution, the terms in the variational lower\\nbound are computable and differentiable, and we can maximize the bound with\\nrespect to, and ? to obtain the best approximation to p(w;a,j3).\\nNote that the third and fourth terms in the variational bound are not straightforward to compute since they involve the entropy of a Dirichlet distribution, a\\n(k - I)-dimensional integral over B which is expensive to compute numerically. In\\nthe full version of this paper, we present a sequence of reductions on these terms\\nwhich use the log r function and its derivatives. This allows us to compute the\\nintegral using well-known numerical routines.\\nVariational inference is coordinate ascent in the bound on the probability of a single\\ndocument. In particular, we alternate between the following two equations until the\\nobjective converges:\\n\\n(5)\\n\\n,i\\n\\n+ 2:~=1 ?ni\\nderivative of the log r function.\\nai\\n\\n(6)\\n\\nwhere \\\\]i is the first\\nNote that the resulting variational parameters can also be used and interpreted as an approximation of the\\nparameters of the true posterior.\\nIn the current paper we focus on maximum likelihood methods for parameter estimation. Given a collection of documents V = {WI' ... ' WM}, we utilize the EM\\n\\n\\falgorithm with a variational E step, maximizing a lower bound on the log likelihood:\\nM\\n\\nlogp(V) 2::\\n\\nl:= Eqm [logp(B, z, w)]- Eqm [logqm(B, z)].\\n\\n(7)\\n\\nm=l\\n\\nThe E step refits qm for each document by running the inference step described\\nabove. The M step optimizes Eq. (7) with respect to the model parameters a\\nand (3. For the multinomial parameters (3ij we have the following M step update\\nequation:\\nM\\n\\n(3ij ex:\\n\\nIwml\\n\\nl:= l:=\\n\\n?>mniwtnn?\\n\\n(8)\\n\\nm=l n=l\\n\\nThe Dirichlet parameters ai are not independent of each other and we apply\\nNewton-Raphson to optimize them:\\n\\nThe variational EM algorithm alternates between maximizing Eq. (7) with respect\\nto qm and with respect to (a, (3) until convergence.\\n\\n4\\n\\nExperiments and Examples\\n\\nWe first tested LDA on two text corpora. 1 The first was drawn from the TREC AP\\ncorpus, and consisted of 2500 news articles, with a vocabulary size of IVI = 37,871\\nwords. The second was the CRAN corpus, consisting of 1400 technical abstracts,\\nwith IVI = 7747 words.\\nWe begin with an example showing how LDA can capture multiple-topic phenomena\\nin documents. By examining the (variational) posterior distribution on the topic\\nmixture q(B; ')'), we can identify the topics which were most likely to have contributed\\nto many words in a given document; specifically, these are the topics i with the\\nlargest ')'i. Examining the most likely words in the corresponding multinomials can\\nthen further tell us what these topics might be about. The following is an article\\nfrom the TREC collection.\\nThe William R andolph Hearst Foundation will give $1.25 million to Lincoln Center,\\nMetropolitan Opera Co., New York Philharmonic and Juilliard School.\\n\\\"Our board felt that we had a real opportunity to make a mark on the future of the\\nperforming arts with these grants an act every bit as important as our traditional areas of support in health , medical research, education and the social services,\\\" Hearst\\nFoundation President Randolph A. Hearst said Monday in announcing the grants.\\nLincoln Center's share will be $200,000 for its new building, which will house young\\nartists and provide new public facilities. The Metropolitan Opera Co. and New York\\nPhilharmonic will receive $400 ,000 each. The Juilliard School, where music and the\\nperforming arts are taught, will get $250,000 .\\nThe Hearst Foundation, a leading supporter of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000 donation, too.\\n\\nFigure 3 shows the Dirichlet parameters of the corresponding variational distribution for those topics where ')'i > 1 (k = 100) , and also lists the top 15 words (in\\niTo enable repeated large scale comparison of various models on large corpora, we\\nimplemented our variational inference algorithm on a parallel computing cluster. The\\n(bottleneck) E step is distributed across nodes so that the qm for different documents are\\ncalculated in parallel.\\n\\n\\fI\\\"\\n\\nTopic 1\\n\\nTopic 2\\n\\nTopic 3\\n\\nTopic 4\\n\\nTopic 5\\n\\nSCHOOL\\nSAID\\nSTUDENTS\\nBOARD\\nSCHOOLS\\nSTUDENT\\nTEACHER\\nPOLICE\\nPROGRAM\\nTEACHERS\\nMEMBERS\\nYEAROLD\\nGANG\\nDEPARTMENT\\n\\nMILLION\\nYEAR\\nSAID\\nSALES\\nBILLION\\nTOTAL\\nSHARE\\nEARNINGS\\nPROFIT\\nQUARTER\\nORDERS\\nLAST\\nDEC\\nREVENUE\\n\\nSAID\\nAIDS\\nHEALTH\\nDISEASE\\nVIRUS\\nCHILDREN\\nBLOOD\\nPATIENTS\\nTREATMENT\\nSTUDY\\nIMMUNE\\nCANCER\\nPEOPLE\\nPERCENT\\n\\nSAID\\nNEW\\nPRESIDENT\\nCHIEF\\nCHAIRMAN\\nEXECUTIVE\\nVICE\\nYEARS\\nCOMPANY\\nYORK\\nSCHOOL\\nTWO\\nTODAY\\nCOLUMBIA\\n\\nSAID\\nNEW\\nMUSIC\\nYEAR\\nTHEATER\\nMUSICAL\\nBAND\\nPLAY\\nWON\\nTWO\\nAVAILABLE\\nAWARD\\nOPERA\\nBEST\\n\\nFigure 3: The Dirichlet parameters where Ii > 1 (k = 100), and the top 15 words\\nfrom the corresponding topics, for the document discussed in the text.\\n__ LDA\\n-x- pLSI\\n\\n..\\n~\\n\\npLSI(00 lemper)\\nMIx1Un;grams\\n\\nv ? ram\\n\\nwoo '\\n.~\\n\\n4500\\n\\n',.. _\\n\\n.l\\\\\\n\\n!\\n-><---------------\\n\\nk (number of topics)\\n\\nk (number of topiCS)\\n\\nFigure 4: Perplexity results on the CRAN and AP corpora for LDA, pLSI, mixture\\nof unigrams, and t he unigram model.\\norder) from these topics. This document is mostly a combination of words about\\nschool policy (topic 4) and music (topic 5). The less prominent topics reflect other\\nwords about education (topic 1) , finance (topic 2), and health (topic 3).\\n4.1\\n\\nFormal evaluation: Perplexity\\n\\nTo compare the generalization performance of LDA with other models, we computed the perplexity of a test set for the AP and CRAN corpora. The perplexity, used by convention in language modeling, is monotonically decreasing in the\\nlikelihood of the test data, and can be thought of as the inverse of the per-word\\nlikelihood. More formally, for a test set of M documents, perplexity(Vtest ) =\\nexp (-l:m logp(wm)/ l:m Iwml}.\\nWe compared LDA to both the mixture of unigrams and pLSI described in Section 2.2. We trained the pLSI model with and without tempering to reduce overfitting. When tempering, we used part of the test set as the hold-out data, thereby\\ngiving it a slight unfair advantage. As mentioned previously, pLSI does not readily\\ngenerate or assign probabilities to previously unseen documents; in our experiments,\\nwe assigned probability to a new document d by marginalizing out the dummy training set indices 2 : pew ) = l: d( rr : =1l:z p(w n lz)p(z ld))p(d) .\\n2 A second natural method, marginalizing out d and z to form a unigram model using\\nthe resulting p(w)'s, did not perform well (its performance was similar to the standard\\nunigram model).\\n\\n\\f1-:- ~Dc.~UrUg,ams I\\n\\nW'\\n? M\\\"\\n~\\nx\\n\\nNaiveBaes\\n\\nk (number of topics)\\n\\nk (number of topics)\\n\\nFigure 5: Results for classification (left) and collaborative filtering (right)\\nFigure 4 shows the perplexity for each model and both corpora for different values\\nof k. The latent variable models generally do better than the simple unigram model.\\nThe pLSI model severely overfits when not tempered (the values beyond k = 10\\nare off the graph) but manages to outperform mixture of unigrams when tempered.\\nLDA consistently does better than the other models. To our knowledge, these are\\nby far the best text perplexity results obtained by a bag-of-words model.\\n4.2\\n\\nClassification\\n\\nWe also tested LDA on a text classification task. For each class c, we learn a separate\\nmodel p(wlc) of the documents in that class. An unseen document is classified by\\npicking argmaxcp(Clw) = argmaxcp(wlc)p(c). Note that using a simple unigram\\ndistribution for p(wlc) recovers the traditional naive Bayes classification model.\\nUsing the same (standard) subset of the WebKB dataset as used in [6], we obtained\\nclassification error rates illustrated in Figure 5 (left). In all cases, the difference\\nbetween LDA and the other algorithms' performance is statistically significant (p <\\n0.05).\\n4.3\\n\\nCollaborative filtering\\n\\nOur final experiment utilized the EachMovie collaborative filtering dataset. In this\\ndataset a collection of users indicates their preferred movie choices. A user and\\nthe movies he chose are analogous to a document and the words in the document\\n(respectively) .\\nThe collaborative filtering task is as follows. We train the model on a fully observed set of users. Then, for each test user, we are shown all but one of the\\nmovies that she liked and are asked to predict what the held-out movie is. The\\ndifferent algorithms are evaluated according to the likelihood they assign to the\\nheld-out movie. More precisely define the predictive perplexity on M test users\\nto be exp( - ~~=llogP(WmNd lwml' ... ,Wm(Nd-l))/M) . With 5000 training users,\\n3500 testing users, and a vocabulary of 1600 movies, we find predictive perplexities\\nillustrated in Figure 5 (right).\\n\\n5\\n\\nConclusions\\n\\nWe have presented a generative probabilistic framework for modeling the topical\\nstructure of documents and other collections of discrete data. Topics are represented\\n\\n\\fexplicitly via a multinomial variable Zn that is repeatedly selected, once for each\\nword, in a given document. In this sense, the model generates an allocation of\\nthe words in a document to topics. When computing the probability of a new\\ndocument, this unknown allocation induces a mixture distribution across the words\\nin the vocabulary. There is a many-to-many relationship between topics and words\\nas well as a many-to-many relationship between documents and topics.\\nWhile Dirichlet distributions are often used as conjugate priors for multinomials in\\nBayesian modeling, it is preferable to instead think of the Dirichlet in our model as\\na component of the likelihood. The Dirichlet random variable e is a latent variable\\nthat gives generative probabilistic semantics to the notion of a \\\"document\\\" in the\\nsense that it allows us to put a distribution on the space of possible documents.\\nThe words that are actually obtained are viewed as a continuous mixture over this\\nspace, as well as being a discrete mixture over topics. 3\\nThe generative nature of LDA makes it easy to use as a module in more complex\\narchitectures and to extend it in various directions. We have already seen that\\ncollections of LDA can be used in a classification setting. If the classification variable\\nis treated as a latent variable we obtain a mixture of LDA models, a useful model for\\nsituations in which documents cluster not only according to their topic overlap, but\\nalong other dimensions as well. Another extension arises from generalizing LDA to\\nconsider Dirichlet/multinomial mixtures of bigram or trigram models, rather than\\nthe simple unigram models that we have considered here. Finally, we can readily\\nfuse LDA models which have different vocabularies (e.g., words and images); these\\nmodels interact via a common abstract topic variable and can elegantly use both\\nvocabularies in determining the topic mixture of a given document.\\nAcknowledgments\\nA. Ng is supported by a Microsoft Research fellowship. This work was also supported by a grant from Intel Corporation, NSF grant IIS-9988642, and ONR MURI\\nN00014-00-1-0637.\\n\\nReferences\\n[1] D. Cohn and T. Hofmann. The missing link- A probabilistic model of document\\ncontent and hypertext connectivity. In Advances in Neural Information Processing\\nSystems 13, 2001.\\n[2] P.J. Green and S. Richardson. Modelling heterogeneity with and without the Dirichlet\\nprocess. Technical Report, University of Bristol, 1998.\\n[3] T. Hofmann. Probabilistic latent semantic indexing. Proceedings of th e Twenty-Second\\nAnnual International SIGIR Conference, 1999.\\n[4] T. J. Jiang, J. B. Kadane, and J. M. Dickey. Computation of Carlson's multiple\\nhypergeometric functions r for Bayesian applications. Journal of Computational and\\nGraphical Statistics, 1:231- 251 , 1992.\\n[5] M. I. Jordan , Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. Introduction to variational methods for graphical models. Machine Learning, 37:183- 233, 1999.\\n[6] K. Nigam, A. Mccallum, S. Thrun, and T. Mitchell. Text classification from labeled\\nand unlabeled documents using EM. Machine Learning, 39(2/3):103- 134, 2000.\\n[7] A. Popescul, L. H. Ungar, D. M. Pennock, and S. Lawrence. Probabilistic models for\\nunified collaborative and content-based recommendation in sparse-data environments.\\nIn Uncertainty in Artificial Intelligence, Proceedings of the Seventeenth Conference,\\n2001.\\n3These remarks also distinguish our model from the Bayesian Dirichlet/Multinomial\\nallocation model (DMA)of [2], which is a finite alternative to the Dirichlet process . The\\nDMA places a mixture of Dirichlet priors on p(wl z ) and sets O i = 00 for all i .\\n\\n\\f\",\n          \"Potential Boosters ?\\nNigel Duffy\\nDepartment of Computer Science\\nUniversity of California\\nSanta Cruz, CA 95064\\n\\nDavid Helmbold\\nDepartment of Computer Science\\nUniversity of California\\nSanta Cruz, CA 95064\\n\\nnigedufJ@cse. ucsc. edu\\n\\ndph@~se . ucsc. edu\\n\\nAbstract\\nRecent interpretations of the Adaboost algorithm view it as performing a gradient descent on a potential function. Simply changing the potential function allows one to create new algorithms related to AdaBoost. However, these new algorithms are generally\\nnot known to have the formal boosting property. This paper examines the question of which potential functions lead to new algorithms that are boosters. The two main results are general sets\\nof conditions on the potential; one set implies that the resulting\\nalgorithm is a booster, while the other implies that the algorithm\\nis not. These conditions are applied to previously studied potential\\nfunctions , such as those used by LogitBoost and Doom II.\\n\\n1\\n\\nIntroduction\\n\\nThe first boosting algorithm appeared in Rob Schapire's thesis [1]. This algorithm\\nwas able to boost the performance of a weak PAC learner [2] so that the resulting\\nalgorithm satisfies the strong PAC learning [3] criteria. We will call any method\\nthat builds a strong PAC learning algorithm from a weak PAC learning algorithm\\na PAC boosting algorithm. Freund and Schapire later found an improved PAC\\nboosting algorithm called AdaBoost [4], which also tends to improve the hypotheses\\ngenerated by practical learning algorithms [5].\\nThe AdaBoost algorithm takes a labeled training set and produces a master hypothesis by repeatedly calling a given learning method. The given learning method\\nis used with different distributions on the training set to produce different base\\nhypotheses. The master hypothesis returned by AdaBoost is a weighted vote of\\nthese base hypotheses. AdaBoost works iteratively, determining which examples\\nare poorly classified by the current weighted vote and selecting a distribution on\\nthe training set to emphasize those examples.\\nRecently, several researchers [6, 7, 8, 9, 10] have noticed that Adaboost is performing\\na constrained gradient descent on an exponential potential function of the margins\\nof the examples. The margin of an example is yF(x) where y is the ?1 valued label\\nof the example x and F(x) E lR is the net weighted vote of master hypothesis F.\\nOnce Adaboost is seen this way it is clear that further algorithms may be derived\\nby changing the potential function [6, 7, 9, 10].\\n\\n\\fPotential Boosters?\\n\\n259\\n\\nThe exponential potential used by AdaBoost has the property that the influence\\nof a data point increases exponentially if it is repeatedly misclassified by the base\\nhypotheses. This concentration on the \\\"hard\\\" examples allows AdaBoost to rapidly\\nobtain a consistent hypothesis (assuming that the base hypotheses have certain\\nproperties). However, it also means that an incorrectly labeled or noisy example\\ncan quickly attract much of the distribution. It appears that this lack of noisetolerance is one of AdaBoost's few drawbacks [11]. Several researchers [7, 8, 9, 10]\\nhave proposed potential functions which do not concentrate as much on these \\\"hard\\\"\\nexamples. However, they generally do not show that the derived algorithms have\\nthe PAC boosting property.\\n\\nIn this paper we return to the original motivation behind boosting algorithms and\\nask: \\\"for which potential functions does gradient descent lead to PAC boosting\\nalgorithms\\\" (i.e. boosters that create strong PAC learning algorithms from arbitrary\\nweak PAC learners). We give necessary conditions that are met by some of the\\nproposed potential functions (most notably the LogitBoost potential introduced by\\nFriedman et al. [7]). Furthermore, we show that simple gradient descent on other\\nproposed potential functions (such as the sigmoidal potential used by Mason et\\nal. [10]) cannot convert arbitrary weak PAC learning algorithms into strong PAC\\nlearners. The aim of this work is to identify properties of potential functions required\\nfor PAC boosting, in order to guide the search for more effective potentials.\\nSome potential functions have an additional tunable parameter [10] or change over\\ntime [12]. Our results do not yet apply to such dynamic potentials.\\n\\n2\\n\\nPAC Boosting\\n\\nHere we define the notions of PAC learningl and boosting, and define the notation\\nused throughout the paper.\\nA concept C is a subset of the learning domain X. A random example of C is a pair\\n(x E X,y E {-1, +1}) where x is drawn from some distribution on X and y = 1 if\\nx E C and -1 otherwise. A concept class is a set of concepts.\\n\\nDefinition 1 A (strong) PAC learner for concept class C has the property that for\\nevery distribution D on X, all concepts C E C, and all 0 < E,O < 1/2: with probability at least 1 - 0 the algorithm outputs a hypothesis h where P D [h( x) :j:. C (x)] ~ E.\\nThe learning algorithm is given C, E, 0, and the ability to draw random examples of\\nC (w.r.t. distribution D), and must run in time bounded by poly(l/E,l/o).\\nDefinition 2 A weak PAC learner is similar to a strong PA C learner, except that\\nit need only satisfy the conditions for a particular 0 < EO, 00 < 1/2 pair, rather than\\nfor all E,O pairs.\\nDefinition 3 A PAC boosting algorithm is a generic algorithm which can leverage\\nany weak PAC learner to meet the strong PAC learning criteria.\\n\\nIn the remainder of the paper we emphasize boosting the accuracy E as it is much\\neasier to boost the confidence 0, see Haussler et al. [13] and Freund [14] for details.\\nFurthermore, we emphasize boosting by re-sampling, where the strong PAC learner\\ndraws a large sample, and each iteration the weak learning algorithm is called with\\nsome distribution over this sample.\\nlTo simplify the presentation we omit the instance space dimension and target representation length parameters.\\n\\n\\fN. Duffy and D. Helmbold\\n\\n260\\n\\nThroughout the paper we use the following notation.\\n? m is the cardinality of the fixed sample {(Xl, Y1), ... , (Xm, Ym) }.\\n? ht(x) is the ?1 valued weak hypothesis created at iteration t.\\n? at is the weight or vote of ht in the master hypothesis, the a's mayor may\\nnot be normalized so that 2::, =1 at' = l.\\n? Ft (x) = 2::'=1 (at' ht, (x) /2:;=1 aT) E !R, is the master hypothesis 2at iteration t.\\n\\n? Ui,t = Yi 2::'=1 at,ht, (X) is the margin of Xi after iteration t; the t subscript is often omitted. Note that the margin is positive when the master\\nhypothesis is correct, and the normalized margin is Ui,t/ 2::'=1 at'?\\n? p(u) is the potential of an instance with margin u, and the total potential\\nis 2:~1 p(Ui).\\n? P v[ ],P s[ ], and Es[ ] are the probability with respect to the unknown\\ndistribution over the domain, and the probability and expectations with\\nrespect to the uniform distribution over the sample, respectively.\\nOur results apply to total potential functions of the form 2:~1 p(Ui) where p is\\npositive and strictly decreasing.\\n\\n3\\n\\nLeveraging Learners by Gradient Descent\\n\\nAdaBoost [4] has recently been interpreted as gradient descent independently by\\nseveral groups [6, 7, 8, 9,\\nUnder this interpretation AdaBoost is seen as minimizing the total potential 2:i=l P(Ui) = 2:~1 exp( -Ui) via feasible direction gradient\\ndescent. On each iteration t + 1, AdaBoost chooses the direction of steepest descent\\nas the distribution on the sample, and calls the weak learner to obtain a new base\\nhypothesis h Hl . The weight at+! of this new weak hypothesis is calculated to minimize3the resulting potential 2:~1 p(Ui,H1) = 2:~1 exp( -(Ui,t + aHIYi ht+! (Xi))).\\n\\n1m.\\n\\nThis gradient descent idea has been generalized to other potential functions [6, 7,\\n10]. Duffy et al. [9] prove bounds for a similar gradient descent technique using a\\nnon-componentwise, non-monotonic potential function.\\nNote that if the weak learner returns a good hypothesis h t (with training error at\\nmost ? < 1/2), then 2:~1 Dt(Xi)Yiht(Xi) > 1 - 2? > O. We set T = 1 - 2?, and\\nassume that each base hypothesis produced satisfies 2:~1 Dt(Xi)Yiht(Xi) ~ T.\\n\\nIn this paper we consider this general gradient descent approach applied to various\\npotentials 2:~1 P(Ui). Note that each potential function P has two corresponding\\ngradient descent algorithms (see [6]). The un-normalized algorithms (like AdaBoost)\\ncontinually add in new weak hypotheses while preserving the old a's. The normalized algorithms re-scale the a's so that they always sum to 1. In general, we call\\nsuch algorithms \\\"leveraging algorithms\\\", reserving the term \\\"boosting\\\" for those\\nthat actually have the PAC boosting property.\\n\\n4\\n\\nPotentials that Don't Boost\\n\\nIn this section we describe sufficient conditions on potential functions so that the\\ncorresponding leveraging algorithm does not have the PAC boosting property. We\\n2The prediction of the master hypothesis on instance x is the sign of Ft(x).\\n30ur current proofs require that the actual at's be no greater than a constant (say 1).\\nTherefore, this minimizing a may need to be reduced.\\n\\n\\fPotential Boosters?\\n\\n261\\n\\napply these conditions to show that two potentials from the literature do not lead\\nto boosting algorithms.\\nTheorem 1 Let p( u) be a potential function for which:\\n1} the derivative, p' (u), is increasing (_p' (u) decreasing} in ?R+, and\\n2} 3{3 > such that for all u > 0, -{3p'(u) ~ -p'(-2u).\\nThen neither the normalized nor the un-normalized leveraging algorithms corresponding to potential p have the PAC boosting property.\\n\\n?\\n\\nThis theorem is proven by an adversary argument. Whenever the concept class is\\nsufficiently rich 4 , the adversary can keep a constant fraction of the sample from\\nbeing correctly labeled by the master hypothesis. Thus as the error tolerance ? goes\\nto zero, the master hypotheses will not be sufficiently accurate.\\nWe now apply this theorem to two potential functions from the literature.\\nFriedman et al. [7J describe a potential they call \\\"Squared Error(p)\\\" where the\\nyo + 1\\neF(Xi)) 2\\npotential at Xi is (\\neF(Xi) + e-F(Xi)\\n. This potential can be re-written\\n\\nT -\\n\\n1 (\\ne- Ui _ eUi\\nas PSE(Ui) = 4\\\" 1 + 2 eUi + e-Ui\\n\\n+\\n\\n(e- Ui _ eUi\\neUi\\n\\n+\\n\\ne - Ui\\n\\n)2)\\n\\n?\\n\\nCorollary 1 Potential \\\"Squared Error{p} \\\" does not lead to a boosting algorithm.\\nProof: This potential satisfies the conditions of Theorem 1. It is strictly decreasing, and the second condition holds for {3 = 2.\\n\\nMason et al. [lOJ examine a normalized algorithm using the potential PD(U) =\\n1- tanh (AU). Their algorithm optimizes over choices of A via cross-validation, and\\nuses weak learners with slightly different properties. However, we can plug this\\npotential directly into the gradient descent framework and examine the resulting\\nalgorithms.\\nCorollary 2 The DOOMII potential PD does not lead to a boosting algorithm for\\nany fixed A.\\nProof: The potential is strictly decreasing, and the second condition of Theorem 1\\nholds for {3 = 1.\\n\\nOur techniques show that potentials that are sigmoidal in nature do not lead to\\nalgorithms with the PAC boosting property. Since sigmoidal potentials are generally better over-estimates of the 0, 1 loss than the potential used by AdaBoost,\\nour results imply that boosting algorithms must use a potential with more subtle\\nproperties than simply upper bounding the 0, 1 loss.\\n\\n5\\n\\nPotential Functions That Boost\\n\\nIn this section we give sufficient conditions on a potential function for it's corresponding un-normalized algorithm to have the PAC boosting property. This result\\nimplies that AdaBoost [4J and LogitBoost [7J have the PAC boosting property (Although this was previously known for AdaBoost [4J, we believe this is a new result\\nfor LogitBoost).\\n4The VC-dimension 4 concept class consisting of pairs of intervals on the real line is\\nsufficient for our adversary.\\n\\n\\fN. Duffy and D. Helmbold\\n\\n262\\n\\nOne set of conditions on the potential imply that it decreases roughly exponentially when the (un-normalized) margins are large. Once the margins are in this\\nexponential region, ideas similar to those used in AdaBoost's analysis show that the\\nminimum normalized margin quickly becomes bounded away from zero. This allows\\nus to bound the generalization error using a theorem from Bartlett et al. [15].\\nA second set of conditions governs the behavior of the potential function before\\nthe un-normalized margins are large enough. These conditions imply that the total\\npotential decreases by a constant factor each iteration. Therefore, too much time\\nwill not be spent before all the margins enter the exponential region.\\nThe margin value bounding the exponential region is U, and once 2::=1 p(Ui) ~\\np(U), all margins p(Ui) will remain in the exponential region. The following theorem\\ngives conditions on p ensuring that 2::=1 P(Ui) quickly becomes less than p(U).\\nTheorem 2 If the following conditions hold for p( u) and U:\\n\\n< pll(U)\\n-qp'(u) Vu > U,\\n\\n1. -p'(u) is strictly decreasing -and 0\\n2. 3q> 0 such that p(u) ~\\n\\n~\\n\\nB, and\\n\\n4Bq 2 m 2 p(0) In ( ~rb)))\\nthen 2:i=l P(Ui) ~ p(U) after Tl ~\\np(U)2r2\\niterations.\\nm\\n\\nThe proof of this theorem approximates the new total potential by the old potential\\nminus a times a linear term, plus an error. By bounding the error as a function of\\na and minimizing we demonstrate that some values of a give a sufficient decrease\\nin the total potential.\\nTheorem 3 If the following conditions hold for p( u), U, q, and iteration Tl:\\n1. 3(3 ~ .J3 such that -p'(u\\nv ~ 1 and u > U,\\n\\n+ v)\\n\\n~ p(u\\n\\n+ v)\\n\\n~ -p'(u)(3-V q whenever -1 ~\\n\\n2. 2::1P(Ui,Tl) ~ p(U),\\n\\n3. -p' (u) is strictly decreasing, and\\n\\n4.\\n\\n3C\\n\\n> 0, 'Y>\\n\\n1 such that Cp(u) ~ 'Y- u Vu\\n\\n>U\\nwhich decreases expo-\\n\\nThe proof of this theorem is a generalization of the AdaBoost proof.\\nCombining these two theorems, and the generalization bound from Theorem 2 of\\nBartlett et al. [15] gives the following result, where d is the VC dimension of the\\nweak hypothesis class.\\nTheorem 4 If for all edges 0 < r < 1/2 there exists T 1,r ~ poly(m,l/r), Ur ,\\nand qr satisfying the conditions of Theorem 3 such that p(Ur ) ~ poly(r) and\\nqrv'f'=T2 = l(r) < 1 - poly(r), then in time poly(m, 1/r) all examples have nor-\\n\\n\\f263\\n\\nPotential Boosters?\\n\\nmalized margin at least () = In ((l~~~l))\\n\\nP D[yFT(X)\\n\\n~ 0] E 0 (\\n\\n1\\n\\nVm\\n\\n/ In(r)\\n\\nand\\n\\n( l n 2(r)dlog2 (m/d)\\n)~)\\n(In (l(r) + 1) -In (21(r)))2 + 10g(1/8)\\n\\nChoosing m appropriately makes the error rate sufficiently small so that the algorithm corresponding to p has the PAC boosting property.\\n\\nWe now apply Theorem 4 to show that the AdaBoost and LogitBoost potentials\\nlead to boosting algorithms.\\n\\n6\\n\\nSome Boosting Potentials\\n\\nIn this section we show as a direct consequence of our Theorem 4 that the potential\\nfunctions for AdaBoost and LogitBoost lead to boosting algorithms. Note that\\nthe LogitBoost algorithm we analyze is not exactly the same as that described by\\nFriedman et al. [7], their \\\"weak learner\\\" optimizes a square loss which appears to\\nbetter fit the potential. First we re-derive the boosting property for AdaBoost.\\nCorollary 3 AdaBoost's [16] potential boosts.\\nProof: To prove this we simply need to show that the potential p(u) = exp( -u)\\nsatisfies the conditions of Theorem 4. This is done by setting Ur = -In(m), qr = 1,\\n'Y = f3 = e, C = 1, and Tl = o.\\nCorollary 4 The log-likelihood potential (as used in LogitBoost [7]) boosts.\\nProof: In this case p(u)\\n\\nC = 2, Ur\\n\\n= In (1 + e-\\n\\nf2/2\\n= -In ( Jl~\\n\\norem 2 shows that after Tl\\nare satisfied.\\n\\n7\\n\\n~\\n\\n-1\\n\\n)\\n\\nU )\\n\\nand -p'(u)\\n\\nand qr\\n\\n= l!~:u.\\n\\n= 1 +exp(-U\\n\\nr )\\n\\n=\\n\\nWe set 'Y = f3\\n\\n= e,\\n\\n{l?1Z\\n~. Now The-\\n\\npoly(m, l/r) iterations the conditions of Theorem 4\\n\\nConclusions\\n\\nIn this paper we have examined leveraging weak learners using a gradient descent\\napproach [9] . This approach is a direct generalization of the Adaboost [4, 16] algorithm, where Adaboost's exponential potential function is replaced by alternative\\npotentials. We demonstrated properties of potentials that are sufficient to show\\nthat the resulting algorithms are PAC boosters, and other properties that imply\\nthat the resulting algorithms are not PAC boosters. We applied these results to\\nseveral potential functions from the literature [7, 10, 16].\\nNew insight can be gained from examining our criteria carefully. The conditions\\nthat show boosting leave tremendous freedom in the choice of potential function\\nfor values less than some U, perhaps this freedom can be used to choose potential\\nfunctions which do not overly concentrate on noisy examples.\\nThere is still a significant gap between these two sets of properties, we are still a long\\nway from classifying arbitrary potential functions as to their boosting properties.\\nThere are other classes of leveraging algorithms. One class looks at the distances\\nbetween successive distributions [17, 18]. Another class changes their potential\\n\\n\\f264\\n\\nN. Duffy and D. Helmbold\\n\\nover time [6, 8, 12, 14]. The criteria for boosting may change significantly with\\nthese different approaches. For example, Freund recently presented a boosting\\nalgorithm [12] that uses a time-varying sigmoidal potential. It would be interesting\\nto adapt our techniques to such dynamic potentials.\\n\\nReferences\\n[1] Robert E. Schapire. The Design and Analysis of Efficient Learning Algorithms. MIT\\nPress, 1992.\\n[2] Michael Kearns and Leslie Valiant . Cryptographic limitations on learning Boolean\\nformulae and finite automata. Journal of the ACM, 41(1):67-95, January 1994.\\n[3] L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, November 1984.\\n[4] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line\\nlearning and an application to boosting. Journal of Computer and System Sciences,\\n55(1):119-139, August 1997.\\n[5] Eric Bauer and Ron Kohavi. An empirical comparison of voting classification algorithms: Bagging, boosting and variants. Machine Learning, 36(1-2):105-39, 1999.\\n[6] Leo Breiman. Arcing the edge. Technical Report 486, Department of Statistics,\\nUniversity of California, Berkeley., 1997. available at www.stat.berkeley.edu.\\n[7] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression:\\na statistical view of boosting. Technical report, Stanford University, 1998.\\n[8] G. lliitsch, T. Onoda, and K-R. Muller. Soft margins for AdaBoost. Machine Learning, 2000. To appear.\\n[9] Nigel Duffy and David P. Helmbold. A geometric approach to leveraging weak learners. In Paul Fischer and Hans Ulrich Simon, editors, Computational Learning Theory:\\n4th European Conference (EuroCOLT '99), pages 18-33. Springer-Verlag, March 1999.\\n[10] Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms\\nas gradient descent. To appear in NIPS 2000.\\n[11] Thomas G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, Boosting, and Randomization. Machine\\nLearning. To appear.\\n[12] Yoav Freund. An adaptive version of the boost-by-majority algorithm. In Proc. l?th\\nAnnu. Conf. on Comput. Learning Theory, pages 102-113. ACM, 1999.\\n[13] David Haussler, Michael Kearns, Nick Littlestone, and Manfred K Warmuth. Equivalence of models for polynomiallearnability. Information and Computation, 95(2):129161, December 1991.\\n[14] Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256-285, September 1995.\\n[15] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the\\nmargin: A new explanation for the effectiveness of voting methods. The Annals of\\nStatistics, 26(5):1651-1686, 1998.\\n[16] Robert E. Schapire and Yoram Singer.\\nImproved boosting algorithms using\\nconfidence-rated predictions. Machine Learning, 37(3):297-336, December 1999.\\n[17] Jyrki Kivinen and Manfred K Warmuth. Boosting as entropy projection. In Proc.\\nl?th Annu. Conf. on Comput. Learning Theory, pages 134-144. ACM, 1999.\\n[18] John Lafferty. Additive models, boosting, and inference for generalized divergences.\\nIn Proc. l?th Annu. Conf. on Comput. Learning Theory, pages 125-133. ACM.\\n\\n\\f\",\n          \"693\\n\\nTeaching Artificial Neural Systems to Drive:\\nManual Training Techniques for Autonomous Systems\\n\\nJ. F. Shepanski and S. A. Macy\\n\\nTRW, Inc .\\nOne Space Park, 02/1779\\nRedondo Beach, CA 90278\\n\\nAbetract\\nWe have developed a methodology for manually training autononlous control systems\\nbased on artificial neural systems (ANS). In applications where the rule set governing an expert's\\ndecisions is difficult to formulate, ANS can be used to ext.ra.c:t rules by associating the information\\nan expert receives with the actions h~ takes . Properly constructed networks imitate rules of\\nbehavior that permits them to function autonomously when they are trained on the spanning set\\nof possible situations. This training can be provided manually, either under the direct. supervision\\nor a system trainer, or indirectly using a background mode where the network assimilates training\\ndata as the expert perrorms his day-to-day tasks. To demonstrate these methods we have trained\\nan ANS network to drive a vehicle through simulated rreeway traffic.\\n\\nI ntJooducticn\\nComputational systems employing fine grained parallelism are revolutionizing the way we\\napproach a number or long standing problems involving pattern recognition and cognitive processing. The field spans a wide variety or computational networks, rrom constructs emulating neural\\nrunctions, to more crystalline configurations that resemble systolic arrays. Several titles are used\\nto describe this broad area or research, we use the term artificial neural systems (ANS). Our concern in this work is the use or ANS ror manually training certain types or autonomous systems\\nwhere the desired rules of behavior are difficult to rormulate.\\nArtificial neural systems consist of a number or processing elements interconnected in a\\nweighted, user-specified fashion, the interconnection weights acting as memory ror the system.\\nEach processing element calculatE',> an output value based on the weighted sum or its inputs. In\\naddition, the input data is correlated with the output or desired output (specified by an instructive\\nagent) in a training rule that is used to adjust the interconnection weights. In this way the ne~\\nwork learns patterns or imitates rules of behavior and decision making.\\nThe partiCUlar ANS architecture we use is a variation of Rummelhart et. al. [lJ multi-layer\\nperceptron employing the generalized delta rule (GD R). Instead of a single, multi-layer ,structure, our final network has a a multiple component or \\\"block\\\" configuration where one blOt'k'~\\noutput reeds into another (see Figure 3). The training methodology we have developed is not\\ntied to a particular training rule or architecture and should work well with alternative networks\\nlike Grossberg's adaptive resonance model[2J.\\n\\n? American Institute of Physics 1988\\n\\n\\f694\\n\\nThe equations describing the network are derived and described in detail by Rumelhart et.\\nal.[l]. In summary, they are:\\nTransfer function:\\n\\nSj =\\n\\n?\\n\\nE WjiOi;\\n\\n(1)\\n\\ni-O\\n\\nWeight adaptation rule:\\nError calculation:\\n\\nAwl'?? =( 1- a l'..)n., l'??0 J?0??\\n\\nOJ\\n\\n+ a l'??Awp.revious\\n.'\\nl'\\n\\n'\\\"\\n\\n=0j{1- OJ) E0.tW.ti,\\n\\n( 2)\\n\\n( 3)\\n\\n.t=1\\n\\nwhere OJ is the output or processing element j or a sensor input, wi is the interconnection weight\\nleading from element ito i, n is the number of inputs to j, Aw is the adjustment of w, '1 is the\\ntraining constant, a is the training \\\"momentum,\\\" OJ is the calculated error for element i, and m\\nis the Canout oC a given element. Element zero is a constant input, equal to one, so that. WjO is\\nequivalent to the bias threshold of element j. The (1- a) factor in equation (2) differs from standard GDR formulation, but. it is useful for keeping track of the relative magnitudes of the two\\nterms. For the network's output layer the summation in equation (3) is replaced with the\\ndifference between the desired and actual output value of element j.\\nThese networks are usually trained by presenting the system with sets of input/output data\\nvectors in cyclic fashion, the entire cycle of database presentation repeated dozens of times . This\\nmethod is effective when the training agent is a computer operating in batch mode, but would be\\nintolerable for a human instructor. There are two developments that will help real-time human\\ntraining. The first is a more efficient incorporation of data/response patterns into a network. The\\nsecond, which we are addressing in this paper, is a suitable environment wherein a man and ANS\\nnetwork can interact in training situation with minimum inconvenience or boredom on the\\nhuman's part. The ability to systematically train networks in this fashion is extremely useful for\\ndeveloping certain types of expert systems including automatic signal processors, autopilots,\\nrobots and other autonomous machines. We report a number of techniques aimed at facilitating\\nthis type of training, and we propose a general method for teaching these networks .\\nSystem. Development\\n\\nOur work focuses on the utility of ANS for system control. It began as an application of\\nBarto and Sutton's associative search network[3]. Although their approach was useful in a\\nnumber of ways, it fell short when we tried to use it for capturing the subtleties of human\\ndecision-making. In response we shifted our emphasis rrom constructing goal runctions for\\nautomatic learning, to methods for training networks using direct human instruction. An integral\\npart or this is the development or suitable interraces between humans, networks and the outside\\nworld or simulator. In this section we will report various approaches to these ends, and describe a\\ngeneral methodology for manually teaching ANS networks . To demonstrate these techniques we\\ntaught a network to drive a robot vehicle down a simulated highway in traffic. This application\\ncombines binary decision making and control of continuous parameters.\\nInitially we investigated the use or automatic learning based on goal functions[3] for training control systems. We trained a network-controlled vehicle to maintain acceptable following\\ndistances from cars ahead or it. On a graphics workstation, a one lane circular track was\\n\\n\\f695\\n\\nconstructed and occupied by two vehicles: a network-controlled robot car and a pace car that\\nvaried its speed at random .. Input data to the network consisted of the separation distance and\\nthe speed of the robot vehicle . The values of a goal function were translated into desired output\\nfor GDR training. Output controls consisted of three binary decision elements : 1) accelerate one\\nincrement of speed, 2) maintain speed, and 3) decelerate one increment of speed. At all times\\nthe desired output vector had exactly one of these three elements active . The goal runction was\\nquadratic with a minimum corresponding to the optimal following distance. Although it had no\\ndirect control over the simulation, the goal function positively or negatively reinforced the\\nsystem's behavior.\\nThe network was given complete control of the robot vehicle, and the human trainer had\\nno influence except the ability to start and terminate training. This proved unsatisractory because\\nthe initial system behavior--governed by random interconnection weights--was very unstable. The\\nrobot tended to run over the car in rront of it before significant training occurred . By carerully\\nhalting and restarting training we achieved stable system behavior. At first the rollowing distance\\nmaintained by the robot car oscillated as ir the vehicle was attached by a sj)ring to the pace car.\\nThis activity gradually damped. Arter about one thousand training steps the vehicle maintained\\nthe optimal following distance and responded quickly to changes in the pace car's speed.\\nConstructing composite goal functions to promote more sophisticated abilities proved\\ndifficult, even ill-defined, because there were many unspecified parameters. To generate goal\\nrunctions ror these abilities would be similar to conventional programming--the type or labor we\\nwant to circumvent using ANS. On the other hand, humans are adept at assessing complex situations and making decisions based on qualitative data, but their \\\"goal runctions\\\" are difficult ir not\\nimpossible to capture analytically. One attraction of ANS is that it can imitate behavior based on\\nthese elusive rules without rormally specifying them. At this point we turned our efforts to\\nmanual training techniques.\\nThe initially trained network was grafted into a larger system and augmented with additional inputs: distance and speed inrormation on nearby pace cars in a second traffic lane, and an\\noutput control signal governing lane changes . The original network's ability to maintain a safe\\nfollowing distance was retained intact. Thts grafting procedure is one of two methods we studied\\nfor adding ne .... abilities to an existin, system. (The second, which employs a block structure, is\\ndescribed below.) The network remained in direct control of the robot vehicle, but a human\\ntrainer instructed it when and when not to change lanes. His commands were interpreted as the\\ndesired output and used in the GDR training algorithm. This technique, which we call coaching,\\nproved userul and the network quickly correlated its environmental inputs with the teacher's\\ninstructions. The network became adept at changing lanes and weaving through traffic. We found\\nthat the network took on the behavior pattern or its trainer. A conservative teacher produced a\\ntimid network, while an aggressive tzainer produced a network that tended to cut off other automobiles and squeeze through tight openings . Despite its success, the coaching method of training\\ndid not solve the problem or initial network instability.\\nThe stability problem was solved by giving the trainer direct control over the simulation.\\nThe system configuration (Figure 1), allows the expert to exert control or release it to the n~t?\\nwork. During initial tzaining the expert is in the driver's seat while the network acts the role of\\n\\n\\f696\\n\\napprentice. It receives sensor information, predicts system commands, and compares its predictions. against the desired output (ie. the trainer's commands) . Figure 2 shows the data and command flow in detail. Input data is processed through different channels and presented to the\\ntrainer and network. Where visual and audio formats are effective for humans, the network uses\\ninformation in vector form. This differentiation of data presentation is a limitation of the system;\\nremoving it is a cask for future ~search. The trainer issues control commands in accordance with\\nhis assigned ~k while the network takes the trainer's actions as desired system responses and\\ncorrelates these with the input. We refer to this procedure as master/apprentice training, network\\ntraining proceeds invisibly in the background as the expert proceeds with his day to day work. It\\navoids the instability problem because the network is free to make errors without the adverse\\nconsequence of throwing the operating environment into disarray.\\nI\\n\\nInput\\n\\nWorld (--> sensors)\\n\\nl+\\n\\nor\\n\\nSimulation\\n~------------------~\\n\\n~\\n\\nActuation\\n\\nI\\n\\nNe',WOrk\\n\\n~-\\n\\nI\\n\\nExpert\\n\\nCommands\\n+\\n~------~---------------------------~\\nJ\\n\\nFigure 1. A scheme for manually training ANS networks. Input data is received by both\\nthe network and trainer. The trainer issues commands that are actuated (solid command\\nline). or he coaches the network in how it ought to respond (broken command line).\\n\\n--+ Commands\\n\\nPreprocessing\\ntortunan\\nInput\\ndata\\nPreprocessing\\nfor network\\n\\nN twork\\ne\\n\\nt\\n\\n--+\\n\\nPredicted\\ncommands\\n\\n~\\n9'l. Actuation\\n\\n.1-r\\\"\\n\\n'-------------.\\nCoaching/emphasis\\n\\nTraining\\nrule\\n\\nFegure 2. Data and convnand flow In the training system. Input data is processed and presented\\n\\nto the trainer and network. In master/appre~ice training (solid command Hne). the trainer's\\norders are actuated and the network treats his commands as the system's desired output. In\\ncoaching. the network's predicted oonvnands are actuated (broken command line). and the\\ntrainer influences weight adaptation by specifying the desired system output and controlHng\\nthe values of trailing constants-his -suggestions- are not cirec:tty actuated.\\nOnce initial. bacqround wainmg is complete, the expert proceeds in a more formal\\nmanner to teach the network. He releases control of the command system to the network in\\norder to evaluate ita behavior and weaknesses. He then resumes control and works through a\\n\\n\\f697\\n\\nseries of scenarios designed to train t.he network out of its bad behavior. By switching back and\\nforth. between human and network control, the expert assesses the network's reliability and\\nteaches correct responses as needed. We find master/apprentice training works well for behavior\\ninvolving continuous functions, like steering. On the other hand, coaching is appropriate for decision Cunctions, like when Ule car ought to pass. Our methodology employs both techniques.\\nThe Driving Network\\nThe fully developed freeway simulation consists of a two lane highway that is made of\\njoined straight and curved segments which vary at. random in length (and curvature). Several\\npace cars move at random speeds near the robot vehicle. The network is given the tasks of tracking the road, negotiating curves. returning to the road if placed far afield, maintaining safe distances from the pace cars, and changing lanes when appropriate. Instead of a single multi-layer\\nstructure, the network is composed of two blocks; one controls the steering and the other regulates speed and decides when the vehicle should change lanes (Figure 3). The first block receives\\ninformation about the position and speed of the robot vehicle relative to other ears in its vicinity.\\nIts output is used to determine the automobile's speed and whet.her the robot should change\\nlanes . The passing signal is converted to a lane assignment based on the car's current lane position. The second block receives the lane assignment and data pertinent to the position and orientation of the vehicle with respect to the road. The output is used to determine the steering angle\\nof the robot car.\\n\\nBlock 1\\n\\nInputs\\n\\nOutputs\\n\\nConstant.\\nSpeed.\\nDisl. Ahead, Pl ?\\nDisl. Ahead, Ol ?\\nDist. Behind, Ol ?\\nReI. Speed Ahead, Pl ?\\nReI. Speed Ahead, Ol ?\\nReI. Speed Behind, Ol ?\\n\\nI\\n\\nSpeed\\nChange lanes\\n\\n?\\n\\nSteering Angle\\n\\nConvert lane change to lane number\\nConstant\\nRei. Orientation\\n-..--t~ lane Nurmer\\nlateral Dist.\\nCurvature\\n\\n?\\n?\\n?\\n?\\n?\\n\\n??\\n?\\n\\nFigure 3. The two blocks of the driving ANS network. Heavy arrows Indicate total interconnectivity\\nbetween layers. PL designates the traffic lane presently oca.apied by the robot vehicle, Ol refers\\nto the other lane, QJrvature refers to the road, lane nurrber is either 0 or 1, relative orientation and\\nlateral distance refers to the robot car's direction and podion relative to the road'l direction and\\ncenter line. respectively.\\n.\\n\\n\\f698\\n\\nThe input data is displayed in pictorial and textual form to the driving instructor. He views\\nthe road and nearby vehicles from the perspective of the driver's seat or overhead. The network\\nreceives information in the form of a vector whose elements have been scaled to unitary order,\\nO( 1) . Wide ranging input parameters, like distance, are compressed using the hyperbolic tangent\\nor logarithmic functions . In each block , the input layer is totally interconnected to both the ou~\\nput and a hidden layer. Our scheme trains in real time, and as we discuss later, it trains more\\nsmoothly with a small modification of the training algorithm .\\nOutput is interpreted in two ways: as a binary decision or as a continuously varying parameter. The first simply compares the sigmoid output against a threshold. The second scales the\\noutput to an appropriate range for its application . For example, on the steering output element, a\\n0.5 value is interpreted as a zero steering angle. Left and right turns of varying degrees are initiated when this output is above or below 0.5, respectively.\\nThe network is divided into two blocks that can be trained separately. Beside being conceptually easier to understand , we find this component approach is easy to train systematically.\\nBecause each block has a restricted, well-defined set of tasks, the trainer can concentrate\\nspecifically on those functions without being concerned that other aspects of the network behavior\\nare deteriorating.\\n\\\"'e trained the system from bottom up, first teaching the network to stay on the road ,\\nnegotiate curves , chan~e lanes, and how to return if the vehicle strayed off the highway. Block 2,\\nresponsible for steering, learned these skills in a few minutes using the master/apprentice mode.\\nIt tended to steer more slowly than a human but further training progressively improved its\\nresponsiveness.\\nWe experimented with different trammg constants and \\\"momentum\\\" values. Large \\\"\\nvalues, about 1, caused weights to change too coarsely. \\\" values an order of magnitude smaller\\nworked well . We found DO advantage in using momentum for this method of training , in fact,\\nthe system responded about three times more slowly when 0 =0.9 than when the momentt:m\\nterm was dropped. Our standard training parameters were\\\" =0.2, and Cl' =00\\n\\na)\\n\\n~\\n\\nDb)~~\\n\\n=D-=-~=~~--=~--= ~\\n\\nFigure 4. Typical behavior of a network-controlled vehicle (dam rectangle) when trained by\\na) a conservative miYer, ItI:I b}. reckless driver. Speed Is indicated by the length of the arrows.\\nAfter Block 2 \\\"Was trained, we gave steering control to the network and concentrated on\\nteaching the network to change lanes and adjust speed. Speed control in this ('\\\"asP. was a continuous variable and was best taught using master/apprentice training. On the other hand, the binary\\ndecision to change lanes was best taught by coaching . About ten minutes of training were needed\\nto teach the network to weave through traffic. We found that the network readily adapts the\\n\\n\\f699\\n\\nbehavioral pattern of its trainer. A conservative trainer generated a network that hardly ever\\npassed, while an aggressive trainer produced a network that drove recklessly and tended to cut off\\nother-cars (Figure 4).\\nDiscussion\\nOne of the strengths of el:pert 5ystf'mS based on ANS is that the use of input data in the\\ndecision making and control proc~ss does not have to be specified . The network adapts its internal weights to conform to input/ output correlat.ions it discovers . It is important, however, that\\ndata used by the human expert is also available to the network. The different processing of sensor data for man and network may have important consequences, key information may be\\npresented to the man but not. the machine.\\nThis difference in data processing is particularly worrisome for image data where human\\nability to extract detail is vastly superior to our au tomatic image processing capabilities. Though\\nwe would not require an image processing system to understand images, it would have to extract\\nrelevant information from cluttered backgrounds. Until we have sufficiently sophisticated algorithms or networks to do this, our efforts at constructing expert systems which halldle image data\\nare handicapped .\\nScaling input data to the unitary order of magnitude is important for training stability. 111is\\nis evident from equations (1) and (2) . The sigmoid transfer function ranges from 0.1 to 0.9 in\\napproximat.eiy four units, that is, over an 0(1) domain. If system response must change in reaction to a large, O( n) swing of a given input parameter, the weight associated with that input will\\nbe trained toward an O( n- 1) magnitude. On the other hand, if the same system responds to an\\ninput whose range is O( 1), its associated weight will also be 0(1). The weight adjustment equation does not recognize differences in weight magnitude, therefore relatively small weights will\\nundergo wild magnitude adjustments and converge weakly. On the other hand, if all input parameters are of the same magnitude their associated weights will reflect this and the training constant\\ncan be adjusted for gentle weight convergence . Because the output of hidden units are constrained between zero and one, O( 1) is a good target range for input parameters. Both the hyperbolic tangent and logarithmic functions are useful for scaling wide ranging inputs . A useful form\\nof the latter is\\n.8[I+ln(x/o)]\\n.8x/o\\n-.8[I+ln(-%/o)]\\n\\nif o<x,\\nif-o::;x::;o,\\nifx<-o,\\n\\n( 4)\\n\\nwhere 0>0 and defines the limits of the intermediate linear section, and .8 is a scaling factor.\\nThis symmetric logarithmic function is continuous in its first derivative, and useful when network\\nbehavior should change slowly as a parameter increases without bound. On the othl'r hand, if the\\nsystem should approach a limiting behavior, the tanh function is appropriate.\\nWeight adaptation is also complicated by relaxing the common practice of restricting interconnections to adjacent layers. Equation (3) shows that the calculated error for a hidden layergiven comparable weights, fanouts and output errors-will be one quarter or less than that of the\\n\\n\\f700\\n\\noutput layer. This is caused by the slope ractor, 0 .. ( 1- oil. The difference in error magnitudes is\\nnot noticeable in networks restricted to adjacent layer interconnectivity. But when this constraint\\nis released the effect of errors originating directly from an output unit has 4\\\" times the magnitude\\nand effect of an error originating from a hidden unit removed d layers from the output layer.\\nCompared to the corrections arising from the output units, those from the hidden units have little\\ninfluence on weight adjustment, and the power of a multilayer structure is weakened . The system\\nwill train if we restrict connections to adjacent layers, but it trains slowly. To compensate for this\\neffect we attenuate the error magnitudes originating from the output layer by the above factor.\\nThis heuristic procedure works well and racilitates smooth learning.\\nThough we have made progress in real-time learning systems using GDR, compared to\\nhumans-who can learn from a single data presentation-they remain relatively sluggish in learning\\nand response rates. We are interested in improvements of the GDR algorithm or alternative\\narchitectures that facilitate one-shot or rapid learning. In the latter case we are considering least\\nsquares restoration techniquesl4] and Grossberg and Carpenter's adaptive resonance modelsI3,5].\\nThe construction of automated expert systems by observation of human personnel is\\nattractive because of its efficient use of the expert's time and effort. Though the classic AI\\napproach of rule base inference is applicable when such rules are clear cut and well organized, too\\noften a human expert can not put his decision making process in words or specify the values of\\nparameters that influence him . The attraction or ANS based systems is that imitations of expert\\nbehavior emerge as a natural consequence of their training.\\n\\nReferenees\\n1) D. E. Rumelhart, G . E. Hinton, and R. J. Williams, \\\"Learning Internal Representations by\\nError Propagation,\\\" in Parallel D~tributed Proceuing: Ezploration~ in the Micro~trvcture 0/ Cognition,\\nVol. I, D. E . Rumelhart and J. L. McClelland (Eds.)' chap. 8, (1986), Bradford BooksjMIT Press,\\nCambridge\\n\\n2) S. Grossberg,\\n\\nStudie~\\n\\n0/ Mind and Brain, (1982), Reidel, Boston\\n\\n3) A. Barto and R. Sutton, \\\"Landmark Learning: An Illustration of Associative Search,\\\" BiologicaIC,6emetiu,42, (1981), p.l\\n4) A. Rosenfeld and A . Kak, Digital Pieture Proeming, Vol. 1, chap. 7, (1982), Academic Press,\\nNew York\\n\\n5) G. A. Carpenter and S. Grossberg, \\\"A Massively Parallel Architecture for a Self-organizing\\nNeural Pattern Recognition Machine,\\\" Computer Vision, Graphiu and Image Procu,ing, 37,\\n( 1987), p.54\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4da7b6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "c4da7b6c",
        "outputId": "312feadb-03a2-48cf-cc15-a6a693bd2eef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year             0\n",
              "title            0\n",
              "event_type    1000\n",
              "pdf_name         0\n",
              "abstract         0\n",
              "paper_text       0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>title</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>event_type</th>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pdf_name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abstract</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>paper_text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ed15d4",
      "metadata": {
        "id": "29ed15d4"
      },
      "source": [
        "## Create New Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1d2e77",
      "metadata": {
        "id": "5e1d2e77"
      },
      "outputs": [],
      "source": [
        "df2 = df['title'] + ' ' + df['paper_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29192ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d29192ef",
        "outputId": "458440f4-0024-46b8-9c3e-b8c13176ed43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings Independent Component Analysis for\\nidentification of artifacts in\\nMagnetoencephalographic recordings\\n\\nRicardo Vigario 1 ; Veikko J ousmiiki2 ,\\nMatti Hiimiiliiinen2, Riitta Hari2, and Erkki Oja 1\\n1 Lab.\\n\\nof Computer & Info. Science\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{Ricardo.Vigario, Erkki.Oja}@hut.fi\\n2 Brain\\n\\nResearch Unit, Low Temperature Lab.\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{veikko, msh, hari}@neuro.hut.fi\\n\\nAbstract\\nWe have studied the application of an independent component analysis\\n(ICA) approach to the identification and possible removal of artifacts\\nfrom a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude\\ndistributions over time, thus distinguishing between strictly periodical\\nsignals, and regularly and irregularly occurring signals. Many artifacts\\nbelong to the last category. In order to assess the effectiveness of the\\nmethod, controlled artifacts were produced, which included saccadic eye\\nmovements and blinks, increased muscular tension due to biting and the\\npresence of a digital watch inside the magnetically shielded room. The\\nresults demonstrate the capability of the method to identify and clearly\\nisolate the produced artifacts.\\n\\n1 Introduction\\nWhen using a magnetoencephalographic (MEG) record, as a research or clinical tool, the\\ninvestigator may face a problem of extracting the essential features of the neuromagnetic\\n? Corresponding author\\n\\n\n",
              "R. Vigario,\\n\\n230\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\nsignals in the presence of artifacts. The amplitude of the disturbance may be higher than\\nthat of the brain signals, and the artifacts may resemble pathological signals in shape. For\\nexample, the heart's electrical activity, captured by the lowest sensors of a whole-scalp\\nmagnetometer array, may resemble epileptic spikes and slow waves (Jousmili and Hari\\n1996).\\nThe identification and eventual removal of artifacts is a common problem in electroencephalography (EEG), but has been very infrequently discussed in context to MEG (Hari\\n1993; Berg and Scherg 1994).\\nThe simplest and eventually most commonly used artifact correction method is rejection,\\nbased on discarding portions of MEG that coincide with those artifacts. Other methods\\ntend to restrict the subject from producing the artifacts (e.g. by asking the subject to fix the\\neyes on a target to avoid eye-related artifacts, or to relax to avoid muscular artifacts). The\\neffectiveness of those methods can be questionable in studies of neurological patients, or\\nother non-co-operative subjects. In eye artifact canceling, other methods are available and\\nhave recently been reviewed by Vigario (I 997b) whose method is close to the one presented\\nhere, and in Jung et aI. (1998).\\nThis paper introduces a new method to separate brain activity from artifacts, based on the\\nassumption that the brain activity and the artifacts are anatomically and physiologically\\nseparate processes, and that their independence is reflected in the statistical relation between the magnetic signals generated by those processes.\\nThe remaining of the paper will include an introduction to the independent component\\nanalysis, with a presentation of the algorithm employed and some justification of this approach. Experimental data are used to illustrate the feasibility of the technique, followed\\nby a discussion on the results.\\n\\n2\\n\\nIndependent Component Analysis\\n\\nIndependent component analysis is a useful extension of the principal component analysis\\n(PC A). It has been developed some years ago in context with blind source separation applications (Jutten and Herault 1991; Comon 1994). In PCA. the eigenvectors of the signal\\ncovariance matrix C = E{xx T } give the directions oflargest variance on the input data\\nx. The principal components found by projecting x onto those perpendicular basis vectors\\nare uncorrelated, and their directions orthogonal.\\nHowever, standard PCA is not suited for dealing with non-Gaussian data. Several authors, from the signal processing to the artificial neural network communities, have shown\\nthat information obtained from a second-order method such as PCA is not enough and\\nhigher-order statistics are needed when dealing with the more demanding restriction of\\nindependence (Jutten and Herault 1991; Comon 1994). A good tutorial on neural ICA implementations is available by Karhunen et al. (1997). The particular algorithm used in this\\nstudy was presented and derived by Hyvarinen and Oja (1997a. 1997b).\\n\\n2.1\\n\\nThe model\\n\\nIn blind source separation, the original independent sources are assumed to be unknown,\\nand we only have access to their weighted sum. In this model, the signals recorded in an\\nMEG study are noted as xk(i) (i ranging from 1 to L, the number of sensors used, and\\nk denoting discrete time); see Fig. 1. Each xk(i) is expressed as the weighted sum of M\\n\\n\n",
              "ICAfor Identification of Artifacts in MEG Recordings\\n\\n231\\n\\nindependent signals Sk(j), following the vector expression:\\nM\\n\\nXk = La(j)sdj) = ASk,\\n\\n(1)\\n\\nj=l\\n\\nwhere Xk = [xk(1), ... , xk(L)]T is an L-dimensional data vector, made up of the L mixtures at discrete time k. The sk(1), ... , sk(M) are the M zero mean independent source\\nsignals, and A = [a(1), . .. , a(M)] is a mixing matrix independent of time whose elements\\nail are th.e unknown coefficients of the mixtures. In order to perform ICA, it is necessary\\nto have at least as many mixtures as there are independent sources (L ~ M). When this\\nrelation is not fully guaranteed, and the dimensionality of the problem is high enough,\\nwe should expect the first independent components to present clearly the most strongly\\nindependent signals, while the last components still consist of mixtures of the remaining\\nsignals. In our study, we did expect that the artifacts, being clearly independent from the\\nbrain activity, should come out in the first independent components. The remaining of the\\nbrain activity (e.g. a and J-L rhythms) may need some further processing.\\nThe mixing matrix A is a function of the geometry of the sources and the electrical conductivities of the brain, cerebrospinal fluid, skull and scalp. Although this matrix is unknown.\\nwe assume it to be constant, or slowly changing (to preserve some local constancy).\\nThe problem is now to estimate the independent signals Sk (j) from their mixtures, or the\\nequivalent problem of finding the separating matrix B that satisfies (see Eq. 1)\\n(2)\\nIn our algorithm, the solution uses the statistical definition of fourth-order cumulant or\\nkurtosis that, for the ith source signal, is defined as\\n\\nkurt(s(i)) = E{s(i)4} - 3[E{s(i)2}]2,\\nwhere E( s) denotes the mathematical expectation of s.\\n\\n2.2 The algorithm\\nThe initial step in source separation, using the method described in this article, is whitening, or sphering. This projection of the data is used to achieve the uncorrelation between\\nthe solutions found, which is a prerequisite of statistical independence (Hyvarinen and Oja\\n1997a). The whitening can as well be seen to ease the separation of the independent signals (Karhunen et al. 1997). It may be accomplished by PCA projection: v = V x, with\\nE{ vv T } = I. The whitening matrix V is given by\\n-=T ,\\nV -- A- 1 / 2 .....\\n\\nwhere A = diag[-\\(1), ... , -\\(M)] is a diagonal matrix with the eigenvalues of the data\\ncovariance matrix E{xxT}, and 8 a matrix with the corresponding eigenvectors as its\\ncolumns.\\nConsider a linear combination y = w T v of a sphered data vector v, with Ilwll = 1. Then\\nE{y2} = .1 andkurt(y) = E{y4}-3, whose gradientwithrespecttow is 4E{v(wTv)3} .\\nBased on this, Hyvarinen and Oja (1997a) introduced a simple and efficient fixed-point\\nalgorithm for computing ICA, calculated over sphered zero-mean vectors v, that is able to\\nfind one of the rows of the separating matrix B (noted w) and so identify one independent\\nsource at a time - the corresponding independent source can then be found using Eq. 2.\\nThis algorithm, a gradient descent over the kurtosis, is defined for a particular k as\\n1. Take a random initial vector Wo of unit norm. Let l = 1.\\n\\n\n",
              "232\\n\\nR. Vigario,\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\n2. Let Wi = E{V(W[.1 v)3} - 3Wl-I. The expectation can be estimated using a\\nlarge sample OfVk vectors (say, 1,000 vectors).\\n3. Divide Wi by its norm (e.g. the Euclidean norm\\n4.\\n\\nIlwll = JLi wI J.\\n\\nlflwT wi-II is not close enough to 1, let I = 1+1 andgo back to step 2.\\n\\nOtherwise,\\n\\noutput the vector Wi.\\n\\nIn order to estimate more than one solution, and up to a maximum of lvI, the algorithm\\nmay be run as many times as required. It is, nevertheless, necessary to remove the infonnation contained in the solutions already found, to estimate each time a different independent\\ncomponent. This can be achieved, after the fourth step of the algorithm, by simply subtracting the estimated solution s = w T v from the unsphered data Xk . As the solution is\\ndefined up to a multiplying constant, the subtracted vector must be multiplied by a vector\\ncontaining the regression coefficients over each vector component of Xk.\\n\\n3\\n\\nMethods\\n\\nThe MEG signals were recorded in a magnetically shielded room with a 122-channel\\nwhole-scalp Neuromag-122 neuromagnetometer. This device collects data at 61 locations\\nover the scalp, using orthogonal double-loop pick-up coils that couple strongly to a local\\nsource just underneath, thus making the measurement \"near-sighted\" (HamaHi.inen et al.\\n1993).\\nOne of the authors served as the subject and was seated under the magnetometer. He kept\\nhis head immobile during the measurement. He was asked to blink and make horizontal\\nsaccades, in order to produce typical ocular artifacts. Moreover, to produce myographic\\nartifacts, the subject was asked to bite his teeth for as long as 20 seconds. Yet another\\nartifact was created by placing a digital watch one meter away from the helmet into the\\nshieded room. Finally, to produce breathing artifacts, a piece of metal was placed next\\nto the navel. Vertical and horizontal electro-oculograms (VEOG and HEOG) and electrocardiogram (ECG) between both wrists were recorded simultaneously with the MEG, in\\norder to guide and ease the identification of the independent components. The bandpassfiltered MEG (0.03-90 Hz), VEOG, HEOG, and ECG (0.1-100 Hz) signals were digitized\\nat 297 Hz, and further digitally low-pass filtered, with a cutoff frequency of 45 Hz and\\ndownsampled by a factor of 2. The total length of the recording was 2 minutes. A second\\nset of recordings was perfonned, to assess the reproducibility of the results.\\nFigure 1 presents a subset of 12 spontaneous MEG signals from the frontal, temporal and\\noccipital areas. Due to the dimension of the data (122 magnetic signals were recorded), it\\nis impractical to plot all MEG signals (the complete set is available on the internet - see\\nreference list for the adress (Vigario 1997a?. Also both EOG channels and the electrocardiogram are presented.\\n\\n4\\n\\nResults\\n\\nFigure 2 shows sections of9 independent components (IC's) found from the recorded data,\\ncorresponding to a I min period, starting 1 min after the beginning of the measurements.\\nThe first two IC's, with a broad band spectrum, are clearly due to the musclular activity\\noriginated from the biting. Their separation into two components seems to correspond, on\\nthe basis of the field patterns, to two different sets of muscles that were activated during\\nthe process. IC3 and IC5 are, respectively showing the horizontal eye movements and the\\neye blinks, respectively. IC4 represents cardiac artifact that is very clearly extracted. In\\nagreement with Jousmaki and Hari (1996), the magnetic field pattern of IC4 shows some\\npredominance on the left.\\n\\n\n",
              "ICA/or Identification 0/ Artifacts in MEG Recordings\\n\\n233\\nMEG [ 1000 fTlcm\\n\\nI--\\n\\n---l\\n\\nsaccades\\n\\nI--\\n\\n---l\\n\\nblinking\\n\\nEOG [\\n\\n500 IlV\\n\\nECG [\\n\\n500 IlV\\n\\nI--\\n\\nbiting\\n\\n---l MEG\\n\\n~=::::::::::::::=::\\n~?'104~\\n\\nrJ. .........\\n\\nM\\n\\n,J.\\.......1iIIiM~..\\n\\n::\\nt...\\n\\n:;::::::;:::~=\\n\\n~::::::::;::=\\n~ ?? \",~Jrt\\n\\n..,.\\n\\nt\\n\\n....\\n\\n~,.~ . ? .J..\\n\\n.\\n\\n.../\\\"\"$\"\"~I\\n\\n2 t\\n\\n:;\\n\\n:;\\n4\\n\\n~\\n\\n5 t\\n\\n., ... ...., ,'fIJ'\\,\\n..........-.\\n\\n,..,d\\n\\n,LIlt ... .,\\n\\nI?\\n\\n.,............. ................. \"\\n\\n....,..,.\"........ .\\n\\n.... Dei ..... \"\\n\\n.'''IIb'''*. rt\\n\\n-1I\\JY. ? ---\\n\\nI p\", . . . . , . . . . . . . . . . . . at ...'....\\n\\nI; rp ..\\n\\n,P....\\n\\n,\\n\\n.,...............' tMn':M.U\\n\\n... ,\\n\\n, ..... '\\n\\nU\\..,.--II..------'-__\\n\\nooII..Jl,,-\\n\\n\".'tIItS\\n\\n5 ~\\n\\n6 t\\n\\nVEOG\\n\\nIt ... 11.1. HEOG\\n\\n~UijuJJJ.LU Wl Uij.lJU.LllU.UUUllUUij,UU~ijJJJ\\n\\nECG\\n\\n10 s\\n\\nFigure 1: Samples of MEG signals, showing artifacts produced by blinking, saccades,\\nbiting and cardiac cycle. For each of the 6 positions shown, the two orthogonal directions\\nof the sensors are plotted.\\nThe breathing artifact was visible in several independent components, e.g. IC6 and IC7. It\\nis possible that, in each breathing the relative position and orientation of the metallic piece\\nwith respect to the magnetometer has changed. Therefore, the breathing artifact would be\\nassociated with more than one column of the mixing matrix A, or to a time varying mixing\\nvector.\\nTo make the analysis less sensible to the breathing artifact, and to find the remaining artifacts, the data were high-pass filtered, with cutoff frequency at 1 Hz. Next, the independent\\ncomponent IC8 was found. It shows clearly the artifact originated at the digital watch,\\nlocated to the right side of the magnetometer.\\nThe last independent component shown, relating to the first minute of the measurement,\\nshows an independent component that is related to a sensor presenting higher RMS (root\\nmean squared) noise than the others.\\n\\n5\\n\\nDiscussion\\n\\nThe present paper introduces a new approach to artifact identification from MEG recordings, based on the statistical technique of Independent Component Analysis. Using this\\nmethod, we were able to isolate both eye movement and eye blinking artifacts, as well as\\n\\n\n",
              "R. Vigario,\\n\\n234\\n\\nv. Jousmiiki, M HtJmlJliiinen, R. Hari and E. Oja\\n\\ncardiac, myographic, and respiratory artifacts.\\nThe basic asswnption made upon the data used in the study is that of independence between brain and artifact waveforms. In most cases this independence can be verified by the\\nknown differences in physiological origins of those signals. Nevertheless, in some eventrelated potential (ERP) studies (e.g. when using infrequent or painful stimuli), both the\\ncerebral and ocular signals can be similarly time-locked to the stimulus. This local time\\ndependence could in principle affect these particular ICA studies. However, as the independence between two signals is a measure of the similarity between their joint amplitude\\ndistribution and the product of each signal's distribution (calculated throughout the entire\\nsignal, and not only close to the stimulus applied), it can be expected that the very local\\nrelation between those two signals, during stimulation, will not affect their global statistical\\nrelation.\\n\\n6\\n\\nAcknowledgment\\n\\nSupported by a grant from Junta Nacional de Investiga~ao Cientifica e Tecnologica, under\\nits 'Programa PRAXIS XXI' (R.Y.) and the Academy of Finland (R.H.).\\n\\nReferences\\nBerg, P. and M. Scherg (1994). A multiple source approach to the correction of eye\\nartifacts. Electroenceph. clin. Neurophysiol. 90, 229-241.\\nComon, P. (1994). Independent component analysis - a new concept? Signal Processing 36,287-314.\\nHamalainen, M., R. Hari, R. Ilmoniemi, 1. Knuutila, and O. Y. Lounasmaa (1993, April).\\nMagnetoencephalography-theory, instrumentation, and applications to noninvasive\\nstudies of the working human brain. Reviews o/Modern Physics 65(2), 413-497.\\nHari, R. (1993). Magnetoencephalography as a tool of clinical neurophysiology. In\\nE. Niedermeyer and F. L. da Silva (Eds.), Electroencephalography. Basic principles, clinical applications, and relatedjields, pp. 1035-1061 . Baltimore: Williams\\n& Wilkins.\\nHyvarinen, A. and E. Oja (l997a). A fast fixed-point algorithm for independent component analysis. Neural Computation (9), 1483-1492.\\nHyvarinen, A. and E. Oja (1997b). One-unit learning rules for independent component\\nanalysis. In Neural Information Processing Systems 9 (Proc. NIPS '96). MIT Press.\\nJousmiiki, Y. and R. Hari (1996). Cardiac artifacts in magnetoencephalogram. Journal\\no/Clinical Neurophysiology 13(2), 172-176.\\nJung, T.-P., C. Hwnphries, T.-W. Lee, S. Makeig, M. J. McKeown, Y. lragui, and\\nT. Sejnowski (1998). Extended ica removes artifacts from electroencephalographic\\nrecordings. In Neural Information Processing Systems 10 (Proc. NIPS '97). MIT\\nPress.\\nJutten, C. and 1. Herault (1991). Blind separation of sources, part i: an adaptive algorithm based on neuromimetic architecture. Signal Processing 24, 1-10.\\nKarhunen, J., E. Oja, L. Wang, R. Vigmo, and J. Joutsensalo (1997). A class of neural\\nnetworks for independent component analysis. IEEE Trans. Neural Networks 8(3),\\n1-19.\\nVigmo, R. (1997a). WWW adress for the MEG data:\\nhttp://nuc1eus.hut.firrvigarioINIPS97_data.html.\\nVigmo, R. (1997b). Extraction of ocular artifacts from eeg using independent component analysis. To appear in Electroenceph. c/in. Neurophysiol.\\n\\n\n",
              "ICAfor Identification ofArtifacts in MEG Recordings\\n\\n235\\n\\n~~~\\n\\nIC1\\n\\n------,--y~-------------------------.-.------~~.. ,.. ~\\nU\\n\\n...\\n\\nIC2\\n\\nIC3\\n.\",\\n\\n''' ... '' .. '\\n\\n<> .\\n).\\~\\n.\\ C:> ?\\n\\\\n\\n~~~}a\\n\\n~~-\"\\n\\n____I4-_. _\\n. . . ._.---_._. . . .-.__\\n. \"\"\"\"\"\"?t;_-\"'' '....\\n~\\n\\n. . . . .-......,.....\\n\\n~_1\\n\\nIC4\\n\\nIC5\\n\\nIC6\\n~\\n...W\"\\n....\\n\"1011\\n...~\"_f....\\n..\".,.\"\"'_\\n\\n/tJ'IfII/'h\\n\\nI' ......\\n\\nd1b ..\\n\\n~*W,.'tJ ......\\n\\nr' .. ns...\\n\\nIC7\\n\\nICB\\n\\nICg\\n~._-~.,.\\n\\n. . . . .t . .\\n\\nWt:n:ePWt.~..,.~I'NJ'~~\\nI\\n\\n10 s\\n\\nI\\n\\nFigure 2: Nine independent components found from the MEG data. For each component the\\nleft, back and right views of the field patterns generated by these components are shown full line stands for magnetic flux coming out from the head, and dotted line the flux inwards.\\n\\n\n",
              "\n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Near-Maximum Entropy Models for Binary Neural Representations of Natural Images Near-Maximum Entropy Models for Binary\\nNeural Representations of Natural Images\\n\\nMatthias Bethge and Philipp Berens\\nMax Planck Institute for Biological Cybernetics\\nSpemannstrasse 41, 72076, T?ubingen, Germany\\nmbethge,berens@tuebingen.mpg.de\\n\\nAbstract\\nMaximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these\\napproaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new\\napproach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data?the model parameters can be derived\\nin closed form and sampling is easy. Therefore, our NearMaxEnt approach can\\nserve as a tool for testing predictions from a pairwise maximum entropy model not\\nonly for low-dimensional marginals, but also for high dimensional measurements\\nof more than thousand units. We demonstrate its usefulness by studying natural\\nimages with dichotomized pixel intensities. Our results indicate that the statistics\\nof such higher-dimensional measurements exhibit additional structure that are not\\npredicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of\\ndimensionality where estimation of the full joint distribution is feasible.\\n\\n1\\n\\nIntroduction\\n\\nA core issue in sensory coding is to seek out and model statistical regularities in high-dimensional\\ndata. In particular, motivated by developments in information theory, it has been hypothesized\\nthat modeling these regularities by means of redundancy reduction constitutes an important goal of\\nearly visual processing [2]. Recent studies conjectured that the binary spike responses of retinal\\nganglion cells may be characterized completely in terms of second-order correlations when using\\na maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual\\ninput, however, this would be very surprising: Natural images are known to exhibit complex higherorder correlations which are extremely difficult to model yet being perceptually relevant. Thus, if\\nwe assume that retinal ganglion cells do not discard the information underlying these higher-order\\ncorrelations altogether, it would be a very difficult signal processing task to remove all of those\\nalready within the retinal network.\\nOftentimes, neurons involved in early visual processing are modeled as rather simple computational\\nunits akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity.\\nFor such simple neuron models, the possibility of removing higher-order correlations present in the\\ninput is very limited [3].\\nHere, we study the role of second-order correlations in the multivariate binary output statistics of\\nsuch linear-nonlinear model neurons with a threshold nonlinearity responding to natural images.\\nThat is, each unit can be described by an affine transformation zk = wkT x + ? followed by a\\npoint-wise signum function sk = sgn(zk ). Our interest in this model is twofold: (A) It can be\\nregarded a parsimonious model for the analysis of population codes of natural images for which the\\n1\\n\\n\n",
              "A\\n\\n?3\\n\\nB\\n\\n3\\n\\n0\\n\\n2\\n\\nC\\n\\n6\\n\\nJS?Divergence (bits)\\n\\n?H (%)\\n\\nx 10\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n1\\n\\nlog? H (%)\\n\\n?H (%)\\n\\n4\\n3\\n2\\n1\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n10\\n\\nD\\n10\\n\\n0.5\\n\\n0\\n\\n?5\\n\\n5\\n\\n0\\n\\n10\\n\\nx 10\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n0\\n\\n10\\n\\n?1\\n\\n10\\n\\n?2\\n\\n20\\n\\n10\\n\\nDimension\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n20\\n\\nlog 2 (Number of Samples)\\n\\nFigure 1: Similarity between the Ising and the DG model. A+C: Entropy difference ?H between the Ising\\nmodel and the Dichotomized Gaussian distribution as a function of dimensionality. A: Up to 10 dimensions\\nwe can compute HDG directly by evaluating Eq. 6. Gray dots correspond to different sets of parameters. For\\nm ? 4, the relatively large scatter and the existence of negative values is due to the limited numerical precision\\nof the Monte-Carlo integration. Errorbars show standard error of the mean. B. JS-divergence DJS between PI\\nand PDG . C. ?H as above, for higher dimensions. Up to 20 dimensions ?H remains very small. The increase\\nfor m ? 20 is most likely due to undersampling of the distributions. D. ?H as function of sample size used\\nto estimate HDG , at seven (black) and ten (grey) dimensions (note log scale on both axes). ?H decreases with\\na power law with increasing sample sizes.\\n\\ncomputational power and the bandwidth of each unit is limited. (B) The same model can also be\\nused more generally to fit multivariate binary data with given pairwise correlations, if x is drawn\\nfrom a Gaussian distribution. In particular, we will show that the resulting distribution closely\\nresembles the binary maximum entropy models known as Ising models or Boltzmann machines\\nwhich have recently become popular for the analysis of spike train recordings from retinal ganglion\\ncell responses [13, 12].\\nMotivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more general level in the following questions: are pairwise interactions enough for understanding the statistical regularities in high-dimensional natural data (given that they provide a good fit in the lowdimensional case)? If we suppose that pairwise interactions are enough, what can we say about the\\namount of redundancies in high-dimensional data? In comparison with neural spike data, natural\\nimages provide two advantages for studying these questions: 1) It is much easier to obtain large\\namounts of data with millions of samples which are less prone to nonstationarities. 2) Often differences in the higher-order statistics such as between pink noise and natural images can be recognized\\nby eye.\\n\\n2\\n\\nSecond order models for binary variables\\n\\nIn order to study whether pairwise interactions are enough to determine the statistical regularities\\nin high-dimensional data, it is necessary to be able to compute the maximum entropy distribution\\nfor large number of dimensions N . Given a set of measured statistics, maximum entropy models\\nyield a full probability distribution that is consistent with these constraints but does not impose any\\n2\\n\\n\n",
              "0.05\\n0\\n0\\n?4\\n\\n1\\n?\\n\\n2\\n\\n1\\n\\n2\\n\\nx 10\\n\\n2\\n1\\n0\\n\\n0\\n\\n?\\n\\nFigure 2: Examples of covariance matrices (A+B.) and their learned approximations (C+D) at m = 10 for\\nclarity. ? is the parameter controlling the steepness of correlation decrease. E+F. Eigenvalue spectra of both\\nmatrices. G. Entropy difference ?H and H. JS-divergence between the distribution of samples obtained from\\nthe two models at m = 7.\\n\\nadditional structure on the distribution [7]. For binary data with given mean activations ?i = hsi i\\nand correlations between neurons ?ij = hsi sj i ? hsi ihsj i, one obtains a quadratic exponential\\nprobability mass function known as the Ising model in physics or as the Boltzmann machine in\\nmachine learning.\\nCurrently all methods used to determine the parameters of such binary maximum entropy models\\nsuffer from the same drawback: since the parameters do not correspond directly to any of the measured statistics, they have to be inferred (or ?learned?) from data. In high dimensions though, this\\nposes a difficult computational problem. Therefore the characterization of complete neural circuits\\nwith possibly hundreds of neurons is still out of reach, even though analysis was recently extended\\nto up to forty neurons [14].\\nTo make the maximum entropy approach feasible in high dimensions, we propose a new strategy:\\nSampling from a ?near-maximum? entropy model that does not require any complicated learning\\nof parameters. In order to justify this approach, we verify empirically that the entropy of the full\\nprobability distributions obtained with the near-maximum entropy model are indistinguishable from\\nthose obtained with classical methods such as Gibbs sampling for up to 20 dimensions.\\n2.1\\n\\nBoltzmann machine learning\\n\\nFor a binary vector of neural activities s ? {?1, 1}m and specified ?i and ?ij the Ising model takes\\nthe form\\n?\\n?\\nm\\nX\\nX\\n1\\n1\\nPI (s) = exp ?\\nhi si +\\nJij si sj ? ,\\n(1)\\nZ\\n2\\ni=1\\ni6=j\\n\\nwhere the local fields hi and the couplings Jij have to be chosen such that hsi i = ?i and hsi sj i ?\\nhsi ihsj i = ?ij . Unfortunately, finding the correct parameters turns out to be a difficult problem\\nwhich cannot be solved in closed form.\\nTherefore, one has to resort to an optimization approach to learn the model parameters hi and Jij\\nfrom data. This problem is called Boltzmann machine learning and is based on maximization of the\\nlog-likelihood L = ln PI ({si }N\\ni=1 |h, J) [1] where N is the number of samples. The gradient of the\\nlikelihood can be computed in terms of the empirical covariance and the covariance of si and sj as\\nproduced by the current model:\\n?L\\n= hsi sj iData ? hsi sj iModel\\n?Jij\\n\\n(2)\\n\\nThe second term on the right hand side is difficult to compute, as it requires sampling from the model.\\nSince the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such\\n3\\n\\n\n",
              "Figure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from\\nthe corresponding dichotomized Gaussian distribution with equal covariance matrix (middle). It is not possible\\nto see any systematic difference between the samples from the two distributions. For comparison, this is not so\\nfor the sample from the independent model (right).\\n\\nas Gibbs sampling are employed [9] in order to approximate the required model average. This is\\ncomputationally demanding as sampling is necessary for each individual update. While efficient\\nsampling algorithms exist for special cases [6], it still remains a hard and time consuming problem\\nin the general case. Additionally, most sampling algorithms do not come with guarantees for the\\nquality of the approximation of the required average. In conclusion, parameter fitting of the Ising\\nmodel is slow and oftentimes painstaking, especially in high dimensions.\\n2.2\\n\\nModeling with the dichotomized Gaussian\\n\\nHere we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model\\nby a ?near-maximum? entropy model, for which both parameter computation and sampling is easy. A\\nvery convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian\\ndistribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated\\nfrom a hidden Gaussian variable\\nz ? N (?, ?) ,\\n\\nsi = sgn(zi ).\\n\\n(3)\\n\\nWithout loss of generality, we can assume unit variances for the Gaussian, i.e. ?ii = 1, the mean ?\\nand the covariance matrix ? of s are given by\\n?i = 2?(?i ) ? 1 ,\\n\\n?ii = 4?(?i )?(??i ) ,\\n\\n?ij = 4?(?i , ?j , ?ij ) for i 6= j\\n\\n(4)\\n\\nwhere ?(x, y, ?) = ?2 (x, y, ?) ? ?(x)?(y) . Here ? is the univariate standardized cumulative\\nGaussian distribution and ?2 its bivariate counterpart. While the computation of the model parameters was hard for the Ising model, these equations can be easily inverted to find the parameters of\\nthe hidden Gaussian distribution:\\n\u0012\\n\u0013\\n?i + 1\\n?i = ??1\\n(5)\\n2\\nDetermining ?ij generally requires to find a suitable value such that ?ij ? 4?(?i , ?j , ?ij ) = 0.\\nThis can be efficently solved by numerical computations, since the function is monotonic in ?ij\\nand has a unique\u0001 zero crossing. We obtain an especially easy case, when ?i = ?j = 0, as then\\n?ij = sin ?2 ?ij .\\nIt is also possible to evaluate the probability mass function of the DG model by numerical integration,\\nZ b1\\nZ bm\\n\u0001\\n1\\nT ?1\\nPDG (s) =\\n.\\n.\\n.\\nexp\\n?(s\\n?\\n?)\\n?\\n(s\\n?\\n?)\\n,\\n(6)\\n(2?)N/2 |?|1/2 a1\\nam\\nwhere the integration limits are chosen as ai = 0 and bi = ?, if si = 1, and ai = ?? and bi = 0,\\notherwise.\\nIn summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling\\nis easy, and (2) finding the model parameters is easy too.\\n4\\n\\n\n",
              "3\\n\\nNear-maximum entropy behavior of the dichotomized Gaussian\\ndistribution\\n\\nIn the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that\\nin many cases it can serve as a convenient approximation to the Ising model. Now, we investigate\\nhow good this approximation is. For a wide range of interaction terms and mean activations we\\nverify that the DG model closely resembles the Ising model. In particular we show that the entropy of\\nthe DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions.\\n3.1\\n\\nRandom Connectivity\\n\\nWe created randomly connected networks of varying size m, where mean activations hi and\\ninteractions\\nterms Jij were drawn from N (0, 0.4). First, we compared the entropy HI =\\nP\\n? s PI (s) log2 PI (s) of the thus specified Ising model obtained by evaluating Eq. 1 with the entropy of the DG distribution HDG computed by numerical integration1 from Eq. 6 (twenty parameter\\nsets). The entropy difference ?H = HI ? HDG was smaller than 0.002 percent of HI (Fig. 1 A,\\nnote scale) and probably within the range of the numerical integration accuracy. In addition, we\\ncomputed the Jensen-Shannon divergence DJS [PI kPDG ] = 12 (DKL [PI kM ] + DKL [PDG kM ]),\\nwhere M = 21 (PI + PDG ) [8]. We find that DJS [PI kPDG ] is extremly small up to 10 dimensions\\n(Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also\\nto have a very similar structure.\\nNext, we extended this analysis to networks of larger size and repeated the same analysis for up to\\ntwenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m ? 20 due\\nto the large number of states, we used a histogram based estimate of PDG (using 3 ? 106 samples\\nfor m < 15 and 15 ? 106 samples for m ? 15). The estimate of ?H is still very small at high\\ndimensions (Fig. 1 C, below 0.5%). We also computed DJS , which scaled similarly to ?H (data\\nnot shown).\\nIn Fig. 1 C, ?H seems to increase with dimensionality. Therefore, we investigated how the estimate\\nof ?H is influenced by the number of samples used. We computed both quantities for varying numbers of samples from the DG distribution (for m = 7, 10). As ?H decreases according to a power\\nlaw with increasing m, the rise of ?H observed in Fig. 1 C is most likely due to undersampling of\\nthe distribution.\\n3.2\\n\\nSpecified covariance structure\\n\\nTo explore the relationship between the two techniques more systematically, we generated covariance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the nth\\ndiagonal is set to a constant value exp(?? ? n) (Fig. 2A and B, m = 7, 10). We varied the decay\\nparameter ?, which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E\\nand F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The covariance matrix of the samples drawn from the Ising model resembles the original very closely (Fig.\\n2C and D). We also computed the entropy of the DG model using the desired covariance structure.\\nWe estimated ?H and DJS [PG kPDG ] averaged over 10 trials with 105 samples obtained by Gibbs\\nsampling from the Ising model. ?H is very close to zero (Fig. 2G, m = 7) except for small ?s\\nand never exceeded 0.05%. Moreover, the structure of both distributions seems to be very similar as\\nwell (Fig. 2H, m = 7). At m = 10, both quantities scaled qualitatively similair (data not shown).\\nWe also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data\\nnot shown).\\nOur experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good\\napproximation to the quadratic exponential distribution for a large parameter range. In the following\\nsection, we will exploit the similarity between the two models to study how the role of second-order\\ncorrelations may change between low-dimensional and high-dimensional statistics in case of natural\\nimages.\\n1\\nFor integration, we used the mvncdf function of Matlab. For m ? 4 this function employs Monte-Carlo\\nintegration.\\n\\n5\\n\\n\n",
              "Figure 4: A: Negative log probabilities of the DG model are plotted against ground truth (red dots). Identical\\ndistributions fall on the diagonal. Data points outside the area enclosed by the dashed lines indicate significant\\ndifferences between the model and ground truth. The DG model matches the true distribution very well. For\\ncomparison the independent model is shown as well (blue crosses). B: The multi-information of the true\\ndistribution (blue dots) accurately agrees with the multi-information of the DG model (red line). Similar to\\nthe analysis in [12], we observe a power law behavior of the entropy of the independent model (black solid\\nline) and the mutli-information. Linear extrapolation (in the log-log plot) to higher dimensions is indicated by\\ndashed lines. C: Different way of presentation of the same data as in B: the joint entropy H = Hindep ? I\\n(blue dots) is plotted instead of I and the axis are in linear scale. The dashed red line represents the same\\nextrapolation as in B.\\n\\n4\\n\\nNatural images: Second order and beyond\\n\\nWe now investigate to which extent the statistics of natural images with dichotomized pixel intensities can be characterized by pairwise correlations only. In particular, we would like to know how\\nthe role of pairwise correlations opposed to higher-order correlations changes depending on the dimensionality. Thanks to the DG model introduced above, we are in the position to study the effect\\nof pairwise correlations for high-dimensional binary random variables (N ? 1000 or even larger).\\nWe use the van Hateren image database in log-intensity scale, from which we sample small image\\npatches at random positions. The threshold for the dichotomization is set to the median of pixel\\nintensities. That is, each binary variable encodes whether the corresponding pixel intensity is above\\nor below the median over the ensemble. Up to patch sizes of 4 ? 4 pixel, the true joint statistics can\\nbe assessed using nonparametric histogram methods. Before we present quantitative comparisons, it\\nis instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model\\nwith same mean and covariance (Fig. 3, middle), and from the corresponding independent model\\n(Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well.\\nIn order to quantify how well the DG model matches the true distribution, we draw two independent\\nsets of samples from each (N = 2 ? 106 for each set) and generate a scatter plot as shown in\\nFig. 4 A for 4 ? 4 image patches. Each dot corresponds to one of the 216 = 65536 possible different\\nbinary patterns. The relative frequencies of these patterns according to the DG model (red dots) and\\naccording to the independent model (blue dots) are plotted against the relative frequencies obtained\\nfrom the natural image patches. The solid diagonal line corresponds to a perfect match between\\nmodel and ground truth. The dashed lines enclose the regions within which deviations are to be\\nexpected due to the finite sampling size. Since most of the red dots fall within this region, the DG\\nmodel fits the data distribution very well.\\nP\\nWe also systematically evaluated the JS-divergence and the multi-information I[S] = k H[Sk ] ?\\nH[S] as a function of dimensionality. That is, we started with the bivariate marginal distribution\\nof two randomly selected pixels. Then we incrementally added more pixels of random location\\nuntil the random vector contains all the 16 pixels of the 4 ? 4 image patches. Independent of the\\ndimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015\\nbits. For comparison, the JS-divergence between the independent model and the true distribution\\nincreases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in\\nthe case of 16 pixels. For two independent sets of samples both drawn from natural image data the\\nJS-divergence ranges between 0.006 and 0.007 bits for 4 ? 4 patches setting the gold standard for\\nthe minimal possible JS-divergence one could achieve with any model due to finite sampling size.\\nCarrying out the same type of analysis as in [12], we make qualitatively the same observations as it\\nwas reported there: as shown above, we find a quite accurate match between the two distributions.\\n6\\n\\n\n",
              "Figure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and\\nfrom the corresponding dichotomized Gaussian distribution with equal covariance matrix (right). For the latter, the percept of typical objects is missing due to the ignorance of higher-order correlations. This striking\\ndifference is not obvious, however, at the level of 4x4 patches, for which we found an excellent match of the\\ndichotomized Gaussian to the ensemble of natural images.\\n\\nFurthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue\\ndots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings\\ncan be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12],\\ntwo claims about the higher-dimensional statistics have been based on these two observations: First,\\nthat pairwise correlations may be sufficient to determine the full statistics of binary responses, and\\nsecondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards\\nstrong order.\\nUsing natural images instead of retinal ganglion cell data, we would like to verify to what extent\\nthe low-dimensional observations can be used to support these claims about the high-dimensional\\nstatistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions\\n(dashed lines) as in [12]. The difference between the entropy of the independent model and the\\nmulti-information yields the joint entropy of the respective distribution. If the extrapolation is taken\\nseriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint\\nentropy of the neural responses approaches zero at this size?say for 7 ? 7 image patches (Fig. 4 C).\\nThough it was not taken literally, this point of ?freezing? has been pointed out in [12] as a critical\\nnetwork size at which a transition to strong order is to be expected. The meaning of this assertion,\\nhowever, is not clear. First of all, the joint entropy of a distribution can never be smaller than the\\njoint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing\\nnumber of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to\\nask more precisely how the growth rate of the joint entropy can be characterized and whether there\\nis a critical number of dimensions at which the growth rate suddenly drops. In our study with natural\\nimages, visual inspection does not indicate anything special to happen at the ?critical patch size? of\\n7 ? 7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5\\n(right) we show a sample from the DG model for 32?32 image patches (i.e. 1024 dimensions) which\\nprovides no indication for a particularly interesting change in the statistics towards strong order. The\\nexact law according to which the multi-information grows with the number of dimensions for large\\nm, however, is not easily assessed and remains to be explored.\\nFinally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions\\ndoes not hold any more in the case of large m: the samples from the true distribution at the left\\nhand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5,\\nright), indicating that pairwise correlations do not suffice to determine the full statistics of large\\nimage patches. Even if the match between the DG model and the Ising model may turn out to be\\nless accurate in high dimensions, this would not affect our conclusion. Any mismatch would only\\nintroduce more order in the DG model than justified by pairwise correlations only.\\n\\n5\\n\\nConclusion and Outlook\\n\\nWe proposed a new approach to maximum entropy modeling of binary variables, extending maximum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa7\\n\\n\n",
              "rameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of\\nMonte-Carlo methods. We verified numerically that the empirical entropy of the DG model is comparable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes,\\nthe DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due\\nto the lack of independence between consecutive samples in the Gibbs sampler.\\nAlthough the Ising model and the DG model are in principle different, the match between the two\\nturns out to be surprisingly good for a large region of the parameter space. Currently, we are trying\\nto determine where the close similarity between the Ising model and the DG model breaks down.\\nIn addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal\\ndensity for Monte-Carlo methods such as importance sampling. As it is a very close approximation\\nto the Ising model, we expect this combination to yield highly efficient sampling behaviour. In\\nsummary, by linking the DG model to the Ising model, we believe that maximum entropy modeling\\nof multivariate binary random variables will become much more practical in the future.\\nWe used the DG model to investigate the role of second-order correlations in the context of sensory coding of natural images. While for small image patches the DG model provided an excellent\\nfit to the true distribution, we were able to show that this agreement breakes down in the case\\nof larger image patches. Thus caution is required when extrapolating from low-dimensional measurements to higher-dimensional distributions because higher-order correlations may be invisible in\\nlow-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be\\na promising tool for the analysis of correlated neural activities, and the DG model can facilitate its\\nuse significantly in practice.\\nAcknowledgments\\nWe thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stimulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute advice. An implementation of the DG model in Matlab and R will be avaible at our website\\nhttp://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling.\\n\\nReferences\\n[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive\\nScience, 9:147?169, 1985.\\n[2] H.B. Barlow. Sensory mechanisms, the reduction of redundancy, and intelligence. In The Mechanisation\\nof Thought Processes, pages 535?539, London: Her Majesty?s Stationery Office, 1959.\\n[3] M. Bethge. Factorial coding of natural images: How effective are linear model in removing higher-order\\ndependencies? J. Opt. Soc. Am. A, 23(6):1253?1268, June 2006.\\n[4] D.R. Cox and N. Wermuth. On some models for multivariate binary variables parallel in complexity with\\nthe multivariate gaussian distribution. Biometrika, 89:462?469, 2002.\\n[5] L.J. Emrich and M.R. Piedmonte. A method for generating high-dimensional multivariate binary variates.\\nThe American Statistician, 45(4):302?304, 1991.\\n[6] M. Huber. A bounding chain for swendsen-wang. Random Structures & Algorithms, 22:53?59, 2002.\\n[7] E.T. Jaynes. Where do we stand on maximum entropy inference. In R.D. Levine and M. Tribus, editors,\\nThe Maximum Entropy Formalism. MIT Press, Cambridge, MA, 1978.\\n[8] J. Linn. Divergence measures based on the shannon entropy. IEEE Trans Inf Theory, 37:145?151, 1991.\\n[9] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press,\\n2003.\\n[10] Sheila H Nirenberg and Jonathan D Victor. Analyzing the activity of large populations of neurons: how\\ntractable is the problem? Current Opinion in Neurobiology, 17:397?400, August 2007.\\n[11] Karl Pearson. On a new method of determining correlation between a measured character a, and a character b, of which only the percentage of cases wherein b exceeds (or falls short of) a given intensity is\\nrecorded for each grade of a. Biometrika, 7:96?105, 1909.\\n[12] Elad Schneidman, Michael J Berry, Ronen Segev, and William Bialek. Weak pairwise correlations imply\\nstrongly correlated network states in a neural population. Nature, 440(7087):1007?1012, Apr 2006.\\n[13] J Shlens, JD Field, JL Gauthier, MI Grivich, D Petrusca, A Sher, AM Litke, and EJ Chichilnisky. The\\nstructure of multi-neuron firing patterns in primate retina. J Neurosci, 26(32):8254?8266, Aug 2006.\\n[14] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek. Ising models for networks of real neurons. arXiv:qbio.NC/0611072, 1:1?4, 2006.\\n\\n8\\n\\n\n",
              "\n",
              "2    Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions Nearest-Neighbor Sample Compression:\\nEfficiency, Consistency, Infinite Dimensions\\n\\nAryeh Kontorovich\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nkaryeh@cs.bgu.ac.il\\n\\nSivan Sabato\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nsabatos@bgu.ac.il\\n\\nRoi Weiss\\nDepartment of Computer Science and Applied Mathematics\\nWeizmann Institute of Science\\nroiw@weizmann.ac.il\\n\\nAbstract\\nWe examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based\\nmulticlass learning algorithm. This algorithm is derived from sample compression\\nbounds and enjoys the statistical advantages of tight, fully empirical generalization\\nbounds, as well as the algorithmic advantages of a faster runtime and memory\\nsavings. We prove that this algorithm is strongly Bayes-consistent in metric\\nspaces with finite doubling dimension ? the first consistency result for an efficient\\nnearest-neighbor sample compression scheme. Rather surprisingly, we discover\\nthat this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which\\nclassic consistency proofs hinge are violated. This is all the more surprising, since\\nit is known that k-NN is not Bayes-consistent in this setting. We pose several\\nchallenging open problems for future research.\\n\\n1\\n\\nIntroduction\\n\\nThis paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by\\nFix and Hodges in 1951 [16], this seemingly naive learning paradigm remains competitive against\\nmore sophisticated methods [8, 46] and, in its celebrated k-NN version, has been placed on a solid\\ntheoretical foundation [11, 44, 13, 47].\\nAlthough the classic 1-NN is well known to be inconsistent in general, in recent years a series of\\npapers has presented variations on the theme of a regularized 1-NN classifier, as an alternative to the\\nBayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can\\nact as a regularizer, actually improving generalization performance rather than just injecting noise.\\nIn a follow-up work, [27] showed that applying Structural Risk Minimization to (essentially) the\\nmargin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classifier.\\nA further development has seen margin-based regularization analyzed through the lens of sample\\ncompression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later\\nextended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in\\n[27], margin-regularized 1-NN methods enjoy a number of statistical and computational advantages\\nover the traditional k-NN classifier. Salient among these are explicit data-dependent generalization\\nbounds, and considerable runtime and memory savings. Sample compression affords additional\\nadvantages, in the form of tighter generalization bounds and increased efficiency in time and space.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\n",
              "In this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning\\nalgorithm, in both finite-dimensional and infinite-dimensional metric spaces. The algorithm is\\nessentially the passive component of the active learner proposed by Kontorovich, Sabato, and Urner\\nin [25], and we refer to it in the sequel as KSU; for completeness, we present it here in full (Alg. 1).\\nWe show that in finite-dimensional metric spaces, KSU is both computationally efficient and Bayesconsistent. This is the first compression-based multiclass 1-NN algorithm proven to possess both of\\nthese properties. We further exhibit a surprising phenomenon in infinite-dimensional spaces, where\\nwe construct a distribution for which KSU is Bayes-consistent while k-NN is not.\\nMain results. Our main contributions consist of analyzing the performance of KSU in finite and\\ninfinite dimensional settings, and comparing it to the classical k-NN learner. Our key findings are\\nsummarized below.\\n? In Theorem 2, we show that KSU is computationally efficient and strongly Bayes-consistent\\nin metric spaces with a finite doubling dimension. This is the first (strong or otherwise)\\nBayes-consistency result for an efficient sample compression scheme for a multiclass (or\\neven binary)1 1-NN algorithm. This result should be contrasted with the one in [27], where\\nmargin-based regularization was employed, but not compression; the proof techniques\\nfrom [27] do not carry over to the compression-based scheme. Instead, novel arguments\\nare required, as we discuss below. The new sample compression technique provides a\\nBayes-consistency proof for multiple (even countably many) labels; this is contrasted with\\nthe multiclass 1-NN algorithm in [28], which is not compression-based, and requires solving\\na minimum vertex cover problem, thereby imposing a 2-approximation factor whenever\\nthere are more than two labels.\\n? In Theorem 4, we make the surprising discovery that KSU continues to be Bayes-consistent\\nin a certain infinite-dimensional setting, even though this setting violates the basic measuretheoretic conditions on which classic consistency proofs hinge, including Theorem 2. This\\nis all the more surprising, since it is known that k-NN is not Bayes-consistent for this\\nconstruction [9]. We are currently unaware of any separable2 metric probability space on\\nwhich KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.\\nOur results indicate that in finite dimensions, an efficient, compression-based, Bayes-consistent\\nmulticlass 1-NN algorithm exists, and hence can be offered as an alternative to k-NN, which is well\\nknown to be Bayes-consistent in finite dimensions [12, 41]. In contrast, in infinite dimensions, our\\nresults show that the condition characterizing the Bayes-consistency of k-NN does not extend to all\\nNN algorithms. It is an open problem to characterize the necessary and sufficient conditions for the\\nexistence of a Bayes-consistent NN-based algorithm in infinite dimensions.\\nRelated work. Following the pioneering work of [11] on nearest-neighbor classification, it was\\nshown by [13, 47, 14] that the k-NN classifier is strongly Bayes consistent in Rd . These results\\nmade extensive use of the Euclidean structure of Rd , but in [41] a weak Bayes-consistency result was\\nshown for metric spaces with a bounded diameter and a bounded doubling dimension, and additional\\ndistributional smoothness assumptions. More recently, some of the classic results on k-NN risk\\ndecay rates were refined by [10] in an analysis that captures the interplay between the metric and the\\nsampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e.,\\nthe so-called curse of dimensionality), and Pestov [33, 34] examines this phenomenon closely under\\nvarious distributional and structural assumptions.\\nConsistency of NN-type algorithms in more general (and in particular infinite-dimensional) metric\\nspaces was discussed in [1, 5, 6, 9, 30]. In [1, 9], characterizations of Bayes-consistency were\\ngiven in terms of Besicovitch-type conditions (see Eq. (3)). In [1], a generalized ?moving window?\\nclassification rule is used and additional regularity conditions on the regression function are imposed.\\nThe filtering technique (i.e., taking the first d coordinates in some basis representation) was shown to\\nbe universally consistent in [5]. However, that algorithm suffers from the cost of cross-validating\\nover both the dimension d and number of neighbors k. Also, the technique is only applicable in\\n1\\nAn efficient sample compression algorithm was given in [20] for the binary case, but no Bayes-consistency\\nguarantee is known for it.\\n2\\nC?rou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor\\nmethods, including k-NN and KSU, obviously fail.\\n\\n2\\n\\n\n",
              "Hilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency,\\nwithout finite-sample bounds such as those provided by KSU. The insight of [5] is extended to the\\nmore general Banach spaces in [6] under various regularity assumptions.\\nNone of the aforementioned generalization results for NN-based techniques are in the form of\\nfully empirical, explicitly computable sample-dependent error bounds. Rather, they are stated in\\nterms of the unknown Bayes-optimal rate, and some involve additional parameters quantifying the\\nwell-behavedness of the unknown distribution (see [27] for a detailed discussion). As such, these\\nguarantees do not enable a practitioner to compute a numerical generalization error estimate for a\\ngiven training sample, much less allow for a data-dependent selection of k, which must be tuned via\\ncross-validation. The asymptotic expansions in [43, 37, 23, 40] likewise do not provide a computable\\nfinite-sample bound. The quest for such bounds was a key motivation behind the series of works\\n[18, 28, 20], of which KSU [25] is the latest development.\\nThe work of Devroye et al. [14, Theorem 21.2] has implications for 1-NN classifiers in Rd that\\nare defined based on data-dependent majority-vote partitions of the space. It is shown that under\\nsome conditions, a fixed mapping from each sample size to a data-dependent partition rule induces a\\nstrongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC\\ndimension, and since this rule must be fixed in advance, the algorithm is not fully adaptive. Theorem\\n19.3 ibid. proves weak consistency for an inefficient compression-based algorithm, which selects\\namong all the possible compression sets of a certain size, and maintains a certain rate of compression\\nrelative to the sample size. The generalizing power of sample compression was independently\\ndiscovered by [31], and later elaborated upon by [22]. In the context of NN classification, [14] lists\\nvarious condensing heuristics (which have no known performance guarantees) and leaves open the\\nalgorithmic question of how to minimize the empirical risk over all subsets of a given size.\\nThe first compression-based 1-NN algorithm with provable optimality guarantees was given in [20];\\nit was based on constructing ?-nets in spaces with a finite doubling dimension. The compression\\nsize of this construction was shown to be nearly unimprovable by an efficient algorithm unless P=NP.\\nWith ?-nets as its algorithmic engine, KSU inherits this near-optimality. The compression-based\\n1-NN paradigm was later extended to semimetrics in [21], where it was shown to survive violations\\nof the triangle inequality, while the hierarchy-based search methods that have become standard for\\nmetric spaces (such as [4, 18] and related approaches) all break down.\\nIt was shown in [27] that a margin-regularized 1-NN learner (essentially, the one proposed in [18],\\nwhich, unlike [20], did not involve sample compression) becomes strongly Bayes-consistent when the\\nmargin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based\\ntechnique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm\\nrelied on computing a minimum vertex cover, it was not possible to make it both computationally\\nefficient and Bayes-consistent when the number of lables exceeds two. An additional improvement\\nover [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence\\non the number of labels, while our compression scheme extends seamlessly to countable label spaces.\\nPaper outline. After fixing the notation and setup in Sec. 2, in Sec. 3 we present KSU, the\\ncompression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions\\nregarding KSU, together with some open problems. High-level proof sketches are given in Sec. 5 for\\nthe finite-dimensional case, and Sec. 6 for the infinite-dimensional case. Full detailed proofs can be\\nfound in [26].\\n\\n2\\n\\nSetting and Notation\\n\\nOur instance space is the metric space (X , ?), where X is the instance domain and ? is the metric.\\n(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable\\nlabel space Y. The unknown sampling distribution is a probability measure ?\\n? over X ? Y, with\\nmarginal ? over X . Denote by (X, Y ) ? ?\\n? a pair drawn according to ?\\n?. The generalization error of a\\nclassifier f : X ? Y is given by err?? (f ) := P?? (Y 6= f (X)),\\nP and its empirical error with respect to\\na labeled set S 0 ? X ? Y is given by err(f,\\nc S 0 ) := |S10 | (x,y)?S 0 1[y 6= f (x)]. The optimal Bayes\\nrisk of ?\\n? is R??? := inf err?? (f ), where the infimum is taken over all measurable classifiers f : X ? Y.\\nWe say that ?\\n? is realizable when R??? = 0. We omit the overline in ?\\n? in the sequel when there is no\\nambiguity.\\n3\\n\\n\n",
              "For a finite labeled set S ? X ? Y and any x ? X , let Xnn (x, S) be the nearest neighbor of x with\\nrespect to S and let Ynn (x, S) be the nearest neighbor label of x with respect to S:\\n(Xnn (x, S), Ynn (x, S)) := argmin ?(x, x0 ),\\n(x0 ,y 0 )?S\\n\\nwhere ties are broken arbitrarily. The 1-NN classifier induced by S is denoted by hS (x) :=\\nYnn (x, S). The set of points in S, denoted by X = {X1 , . . . , X|S| } ? X , induces\\na Voronoi partition of X , V(X) := {V1 (X), . . . , V|S| (X)}, where each Voronoi cell is\\nVi (X) := {x ? X : argminj?{1,...,|S|} ?(x, Xj ) = i}. By definition, ?x ? Vi (X), hS (x) = Yi .\\nA 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ? ?\\n?n to a labeled set Sn0 ? X ? Y,\\nyielding the 1-NN classifier hSn0 . While the classic 1-NN algorithm sets Sn0 := Sn , in this work we\\nstudy a compression-based algorithm which sets Sn0 adaptively, as discussed further below.\\nA 1-NN algorithm is strongly Bayes-consistent on ?\\n? if err(hSn0 ) converges to R? almost surely,\\nthat is P[limn?? err(hSn0 ) = R? ] = 1. An algorithm is weakly Bayes-consistent on ?\\n? if err(hSn0 )\\nconverges to R? in expectation, limn?? E[err(hSn0 )] = R? . Obviously, the former implies the\\nlatter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all\\ndistributions in the metric space.\\nA convenient property that is used when studying the Bayes-consistency of algorithms in metric\\nspaces is the doubling dimension. Denote the open ball of radius r around x by Br (x) := {x0 ?\\n?r (x) denote the corresponding closed ball. The doubling dimension of a\\nX : ?(x, x0 ) < r} and let B\\nmetric space (X , ?) is defined as follows. Let n be the smallest number such that every ball in X can\\nbe covered by n balls of half its radius, where all balls are centered at points of X . Formally,\\nn := min{n ? N : ?x ? X , r > 0, ?x1 , . . . , xn ? X s.t. Br (x) ? ?ni=1 Br/2 (xi )}.\\nThen the doubling dimension of (X , ?) is defined by ddim(X , ?) := log2 n.\\nFor an integer n, let [n] := {1, . . . , n}. Denote the set of all index vectors of length d by In,d :=\\n[n]d . Given a labeled set Sn = (Xi , Yi )i?[n] and any i = {i1 , . . . , id } ? In,d , denote the subsample of Sn indexed by i by Sn (i) := {(Xi1 , Yi1 ), . . . , (Xid , Yid )}. Similarly, for a vector Y 0 =\\n{Y10 , . . . , Yd0 } ? Y d , denote by Sn (i, Y 0 ) := {(Xi1 , Y10 ), . . . , (Xid , Yd0 )}, namely the sub-sample\\nof Sn as determined by i where the labels are replaced with Y 0 . Lastly, for i, j ? In,d , we denote\\nSn (i; j) := {(Xi1 , Yj1 ), . . . , (Xid , Yjd )}.\\n\\n3\\n\\n1-NN majority-based compression\\n\\nIn this work we consider the 1-NN majority-based compression algorithm proposed in [25], which\\nwe refer to as KSU. This algorithm is based on constructing ?-nets at different scales; for ? > 0\\nand A ? X , a set X ? A is said to be a ?-net of A if ?a ? A, ?x ? X : ?(a, x) ? ? and for all\\nx 6= x0 ? X, ?(x, x0 ) > ?.3\\nThe algorithm (see Alg. 1) operates as follows. Given an input sample Sn , whose set of points is\\ndenoted Xn = {X1 , . . . , Xn }, KSU considers all possible scales ? > 0. For each such scale it\\nconstructs a ?-net of Xn . Denote this ?-net by X(?) := {Xi1 , . . . , Xim }, where m ? m(?) denotes\\nits size and i ? i(?) := {i1 , . . . , im } ? In,m denotes the indices selected from Sn for this ?-net.\\nFor every such ?-net, the algorithm attaches the labels Y 0 ? Y 0 (?) ? Y m , which are the empirical\\nmajority-vote labels in the respective Voronoi cells in the partition V(X(?)) = {V1 , . . . , Vm }.\\nFormally, for i ? [m],\\nYi0 ? argmax |{j ? [n] | Xj ? Vi , Yj = y}|,\\n(1)\\ny?Y\\n\\nwhere ties are broken arbitrarily. This procedure creates a labeled set Sn0 (?) := Sn (i(?), Y 0 (?)) for\\nevery relevant ? ? {?(Xi , Xj ) | i, j ? [n]} \\ {0}. The algorithm then selects a single ?, denoted\\n? ? ? ?n? , and outputs hSn0 (? ? ) . The scale ? ? is selected so as to minimize a generalization error\\nbound, which upper bounds err(Sn0 (?)) with high probability. This error bound, denoted Q in the\\nalgorithm, can be derived using a compression-based analysis, as described below.\\n3\\nFor technical reasons, having to do with the construction in Sec. 6, we depart slightly from the standard\\ndefinition of a ?-net X ? A. The classic definition requires that (i) ?a ? A, ?x ? X : ?(a, x) < ? and (ii)\\n?x 6= x0 ? X : ?(x, x0 ) ? ?. In our definition, the relations < and ? in (i) and (ii) are replaced by ? and >.\\n\\n4\\n\\n\n",
              "Algorithm 1 KSU: 1-NN compression-based algorithm\\nRequire: Sample Sn = (Xi , Yi )i?[n] , confidence ?\\nEnsure: A 1-NN classifier\\n1: Let ? := {?(Xi , Xj ) | i, j ? [n]} \\ {0}\\n2: for ? ? ? do\\n3:\\nLet X(?) be a ?-net of {X1 , . . . , Xn }\\n4:\\nLet m(?) := |X(?)|\\n5:\\nFor each i ? [m(?)], let Yi0 be the majority label in Vi (X(?)) as defined in Eq. (1)\\n6:\\nSet Sn0 (?) := (X(?), Y 0 (?))\\n7: end for\\n8: Set ?(?) := err(h\\nc Sn0 (?) , Sn )\\n9: Find ?n? ? argmin??? Q(n, ?(?), 2m(?), ?), where Q is, e.g., as in Eq. (2)\\n10: Set Sn0 := Sn0 (?n? )\\n11: return hSn0\\n\\nm\\nWe say that a mapping Sn 7? Sn0 is a compression scheme if there is a function C : ??\\nm=0 (X ?Y) ?\\n2X ?Y , from sub-samples to subsets of X ? Y, such that for every Sn there exists an m and a sequence\\ni ? In,m such that Sn0 = C(Sn (i)). Given a compression scheme Sn 7? Sn0 and a matching function\\nC, we say that a specific Sn0 is an (?, m)-compression of a given Sn if Sn0 = C(Sn (i)) for some\\ni ? In,m and err(h\\nc Sn0 , Sn ) ? ?. The generalization power of compression was recognized by [17]\\nand [22]. Specifically, it was shown in [21, Theorem 8] that if the mapping Sn 7? Sn0 is a compression\\nscheme, then with probability at least 1 ? ?, for any Sn0 which is an (?, m)-compression of Sn ? ?\\n?n ,\\nwe have (omitting the constants, explicitly provided therein, which do not affect our analysis)\\ns\\nnm\\n? log(n) + log(1/?)\\nn\\nm log(n) + log(1/?)\\nerr(hSn0 ) ?\\n? + O(\\n) + O( n?m\\n). (2)\\nn?m\\nn?m\\nn?m\\n\\nDefining Q(n, ?, m, ?) as the RHS of Eq. (2) provides KSU with a compression bound. The following\\nproposition shows that KSU is a compression scheme, which enables us to use Eq. (2) with the\\nappropriate substitution.4\\nProposition 1. The mapping Sn 7? Sn0 defined by Alg. 1 is a compression scheme whose output Sn0\\nis a (err(h\\nc Sn0 ), 2|Sn0 |)-compression of Sn .\\n? i , Y?i )i?[2m] ) = (X\\n? i , Y?i+m )i?[m] , and observe that for all\\nProof. Define the function C by C((X\\n0\\nSn , we have Sn = C(Sn (i(?); j(?))), where i(?) is the ?-net index set as defined above, and\\nj(?) = {j1 , . . . , jm(?) } ? In,m(?) is some index vector such that Yi0 = Yji for every i ? [m(?)].\\nSince Yi0 is an empirical majority vote, clearly such a j exists. Under this scheme, the output Sn0 of\\nthis algorithm is a (err(h\\nc Sn0 ), 2|Sn0 |)-compression.\\nKSU is efficient, for any countable Y. Indeed, Alg. 1 has a naive runtime complexity of O(n4 ), since\\nO(n2 ) values of ? are considered and a ?-net is constructed for each one in time O(n2 ) (see [20,\\nAlgorithm 1]). Improved runtimes can be obtained, e.g., using the methods in [29, 18]. In this work\\nwe focus on the Bayes-consistency of KSU, rather than optimize its computational complexity. Our\\nBayes-consistency results below hold for KSU, whenever the generalization bound Q(n, ?, m, ?n )\\nsatisfies the following properties:\\nProperty 1 For any integer n and ? ? (0, 1), with probability 1 ? ? over the i.i.d. random sample\\nSn ? ?\\n?n , for all ? ? [0, 1] and m ? [n]: If Sn0 is an (?, m)-compression of Sn , then\\nerr(hSn0 ) ? Q(n, ?, m, ?).\\nProperty 2 Q is monotonically increasing in ? and in m.\\nProperty 3 There is a sequence {?n }?\\nn=1 , ?n ? (0, 1) such that\\nlim\\n\\nP?\\n\\nn=1 ?n\\n\\n< ? and for all m,\\n\\nsup (Q(n, ?, m, ?n ) ? ?) = 0.\\n\\nn?? ??[0,1]\\n4\\n\\nIn [25] the analysis was based on compression with side information, and does not extend to infinite Y.\\n\\n5\\n\\n\n",
              "The compression bound in Eq. (2) clearly\\nP?satisfies these properties. Note that Property 3 is satisfied\\nby Eq. (2) using any convergent series n=1 ?n < ? such that ?n = e?o(n) ; in particular, the decay\\nof ?n cannot be too rapid.\\n\\n4\\n\\nMain results\\n\\nIn this section we describe our main results. The proofs appear in subsequent sections. First, we show\\nthat KSU is Bayes-consistent if the instance space has a finite doubling dimension. This contrasts\\nwith classical 1-NN, which is only Bayes-consistent if the distribution is realizable.\\nTheorem 2. Let (X , ?) be a metric space with a finite doubling-dimension. Let Q be a generalization\\nbound that satisfies Properties 1-3, and let ?n be as stipulated by Property 3 for Q. If the input\\nconfidence ? for input size n is set to ?n , then the 1-NN classifier hSn0 (?n? ) calculated by KSU is\\nstrongly Bayes consistent on (X , ?): P(limn?? err(hSn0 ) = R? ) = 1.\\nThe proof, provided in Sec. 5, closely follows the line of reasoning in [27], where the strong Bayesconsistency of an adaptive margin-regularized 1-NN algorithm was proved, but with several crucial\\ndifferences. In particular, the generalization bounds used by KSU are purely compression-based, as\\nopposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter\\nin practice and guarantee Bayes-consistency of KSU even for countably many labels. This however\\nrequires novel technical arguments, which are discussed in detail in Appendix B.1 in [26]. Moreover,\\nsince the compression-based bounds do not explicitly depend on ddim, they can be used even when\\nddim is infinite, as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency,\\nwe note that the proof technique given here does not carry to an earlier algorithm, suggested in [20,\\nTheorem 4], which also uses ?-nets. It is an open question whether the latter is Bayes-consistent.\\nNext, we study Bayes-consistency of KSU in infinite dimensions (i.e., with ddim = ?) ? in particular, in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed, a straightforward\\napplication of [9, Lemma A.1] yields the following result.\\nTheorem 3 (C?rou and Guyader [9]). There exists an infinite dimensional separable metric space\\n(X , ?) and a realizable distribution ?\\n? over X ? {0, 1} such that no kn -NN learner satisfying\\nkn /n ? 0 when n ? ? is Bayes-consistent under ?\\n?. In particular, this holds for any space and\\nrealizable distribution ?\\n? that satisfy the following condition: The set C of points labeled 1 by ?\\n?\\nsatisfies\\n?r (x))\\n?(C ? B\\n?(C) > 0\\nand\\n?x ? C, lim\\n= 0.\\n(3)\\n?\\nr?0\\n?(Br (x))\\nSince ?(C) > 0, Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling\\nspaces, the Besicovitch covering theorem precludes such a violation [15]. In contrast, as [35, 36]\\nshow, in infinite-dimensional spaces this violation can in fact occur. Moreover, this is not an isolated\\npathology, as this property is shared by Gaussian Hilbert spaces [45].\\nAt first sight, Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.\\nHowever, the following result shows that this is not the case: KSU is Bayes-consistent on a distribution\\nwith this property.\\nTheorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly\\nBayes-consistent, while any k-NN classifier necessarily is not.\\nThe proof relies on a classic construction of Preiss [35] which satisfies Eq. (3). We show that the\\nstructure of the construction, combined with the packing and covering properties of ?-nets, imply that\\nthe majority-vote classifier induced by any ?-net with a sufficienlty small ? approaches the Bayes\\nerror. To contrast with Theorem 4, we next show that on the same construction, not all majority-vote\\nVoronoi partitions succeed. Indeed, if the packing property of ?-nets is relaxed, partition sequences\\nobstructing Bayes-consistency exist.\\nTheorem 5. For the example constructed in Theorem 4, there exists a sequence of Voronoi partitions\\nwith a vanishing diameter such that the induced true majority-vote classifiers are not Bayes consistent.\\nThe above result also stands in contrast to [14, Theorem 21.2], showing that, unlike in finite dimensions, the partitions? vanishing diameter is insufficient to establish consistency when ddim = ?. We\\nconclude the main results by posing intriguing open problems.\\n6\\n\\n\n",
              "Open problem 1. Does there exist a metric probability space on which some k-NN algorithm is\\nconsistent while KSU is not? Does there exist any separable metric space on which KSU fails?\\nOpen problem 2. C?rou and Guyader [9] distill a certain Besicovitch condition which is necessary\\nand sufficient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the\\nBesicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufficient? What is a\\nnecessary condition?\\n\\n5\\n\\nBayes-consistency of KSU in finite dimensions\\n\\nIn this section we give a high-level proof of Theorem 2, showing that KSU is strongly Bayesconsistent in finite-dimensional metric spaces. A fully detailed proof is given in Appendix B in\\n[26].\\nRecall the optimal empirical error ?n? ? ?(?n? ) and the optimal compression size m?n ? m(?n? ) as\\ncomputed by KSU. As shown in Proposition 1, the sub-sample Sn0 (?n? ) is an (?n? , 2m?n )-compression\\nof Sn . Abbreviate the compression-based generalization bound used in KSU by\\nQn (?, m) := Q(n, ?, 2m, ?n ).\\nTo show Bayes-consistency, we start by a standard decomposition of the excess error over the optimal\\nBayes into two terms:\\n\u0001\\n\u0001\\nerr(hSn0 (?n? ) ) ? R? = err(hSn0 (?n? ) ) ? Qn (?n? , m?n ) + Qn (?n? , m?n ) ? R? =: TI (n) + TII (n),\\nand show that each term decays to zero with probability one. For the first term, Property 1 for Q,\\ntogether with the Borel-Cantelli lemma, readily imply lim supn?? TI (n) ? 0 with probability one.\\nThe main challenge is showing that lim supn?? TII (n) ? 0 with probability one. We do so in\\nseveral stages:\\n1. Loosely speaking, we first show (Lemma 10) that the Bayes error R? can be well approximated using 1-NN classifiers defined by the true (as opposed to empirical) majority-vote\\nlabels over fine partitions of X . In particular, this holds for any partition induced by a ?-net\\nof X with a sufficiently small ? > 0. This approximation guarantee relies on the fact that in\\nfinite-dimensional spaces, the class of continuous functions with compact support is dense\\nin L1 (?) (Lemma 9).\\n2. Fix ?? > 0 sufficiently small such that any true majority-vote classifier induced by a ?? -net\\nhas a true error close to R? , as guaranteed by stage 1. Since for bounded subsets of finitedimensional spaces the size of any ?-net is finite, the empirical error of any majority-vote\\n?-net almost surely converges to its true majority-vote error as the sample size n ? ?. Let\\nn(?\\n? ) sufficiently large such that Qn(?? ) (?(?\\n? ), m(?\\n? )) as computed by KSU for a sample of\\n0\\nsize n(?\\n? ) is a reliable estimate for the true error of hSn(?\\n(?\\n?).\\n?)\\n3. Let ?? and n(?\\n? ) be as in stage 2. Given a sample of size n = n(?\\n? ), recall that KSU\\nselects an optimal ? ? such that Qn (?(?), m(?)) is minimized over all ? > 0. For margins\\n? \n",
              " ?? , which are prone to over-fitting, Qn (?(?), m(?)) is not a reliable estimate for\\nhSn0 (?) since compression may not yet taken place for samples of size n. Nevertheless, these\\nmargins are discarded by KSU due to the penalty term in Q. On the other hand, for ?-nets\\nwith margin ? \n",
              " ?? , which are prone to under-fitting, the true error is well estimated by\\nQn (?(?), m(?)). It follows that KSU selects ?n? ? ?? and Qn (?n? , m?n ) ? R? , implying\\nlim supn?? TII (n) ? 0 with probability one.\\nAs one can see, the assumption that X is finite-dimensional plays a major role in the proof. A simple\\nargument shows that the family of continuous functions with compact support is no longer dense\\nin L1 in infinite-dimensional spaces. In addition, ?-nets of bounded subsets in infinite dimensional\\nspaces need no longer be finite.\\n\\n6\\n\\nOn Bayes-consistency of NN algorithms in infinite dimensions\\n\\nIn this section we study the Bayes-consistency properties of 1-NN algorithms on a classic infinitedimensional construction of Preiss [35], which we describe below in detail. This construction was\\n7\\n\\n\n",
              "z1:k?2\\n?k?1\\nz1:k?1\\n?k\\n\\n?k\\nz1:k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\nz\\n\\nC = Z?\\n\\n?? (z) for some z ? C.\\nFigure 1: Preiss?s construction. Encircled is the closed ball B\\nk?1\\nfirst introduced as a concrete example showing that in infinite-dimensional spaces the Besicovich\\ncovering theorem [15] can be strongly violated, as manifested in Eq. (3).\\nExample 1 (Preiss?s construction). The construction (see Figure 1) defines an infinite-dimensional\\nmetric space (X , ?) and a realizable measure ?\\n? over X ? Y with the binary label set Y = {0, 1}.\\nIt relies on two sequences: a sequence of natural numbers {Nk }k?N and a sequence of positive\\nnumbers {ak }k?N . The two sequences should satisfy the following:\\nP?\\nlimk?? ak N1 . . . Nk+1 = ?; and limk?? Nk = ?. (4)\\nk=1 ak N1 . . . Nk = 1;\\nQ\\nThese properties are satisfied, for instance, by setting Nk := k! and ak := 2?k / i?[k] Ni . Let Z0\\nbe the set of all finite sequences (z1 , . . . , zk )k?N of natural numbers such that zi ? Ni , and let Z?\\nbe the set of all infinite sequences (z1 , z2 , . . . ) of natural numbers such that zi ? Ni .\\nDefine the example space X := Z0 ? Z? and denote ?k := 2?k , where ?? := 0. The metric ? over\\nX is defined as follows: for x, y ? X , denote by x ? y their longest common prefix. Then,\\n?(x, y) = (?|x?y| ? ?|x| ) + (?|x?y| ? ?|y| ).\\nIt can be shown (see [35]) that ?(x, y) is a metric; in fact, it embeds isometrically into the square\\nnorm metric of a Hilbert space.\\nTo define ?, the marginal measure over X , let ?? be the uniform product distribution measure\\nover Z? , that is: for all i ? N, each zi in the sequence z = (z1 , z2 , . . . ) ? Z? is independently\\ndrawn from a uniform distribution over [Ni ]. Let ?0 be an atomic measure on Z0 such that for all\\nz ? Z0 , ?0 (z) = a|z| . Clearly, the first condition in Eq. (4) implies ?0 (Z0 ) = 1. Define the marginal\\nprobability measure ? over X by\\n?A ? Z0 ? Z? ,\\n\\n?(A) := ??? (A) + (1 ? ?)?0 (A).\\n\\nIn words, an infinite sequence is drawn with probability ? (and all such sequences are equally likely),\\nor else a finite sequence is drawn (and all finite sequences of the same length are equally likely).\\nDefine the realizable distribution ?\\n? over X ? Y by setting the marginal over X to ?, and by setting\\nthe label of z ? Z? to be 1 with probability 1 and the label of z ? Z0 to be 0 with probability 1.\\nAs shown in [35], this construction satisfies Eq. (3) with C = Z? and ?(C) = ? > 0. It follows\\nfrom Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast, the following theorem\\nshows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows\\nfrom the this result.\\nTheorem 6. Assume (X , ?), Y and ?\\n? as in Example 1. KSU is weakly Bayes-consistent on ?\\n?.\\nThe proof, provided in Appendix C in [26], first characterizes the Voronoi cells for which the true\\nmajority-vote yields a significant error for the cell (Lemma 15). In finite-dimensional spaces, the total\\nmeasure of all such ?bad? cells can be made arbitrarily close to zero by taking ? to be sufficiently\\nsmall, as shown in Lemma 10 of Theorem 2. However, it is not immediately clear whether this can\\nbe achieved for the infinite dimensional construction above.\\nIndeed, we expect such bad cells, due to the unintuitive property that for any x ? C, we have\\n?? (x) ? C)/?(B\\n?? (x)) ? 0 when ? ? 0, and yet ?(C) > 0. Thus, if for example a significant\\n?(B\\n8\\n\\n\n",
              "?? (x) with\\nportion of the set C (whose label is 1) is covered by Voronoi cells of the form V = B\\nx ? C, then for all sufficiently small ?, each one of these cells will have a true majority-vote 0. Thus\\na significant portion of C would be misclassified. However, we show that by the structure of the\\nconstruction, combined with the packing and covering properties of ?-nets, we have that in any ?-net,\\nthe total measure of all these ?bad? cells goes to 0 when ? ? 0, thus yielding a consistent classifier.\\nLastly, the following theorem shows that on the same construction above, when the Voronoi partitions\\nare allowed to violate the packing property of ?-nets, Bayes-consistency does not necessarily hold.\\nTheorem 5 immediately follows from the following result.\\nTheorem 7. Assume (X , ?), Y and ?\\n? as in Example 1. There exists a sequence of Voronoi partitions\\n(Pk )k?N of X with maxV ?Pk diam(V ) ? ?k such that the sequence of true majority-vote classifiers\\n(hPk )k?N induced by these partitions is not Bayes consistent: lim inf k?? err(hPk ) = ? > 0.\\nThe proof, provided in Appendix D, constructs a sequence of Voronoi partitions, where each partition\\nPk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case, C is\\nincorrectly classified by hPk , yielding a significant error. Thus, in infinite-dimensional metric spaces,\\nthe shape of the Voronoi cells plays a fundamental role in the consistency of the partition.\\nAcknowledgments. We thank Fr?d?ric C?rou for the numerous fruitful discussions and helpful\\nfeedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science\\nFoundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel\\nScience Foundation (grant No. 555/15).\\n\\nReferences\\n[1] Christophe Abraham, G?rard Biau, and Beno?t Cadre. On the kernel rule for function classification. Ann. Inst. Statist. Math., 58(3):619?633, 2006.\\n[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics & Probability\\nLetters, 82(6):1102?1110, 2012.\\n[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic\\nCommunications in Probability, 18(3):1?7, 2013.\\n[4] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In\\nICML ?06: Proceedings of the 23rd international conference on Machine learning, pages\\n97?104, New York, NY, USA, 2006. ACM.\\n[5] G?rard Biau, Florentina Bunea, and Marten H. Wegkamp. Functional classification in Hilbert\\nspaces. IEEE Trans. Inform. Theory, 51(6):2163?2172, 2005.\\n[6] G?rard Biau, Fr?d?ric C?rou, and Arnaud Guyader. Rates of convergence of the functional\\nk-nearest neighbor estimate. IEEE Trans. Inform. Theory, 56(4):2034?2040, 2010.\\n[7] V. I. Bogachev. Measure theory. Vol. I, II. Springer-Verlag, Berlin, 2007.\\n[8] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image\\nclassification. In CVPR, 2008.\\n[9] Fr?d?ric C?rou and Arnaud Guyader. Nearest neighbor classification in infinite dimension.\\nESAIM: Probability and Statistics, 10:340?355, 2006.\\n[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In NIPS, 2014.\\n[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions\\non Information Theory, 13:21?27, 1967.\\n[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE\\nTrans. Pattern Anal. Mach. Intell., 3(1):75?78, 1981.\\n[13] Luc Devroye and L?szl? Gy?rfi. Nonparametric density estimation: the L1 view. Wiley Series\\nin Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &\\nSons, Inc., New York, 1985.\\n9\\n\\n\n",
              "[14] Luc Devroye, L?szl? Gy?rfi, and G?bor Lugosi. A probabilistic theory of pattern recognition,\\nvolume 31. Springer Science & Business Media, 2013.\\n[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wissenschaften, Band 153. Springer-Verlag New York Inc., New York, 1969.\\n[16] Evelyn Fix and Jr. Hodges, J. L. Discriminatory analysis. nonparametric discrimination:\\nConsistency properties. International Statistical Review / Revue Internationale de Statistique,\\n57(3):pp. 238?247, 1989.\\n[17] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine learning, 21(3):269?304, 1995.\\n[18] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient classification for metric\\ndata (extended abstract COLT 2010). IEEE Transactions on Information Theory, 60(9):5750?\\n5759, 2014.\\n[19] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality\\nreduction. Theoretical Computer Science, 620:105?118, 2016.\\n[20] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression\\nfor nearest neighbors. In Neural Information Processing Systems (NIPS), 2014.\\n[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Nearly optimal classification for\\nsemimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research, 2017.\\n[22] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on\\nthe prediction error of learning algorithms for classification. Machine Learning, 59(1):55?76,\\n2005.\\n[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classification. Ann.\\nStatist., 33(1):284?306, 02 2005.\\n[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its\\nApplications. Springer-Verlag, 2002.\\n[25] Aryeh Kontorovich, Sivan Sabato, and Ruth Urner. Active nearest-neighbor learning in metric\\nspaces. In Advances in Neural Information Processing Systems, pages 856?864, 2016.\\n[26] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression:\\nEfficiency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017.\\n[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classifier. In Artificial Intelligence\\nand Statistics (AISTATS 2015), 2014.\\n[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In International Conference on Machine Learning (ICML 2014), 2014.\\n[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity\\nsearch. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 791?801, January\\n2004.\\n[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation\\nunder arbitrary sampling. IEEE Trans. Inform. Theory, 41(4):1028?1039, 1995.\\n[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpublished, 1986.\\n[32] James R. Munkres. Topology: a first course. Prentice-Hall, Inc., Englewood Cliffs, N.J., 1975.\\n[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration\\nof measure. Inform. Process. Lett., 73(1-2):47?51, 2000.\\n[34] Vladimir Pestov. Is the k-NN classifier in high dimensions affected by the curse of dimensionality? Comput. Math. Appl., 65(10):1427?1437, 2013.\\n10\\n\\n\n",
              "[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis, pages\\n58?60, 1979.\\n[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin.,\\n22(1):181?193, 1981.\\n[37] Demetri Psaltis, Robert R. Snapp, and Santosh S. Venkatesh. On the finite sample performance\\nof the nearest neighbor classifier. IEEE Transactions on Information Theory, 40(3):820?837,\\n1994.\\n[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third\\nedition, 1976. International Series in Pure and Applied Mathematics.\\n[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1987.\\n[40] Richard J. Samworth. Optimal weighted nearest neighbour classifiers. Ann. Statist., 40(5):2733?\\n2763, 10 2012.\\n[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\\nAlgorithms. Cambridge University Press, 2014.\\n[42] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural\\nrisk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory,\\n44(5):1926?1940, 1998.\\n[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor\\nrisk. Ann. Statist., 26(3):850?878, 1998.\\n[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595?620,\\n1977.\\n[45] Jaroslav Ti?er. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc., 355(8):3277?\\n3289, 2003.\\n[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest\\nneighbor classification. Journal of Machine Learning Research, 10:207?244, 2009.\\n[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of\\nregression functions. J. Multivariate Anal., 21(1):168?178, 1987.\\n\\n11\\n\\n\n",
              "\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings Independent Component Analysis for\\nidentification of artifacts in\\nMagnetoencephalographic recordings\\n\\nRicardo Vigario 1 ; Veikko J ousmiiki2 ,\\nMatti Hiimiiliiinen2, Riitta Hari2, and Erkki Oja 1\\n1 Lab.\\n\\nof Computer &amp; Info. Science\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{Ricardo.Vigario, Erkki.Oja}@hut.fi\\n2 Brain\\n\\nResearch Unit, Low Temperature Lab.\\nHelsinki University of Technology\\nP.O. Box 2200, FIN-02015 HUT, Finland\\n{veikko, msh, hari}@neuro.hut.fi\\n\\nAbstract\\nWe have studied the application of an independent component analysis\\n(ICA) approach to the identification and possible removal of artifacts\\nfrom a magnetoencephalographic (MEG) recording. This statistical technique separates components according to the kurtosis of their amplitude\\ndistributions over time, thus distinguishing between strictly periodical\\nsignals, and regularly and irregularly occurring signals. Many artifacts\\nbelong to the last category. In order to assess the effectiveness of the\\nmethod, controlled artifacts were produced, which included saccadic eye\\nmovements and blinks, increased muscular tension due to biting and the\\npresence of a digital watch inside the magnetically shielded room. The\\nresults demonstrate the capability of the method to identify and clearly\\nisolate the produced artifacts.\\n\\n1 Introduction\\nWhen using a magnetoencephalographic (MEG) record, as a research or clinical tool, the\\ninvestigator may face a problem of extracting the essential features of the neuromagnetic\\n? Corresponding author\\n\\n\fR. Vigario,\\n\\n230\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\nsignals in the presence of artifacts. The amplitude of the disturbance may be higher than\\nthat of the brain signals, and the artifacts may resemble pathological signals in shape. For\\nexample, the heart's electrical activity, captured by the lowest sensors of a whole-scalp\\nmagnetometer array, may resemble epileptic spikes and slow waves (Jousmili and Hari\\n1996).\\nThe identification and eventual removal of artifacts is a common problem in electroencephalography (EEG), but has been very infrequently discussed in context to MEG (Hari\\n1993; Berg and Scherg 1994).\\nThe simplest and eventually most commonly used artifact correction method is rejection,\\nbased on discarding portions of MEG that coincide with those artifacts. Other methods\\ntend to restrict the subject from producing the artifacts (e.g. by asking the subject to fix the\\neyes on a target to avoid eye-related artifacts, or to relax to avoid muscular artifacts). The\\neffectiveness of those methods can be questionable in studies of neurological patients, or\\nother non-co-operative subjects. In eye artifact canceling, other methods are available and\\nhave recently been reviewed by Vigario (I 997b) whose method is close to the one presented\\nhere, and in Jung et aI. (1998).\\nThis paper introduces a new method to separate brain activity from artifacts, based on the\\nassumption that the brain activity and the artifacts are anatomically and physiologically\\nseparate processes, and that their independence is reflected in the statistical relation between the magnetic signals generated by those processes.\\nThe remaining of the paper will include an introduction to the independent component\\nanalysis, with a presentation of the algorithm employed and some justification of this approach. Experimental data are used to illustrate the feasibility of the technique, followed\\nby a discussion on the results.\\n\\n2\\n\\nIndependent Component Analysis\\n\\nIndependent component analysis is a useful extension of the principal component analysis\\n(PC A). It has been developed some years ago in context with blind source separation applications (Jutten and Herault 1991; Comon 1994). In PCA. the eigenvectors of the signal\\ncovariance matrix C = E{xx T } give the directions oflargest variance on the input data\\nx. The principal components found by projecting x onto those perpendicular basis vectors\\nare uncorrelated, and their directions orthogonal.\\nHowever, standard PCA is not suited for dealing with non-Gaussian data. Several authors, from the signal processing to the artificial neural network communities, have shown\\nthat information obtained from a second-order method such as PCA is not enough and\\nhigher-order statistics are needed when dealing with the more demanding restriction of\\nindependence (Jutten and Herault 1991; Comon 1994). A good tutorial on neural ICA implementations is available by Karhunen et al. (1997). The particular algorithm used in this\\nstudy was presented and derived by Hyvarinen and Oja (1997a. 1997b).\\n\\n2.1\\n\\nThe model\\n\\nIn blind source separation, the original independent sources are assumed to be unknown,\\nand we only have access to their weighted sum. In this model, the signals recorded in an\\nMEG study are noted as xk(i) (i ranging from 1 to L, the number of sensors used, and\\nk denoting discrete time); see Fig. 1. Each xk(i) is expressed as the weighted sum of M\\n\\n\fICAfor Identification of Artifacts in MEG Recordings\\n\\n231\\n\\nindependent signals Sk(j), following the vector expression:\\nM\\n\\nXk = La(j)sdj) = ASk,\\n\\n(1)\\n\\nj=l\\n\\nwhere Xk = [xk(1), ... , xk(L)]T is an L-dimensional data vector, made up of the L mixtures at discrete time k. The sk(1), ... , sk(M) are the M zero mean independent source\\nsignals, and A = [a(1), . .. , a(M)] is a mixing matrix independent of time whose elements\\nail are th.e unknown coefficients of the mixtures. In order to perform ICA, it is necessary\\nto have at least as many mixtures as there are independent sources (L ~ M). When this\\nrelation is not fully guaranteed, and the dimensionality of the problem is high enough,\\nwe should expect the first independent components to present clearly the most strongly\\nindependent signals, while the last components still consist of mixtures of the remaining\\nsignals. In our study, we did expect that the artifacts, being clearly independent from the\\nbrain activity, should come out in the first independent components. The remaining of the\\nbrain activity (e.g. a and J-L rhythms) may need some further processing.\\nThe mixing matrix A is a function of the geometry of the sources and the electrical conductivities of the brain, cerebrospinal fluid, skull and scalp. Although this matrix is unknown.\\nwe assume it to be constant, or slowly changing (to preserve some local constancy).\\nThe problem is now to estimate the independent signals Sk (j) from their mixtures, or the\\nequivalent problem of finding the separating matrix B that satisfies (see Eq. 1)\\n(2)\\nIn our algorithm, the solution uses the statistical definition of fourth-order cumulant or\\nkurtosis that, for the ith source signal, is defined as\\n\\nkurt(s(i)) = E{s(i)4} - 3[E{s(i)2}]2,\\nwhere E( s) denotes the mathematical expectation of s.\\n\\n2.2 The algorithm\\nThe initial step in source separation, using the method described in this article, is whitening, or sphering. This projection of the data is used to achieve the uncorrelation between\\nthe solutions found, which is a prerequisite of statistical independence (Hyvarinen and Oja\\n1997a). The whitening can as well be seen to ease the separation of the independent signals (Karhunen et al. 1997). It may be accomplished by PCA projection: v = V x, with\\nE{ vv T } = I. The whitening matrix V is given by\\n-=T ,\\nV -- A- 1 / 2 .....\\n\\nwhere A = diag[-\\(1), ... , -\\(M)] is a diagonal matrix with the eigenvalues of the data\\ncovariance matrix E{xxT}, and 8 a matrix with the corresponding eigenvectors as its\\ncolumns.\\nConsider a linear combination y = w T v of a sphered data vector v, with Ilwll = 1. Then\\nE{y2} = .1 andkurt(y) = E{y4}-3, whose gradientwithrespecttow is 4E{v(wTv)3} .\\nBased on this, Hyvarinen and Oja (1997a) introduced a simple and efficient fixed-point\\nalgorithm for computing ICA, calculated over sphered zero-mean vectors v, that is able to\\nfind one of the rows of the separating matrix B (noted w) and so identify one independent\\nsource at a time - the corresponding independent source can then be found using Eq. 2.\\nThis algorithm, a gradient descent over the kurtosis, is defined for a particular k as\\n1. Take a random initial vector Wo of unit norm. Let l = 1.\\n\\n\f232\\n\\nR. Vigario,\\n\\nv. Jousmiiki, M. Hiimiiliiinen, R. Hari and E. Oja\\n\\n2. Let Wi = E{V(W[.1 v)3} - 3Wl-I. The expectation can be estimated using a\\nlarge sample OfVk vectors (say, 1,000 vectors).\\n3. Divide Wi by its norm (e.g. the Euclidean norm\\n4.\\n\\nIlwll = JLi wI J.\\n\\nlflwT wi-II is not close enough to 1, let I = 1+1 andgo back to step 2.\\n\\nOtherwise,\\n\\noutput the vector Wi.\\n\\nIn order to estimate more than one solution, and up to a maximum of lvI, the algorithm\\nmay be run as many times as required. It is, nevertheless, necessary to remove the infonnation contained in the solutions already found, to estimate each time a different independent\\ncomponent. This can be achieved, after the fourth step of the algorithm, by simply subtracting the estimated solution s = w T v from the unsphered data Xk . As the solution is\\ndefined up to a multiplying constant, the subtracted vector must be multiplied by a vector\\ncontaining the regression coefficients over each vector component of Xk.\\n\\n3\\n\\nMethods\\n\\nThe MEG signals were recorded in a magnetically shielded room with a 122-channel\\nwhole-scalp Neuromag-122 neuromagnetometer. This device collects data at 61 locations\\nover the scalp, using orthogonal double-loop pick-up coils that couple strongly to a local\\nsource just underneath, thus making the measurement \"near-sighted\" (HamaHi.inen et al.\\n1993).\\nOne of the authors served as the subject and was seated under the magnetometer. He kept\\nhis head immobile during the measurement. He was asked to blink and make horizontal\\nsaccades, in order to produce typical ocular artifacts. Moreover, to produce myographic\\nartifacts, the subject was asked to bite his teeth for as long as 20 seconds. Yet another\\nartifact was created by placing a digital watch one meter away from the helmet into the\\nshieded room. Finally, to produce breathing artifacts, a piece of metal was placed next\\nto the navel. Vertical and horizontal electro-oculograms (VEOG and HEOG) and electrocardiogram (ECG) between both wrists were recorded simultaneously with the MEG, in\\norder to guide and ease the identification of the independent components. The bandpassfiltered MEG (0.03-90 Hz), VEOG, HEOG, and ECG (0.1-100 Hz) signals were digitized\\nat 297 Hz, and further digitally low-pass filtered, with a cutoff frequency of 45 Hz and\\ndownsampled by a factor of 2. The total length of the recording was 2 minutes. A second\\nset of recordings was perfonned, to assess the reproducibility of the results.\\nFigure 1 presents a subset of 12 spontaneous MEG signals from the frontal, temporal and\\noccipital areas. Due to the dimension of the data (122 magnetic signals were recorded), it\\nis impractical to plot all MEG signals (the complete set is available on the internet - see\\nreference list for the adress (Vigario 1997a?. Also both EOG channels and the electrocardiogram are presented.\\n\\n4\\n\\nResults\\n\\nFigure 2 shows sections of9 independent components (IC's) found from the recorded data,\\ncorresponding to a I min period, starting 1 min after the beginning of the measurements.\\nThe first two IC's, with a broad band spectrum, are clearly due to the musclular activity\\noriginated from the biting. Their separation into two components seems to correspond, on\\nthe basis of the field patterns, to two different sets of muscles that were activated during\\nthe process. IC3 and IC5 are, respectively showing the horizontal eye movements and the\\neye blinks, respectively. IC4 represents cardiac artifact that is very clearly extracted. In\\nagreement with Jousmaki and Hari (1996), the magnetic field pattern of IC4 shows some\\npredominance on the left.\\n\\n\fICA/or Identification 0/ Artifacts in MEG Recordings\\n\\n233\\nMEG [ 1000 fTlcm\\n\\nI--\\n\\n---l\\n\\nsaccades\\n\\nI--\\n\\n---l\\n\\nblinking\\n\\nEOG [\\n\\n500 IlV\\n\\nECG [\\n\\n500 IlV\\n\\nI--\\n\\nbiting\\n\\n---l MEG\\n\\n~=::::::::::::::=::\\n~?'104~\\n\\nrJ. .........\\n\\nM\\n\\n,J.\\.......1iIIiM~..\\n\\n::\\nt...\\n\\n:;::::::;:::~=\\n\\n~::::::::;::=\\n~ ?? \",~Jrt\\n\\n..,.\\n\\nt\\n\\n....\\n\\n~,.~ . ? .J..\\n\\n.\\n\\n.../\\\"\"$\"\"~I\\n\\n2 t\\n\\n:;\\n\\n:;\\n4\\n\\n~\\n\\n5 t\\n\\n., ... ...., ,'fIJ'\\,\\n..........-.\\n\\n,..,d\\n\\n,LIlt ... .,\\n\\nI?\\n\\n.,............. ................. \"\\n\\n....,..,.\"........ .\\n\\n.... Dei ..... \"\\n\\n.'''IIb'''*. rt\\n\\n-1I\\JY. ? ---\\n\\nI p\", . . . . , . . . . . . . . . . . . at ...'....\\n\\nI; rp ..\\n\\n,P....\\n\\n,\\n\\n.,...............' tMn':M.U\\n\\n... ,\\n\\n, ..... '\\n\\nU\\..,.--II..------'-__\\n\\nooII..Jl,,-\\n\\n\".'tIItS\\n\\n5 ~\\n\\n6 t\\n\\nVEOG\\n\\nIt ... 11.1. HEOG\\n\\n~UijuJJJ.LU Wl Uij.lJU.LllU.UUUllUUij,UU~ijJJJ\\n\\nECG\\n\\n10 s\\n\\nFigure 1: Samples of MEG signals, showing artifacts produced by blinking, saccades,\\nbiting and cardiac cycle. For each of the 6 positions shown, the two orthogonal directions\\nof the sensors are plotted.\\nThe breathing artifact was visible in several independent components, e.g. IC6 and IC7. It\\nis possible that, in each breathing the relative position and orientation of the metallic piece\\nwith respect to the magnetometer has changed. Therefore, the breathing artifact would be\\nassociated with more than one column of the mixing matrix A, or to a time varying mixing\\nvector.\\nTo make the analysis less sensible to the breathing artifact, and to find the remaining artifacts, the data were high-pass filtered, with cutoff frequency at 1 Hz. Next, the independent\\ncomponent IC8 was found. It shows clearly the artifact originated at the digital watch,\\nlocated to the right side of the magnetometer.\\nThe last independent component shown, relating to the first minute of the measurement,\\nshows an independent component that is related to a sensor presenting higher RMS (root\\nmean squared) noise than the others.\\n\\n5\\n\\nDiscussion\\n\\nThe present paper introduces a new approach to artifact identification from MEG recordings, based on the statistical technique of Independent Component Analysis. Using this\\nmethod, we were able to isolate both eye movement and eye blinking artifacts, as well as\\n\\n\fR. Vigario,\\n\\n234\\n\\nv. Jousmiiki, M HtJmlJliiinen, R. Hari and E. Oja\\n\\ncardiac, myographic, and respiratory artifacts.\\nThe basic asswnption made upon the data used in the study is that of independence between brain and artifact waveforms. In most cases this independence can be verified by the\\nknown differences in physiological origins of those signals. Nevertheless, in some eventrelated potential (ERP) studies (e.g. when using infrequent or painful stimuli), both the\\ncerebral and ocular signals can be similarly time-locked to the stimulus. This local time\\ndependence could in principle affect these particular ICA studies. However, as the independence between two signals is a measure of the similarity between their joint amplitude\\ndistribution and the product of each signal's distribution (calculated throughout the entire\\nsignal, and not only close to the stimulus applied), it can be expected that the very local\\nrelation between those two signals, during stimulation, will not affect their global statistical\\nrelation.\\n\\n6\\n\\nAcknowledgment\\n\\nSupported by a grant from Junta Nacional de Investiga~ao Cientifica e Tecnologica, under\\nits 'Programa PRAXIS XXI' (R.Y.) and the Academy of Finland (R.H.).\\n\\nReferences\\nBerg, P. and M. Scherg (1994). A multiple source approach to the correction of eye\\nartifacts. Electroenceph. clin. Neurophysiol. 90, 229-241.\\nComon, P. (1994). Independent component analysis - a new concept? Signal Processing 36,287-314.\\nHamalainen, M., R. Hari, R. Ilmoniemi, 1. Knuutila, and O. Y. Lounasmaa (1993, April).\\nMagnetoencephalography-theory, instrumentation, and applications to noninvasive\\nstudies of the working human brain. Reviews o/Modern Physics 65(2), 413-497.\\nHari, R. (1993). Magnetoencephalography as a tool of clinical neurophysiology. In\\nE. Niedermeyer and F. L. da Silva (Eds.), Electroencephalography. Basic principles, clinical applications, and relatedjields, pp. 1035-1061 . Baltimore: Williams\\n&amp; Wilkins.\\nHyvarinen, A. and E. Oja (l997a). A fast fixed-point algorithm for independent component analysis. Neural Computation (9), 1483-1492.\\nHyvarinen, A. and E. Oja (1997b). One-unit learning rules for independent component\\nanalysis. In Neural Information Processing Systems 9 (Proc. NIPS '96). MIT Press.\\nJousmiiki, Y. and R. Hari (1996). Cardiac artifacts in magnetoencephalogram. Journal\\no/Clinical Neurophysiology 13(2), 172-176.\\nJung, T.-P., C. Hwnphries, T.-W. Lee, S. Makeig, M. J. McKeown, Y. lragui, and\\nT. Sejnowski (1998). Extended ica removes artifacts from electroencephalographic\\nrecordings. In Neural Information Processing Systems 10 (Proc. NIPS '97). MIT\\nPress.\\nJutten, C. and 1. Herault (1991). Blind separation of sources, part i: an adaptive algorithm based on neuromimetic architecture. Signal Processing 24, 1-10.\\nKarhunen, J., E. Oja, L. Wang, R. Vigmo, and J. Joutsensalo (1997). A class of neural\\nnetworks for independent component analysis. IEEE Trans. Neural Networks 8(3),\\n1-19.\\nVigmo, R. (1997a). WWW adress for the MEG data:\\nhttp://nuc1eus.hut.firrvigarioINIPS97_data.html.\\nVigmo, R. (1997b). Extraction of ocular artifacts from eeg using independent component analysis. To appear in Electroenceph. c/in. Neurophysiol.\\n\\n\fICAfor Identification ofArtifacts in MEG Recordings\\n\\n235\\n\\n~~~\\n\\nIC1\\n\\n------,--y~-------------------------.-.------~~.. ,.. ~\\nU\\n\\n...\\n\\nIC2\\n\\nIC3\\n.\",\\n\\n''' ... '' .. '\\n\\n&lt;&gt; .\\n).\\~\\n.\\ C:&gt; ?\\n\\\\n\\n~~~}a\\n\\n~~-\"\\n\\n____I4-_. _\\n. . . ._.---_._. . . .-.__\\n. \"\"\"\"\"\"?t;_-\"'' '....\\n~\\n\\n. . . . .-......,.....\\n\\n~_1\\n\\nIC4\\n\\nIC5\\n\\nIC6\\n~\\n...W\"\\n....\\n\"1011\\n...~\"_f....\\n..\".,.\"\"'_\\n\\n/tJ'IfII/'h\\n\\nI' ......\\n\\nd1b ..\\n\\n~*W,.'tJ ......\\n\\nr' .. ns...\\n\\nIC7\\n\\nICB\\n\\nICg\\n~._-~.,.\\n\\n. . . . .t . .\\n\\nWt:n:ePWt.~..,.~I'NJ'~~\\nI\\n\\n10 s\\n\\nI\\n\\nFigure 2: Nine independent components found from the MEG data. For each component the\\nleft, back and right views of the field patterns generated by these components are shown full line stands for magnetic flux coming out from the head, and dotted line the flux inwards.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Near-Maximum Entropy Models for Binary Neural Representations of Natural Images Near-Maximum Entropy Models for Binary\\nNeural Representations of Natural Images\\n\\nMatthias Bethge and Philipp Berens\\nMax Planck Institute for Biological Cybernetics\\nSpemannstrasse 41, 72076, T?ubingen, Germany\\nmbethge,berens@tuebingen.mpg.de\\n\\nAbstract\\nMaximum entropy analysis of binary variables provides an elegant way for studying the role of pairwise correlations in neural populations. Unfortunately, these\\napproaches suffer from their poor scalability to high dimensions. In sensory coding, however, high-dimensional data is ubiquitous. Here, we introduce a new\\napproach using a near-maximum entropy model, that makes this type of analysis feasible for very high-dimensional data?the model parameters can be derived\\nin closed form and sampling is easy. Therefore, our NearMaxEnt approach can\\nserve as a tool for testing predictions from a pairwise maximum entropy model not\\nonly for low-dimensional marginals, but also for high dimensional measurements\\nof more than thousand units. We demonstrate its usefulness by studying natural\\nimages with dichotomized pixel intensities. Our results indicate that the statistics\\nof such higher-dimensional measurements exhibit additional structure that are not\\npredicted by pairwise correlations, despite the fact that pairwise correlations explain the lower-dimensional marginal statistics surprisingly well up to the limit of\\ndimensionality where estimation of the full joint distribution is feasible.\\n\\n1\\n\\nIntroduction\\n\\nA core issue in sensory coding is to seek out and model statistical regularities in high-dimensional\\ndata. In particular, motivated by developments in information theory, it has been hypothesized\\nthat modeling these regularities by means of redundancy reduction constitutes an important goal of\\nearly visual processing [2]. Recent studies conjectured that the binary spike responses of retinal\\nganglion cells may be characterized completely in terms of second-order correlations when using\\na maximum entropy approach [13, 12]. In light of what we know about the statistics of the visual\\ninput, however, this would be very surprising: Natural images are known to exhibit complex higherorder correlations which are extremely difficult to model yet being perceptually relevant. Thus, if\\nwe assume that retinal ganglion cells do not discard the information underlying these higher-order\\ncorrelations altogether, it would be a very difficult signal processing task to remove all of those\\nalready within the retinal network.\\nOftentimes, neurons involved in early visual processing are modeled as rather simple computational\\nunits akin to generalized linear models, where a linear filter is followed by a point-wise nonlinearity.\\nFor such simple neuron models, the possibility of removing higher-order correlations present in the\\ninput is very limited [3].\\nHere, we study the role of second-order correlations in the multivariate binary output statistics of\\nsuch linear-nonlinear model neurons with a threshold nonlinearity responding to natural images.\\nThat is, each unit can be described by an affine transformation zk = wkT x + ? followed by a\\npoint-wise signum function sk = sgn(zk ). Our interest in this model is twofold: (A) It can be\\nregarded a parsimonious model for the analysis of population codes of natural images for which the\\n1\\n\\n\fA\\n\\n?3\\n\\nB\\n\\n3\\n\\n0\\n\\n2\\n\\nC\\n\\n6\\n\\nJS?Divergence (bits)\\n\\n?H (%)\\n\\nx 10\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n1\\n\\nlog? H (%)\\n\\n?H (%)\\n\\n4\\n3\\n2\\n1\\n\\n2\\n\\n4\\n\\n6\\n\\n8\\n\\nDimension\\n\\n10\\n\\nD\\n10\\n\\n0.5\\n\\n0\\n\\n?5\\n\\n5\\n\\n0\\n\\n10\\n\\nx 10\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n0\\n\\n10\\n\\n?1\\n\\n10\\n\\n?2\\n\\n20\\n\\n10\\n\\nDimension\\n\\n12\\n\\n14\\n\\n16\\n\\n18\\n\\n20\\n\\nlog 2 (Number of Samples)\\n\\nFigure 1: Similarity between the Ising and the DG model. A+C: Entropy difference ?H between the Ising\\nmodel and the Dichotomized Gaussian distribution as a function of dimensionality. A: Up to 10 dimensions\\nwe can compute HDG directly by evaluating Eq. 6. Gray dots correspond to different sets of parameters. For\\nm ? 4, the relatively large scatter and the existence of negative values is due to the limited numerical precision\\nof the Monte-Carlo integration. Errorbars show standard error of the mean. B. JS-divergence DJS between PI\\nand PDG . C. ?H as above, for higher dimensions. Up to 20 dimensions ?H remains very small. The increase\\nfor m ? 20 is most likely due to undersampling of the distributions. D. ?H as function of sample size used\\nto estimate HDG , at seven (black) and ten (grey) dimensions (note log scale on both axes). ?H decreases with\\na power law with increasing sample sizes.\\n\\ncomputational power and the bandwidth of each unit is limited. (B) The same model can also be\\nused more generally to fit multivariate binary data with given pairwise correlations, if x is drawn\\nfrom a Gaussian distribution. In particular, we will show that the resulting distribution closely\\nresembles the binary maximum entropy models known as Ising models or Boltzmann machines\\nwhich have recently become popular for the analysis of spike train recordings from retinal ganglion\\ncell responses [13, 12].\\nMotivated by the analysis in [12, 13] and the discussion in [10] we are interested at a more general level in the following questions: are pairwise interactions enough for understanding the statistical regularities in high-dimensional natural data (given that they provide a good fit in the lowdimensional case)? If we suppose that pairwise interactions are enough, what can we say about the\\namount of redundancies in high-dimensional data? In comparison with neural spike data, natural\\nimages provide two advantages for studying these questions: 1) It is much easier to obtain large\\namounts of data with millions of samples which are less prone to nonstationarities. 2) Often differences in the higher-order statistics such as between pink noise and natural images can be recognized\\nby eye.\\n\\n2\\n\\nSecond order models for binary variables\\n\\nIn order to study whether pairwise interactions are enough to determine the statistical regularities\\nin high-dimensional data, it is necessary to be able to compute the maximum entropy distribution\\nfor large number of dimensions N . Given a set of measured statistics, maximum entropy models\\nyield a full probability distribution that is consistent with these constraints but does not impose any\\n2\\n\\n\f0.05\\n0\\n0\\n?4\\n\\n1\\n?\\n\\n2\\n\\n1\\n\\n2\\n\\nx 10\\n\\n2\\n1\\n0\\n\\n0\\n\\n?\\n\\nFigure 2: Examples of covariance matrices (A+B.) and their learned approximations (C+D) at m = 10 for\\nclarity. ? is the parameter controlling the steepness of correlation decrease. E+F. Eigenvalue spectra of both\\nmatrices. G. Entropy difference ?H and H. JS-divergence between the distribution of samples obtained from\\nthe two models at m = 7.\\n\\nadditional structure on the distribution [7]. For binary data with given mean activations ?i = hsi i\\nand correlations between neurons ?ij = hsi sj i ? hsi ihsj i, one obtains a quadratic exponential\\nprobability mass function known as the Ising model in physics or as the Boltzmann machine in\\nmachine learning.\\nCurrently all methods used to determine the parameters of such binary maximum entropy models\\nsuffer from the same drawback: since the parameters do not correspond directly to any of the measured statistics, they have to be inferred (or ?learned?) from data. In high dimensions though, this\\nposes a difficult computational problem. Therefore the characterization of complete neural circuits\\nwith possibly hundreds of neurons is still out of reach, even though analysis was recently extended\\nto up to forty neurons [14].\\nTo make the maximum entropy approach feasible in high dimensions, we propose a new strategy:\\nSampling from a ?near-maximum? entropy model that does not require any complicated learning\\nof parameters. In order to justify this approach, we verify empirically that the entropy of the full\\nprobability distributions obtained with the near-maximum entropy model are indistinguishable from\\nthose obtained with classical methods such as Gibbs sampling for up to 20 dimensions.\\n2.1\\n\\nBoltzmann machine learning\\n\\nFor a binary vector of neural activities s ? {?1, 1}m and specified ?i and ?ij the Ising model takes\\nthe form\\n?\\n?\\nm\\nX\\nX\\n1\\n1\\nPI (s) = exp ?\\nhi si +\\nJij si sj ? ,\\n(1)\\nZ\\n2\\ni=1\\ni6=j\\n\\nwhere the local fields hi and the couplings Jij have to be chosen such that hsi i = ?i and hsi sj i ?\\nhsi ihsj i = ?ij . Unfortunately, finding the correct parameters turns out to be a difficult problem\\nwhich cannot be solved in closed form.\\nTherefore, one has to resort to an optimization approach to learn the model parameters hi and Jij\\nfrom data. This problem is called Boltzmann machine learning and is based on maximization of the\\nlog-likelihood L = ln PI ({si }N\\ni=1 |h, J) [1] where N is the number of samples. The gradient of the\\nlikelihood can be computed in terms of the empirical covariance and the covariance of si and sj as\\nproduced by the current model:\\n?L\\n= hsi sj iData ? hsi sj iModel\\n?Jij\\n\\n(2)\\n\\nThe second term on the right hand side is difficult to compute, as it requires sampling from the model.\\nSince the partition function Z in Eq. (1) is not available in closed form, Monte-Carlo methods such\\n3\\n\\n\fFigure 3: Random samples of dichotomized 4x4 patches from the van Hateren image data base (left) and from\\nthe corresponding dichotomized Gaussian distribution with equal covariance matrix (middle). It is not possible\\nto see any systematic difference between the samples from the two distributions. For comparison, this is not so\\nfor the sample from the independent model (right).\\n\\nas Gibbs sampling are employed [9] in order to approximate the required model average. This is\\ncomputationally demanding as sampling is necessary for each individual update. While efficient\\nsampling algorithms exist for special cases [6], it still remains a hard and time consuming problem\\nin the general case. Additionally, most sampling algorithms do not come with guarantees for the\\nquality of the approximation of the required average. In conclusion, parameter fitting of the Ising\\nmodel is slow and oftentimes painstaking, especially in high dimensions.\\n2.2\\n\\nModeling with the dichotomized Gaussian\\n\\nHere we explore an intriguing alternative to the Monte-Carlo approach: We replace the Ising model\\nby a ?near-maximum? entropy model, for which both parameter computation and sampling is easy. A\\nvery convenient, but in this context rarely recognized, candidate model is the dichotomized Gaussian\\ndistribution (DG) [11, 5, 4]. It is obtained by supposing that the observed binary vector s is generated\\nfrom a hidden Gaussian variable\\nz ? N (?, ?) ,\\n\\nsi = sgn(zi ).\\n\\n(3)\\n\\nWithout loss of generality, we can assume unit variances for the Gaussian, i.e. ?ii = 1, the mean ?\\nand the covariance matrix ? of s are given by\\n?i = 2?(?i ) ? 1 ,\\n\\n?ii = 4?(?i )?(??i ) ,\\n\\n?ij = 4?(?i , ?j , ?ij ) for i 6= j\\n\\n(4)\\n\\nwhere ?(x, y, ?) = ?2 (x, y, ?) ? ?(x)?(y) . Here ? is the univariate standardized cumulative\\nGaussian distribution and ?2 its bivariate counterpart. While the computation of the model parameters was hard for the Ising model, these equations can be easily inverted to find the parameters of\\nthe hidden Gaussian distribution:\\n\u0012\\n\u0013\\n?i + 1\\n?i = ??1\\n(5)\\n2\\nDetermining ?ij generally requires to find a suitable value such that ?ij ? 4?(?i , ?j , ?ij ) = 0.\\nThis can be efficently solved by numerical computations, since the function is monotonic in ?ij\\nand has a unique\u0001 zero crossing. We obtain an especially easy case, when ?i = ?j = 0, as then\\n?ij = sin ?2 ?ij .\\nIt is also possible to evaluate the probability mass function of the DG model by numerical integration,\\nZ b1\\nZ bm\\n\u0001\\n1\\nT ?1\\nPDG (s) =\\n.\\n.\\n.\\nexp\\n?(s\\n?\\n?)\\n?\\n(s\\n?\\n?)\\n,\\n(6)\\n(2?)N/2 |?|1/2 a1\\nam\\nwhere the integration limits are chosen as ai = 0 and bi = ?, if si = 1, and ai = ?? and bi = 0,\\notherwise.\\nIn summary, the proposed model has two advantages over the traditional Ising model: (1) Sampling\\nis easy, and (2) finding the model parameters is easy too.\\n4\\n\\n\f3\\n\\nNear-maximum entropy behavior of the dichotomized Gaussian\\ndistribution\\n\\nIn the previous section we introduced the dichotomized Gaussian distribution. Our conjecture is that\\nin many cases it can serve as a convenient approximation to the Ising model. Now, we investigate\\nhow good this approximation is. For a wide range of interaction terms and mean activations we\\nverify that the DG model closely resembles the Ising model. In particular we show that the entropy of\\nthe DG distribution is not smaller than the entropy of the Ising model even at rather high dimensions.\\n3.1\\n\\nRandom Connectivity\\n\\nWe created randomly connected networks of varying size m, where mean activations hi and\\ninteractions\\nterms Jij were drawn from N (0, 0.4). First, we compared the entropy HI =\\nP\\n? s PI (s) log2 PI (s) of the thus specified Ising model obtained by evaluating Eq. 1 with the entropy of the DG distribution HDG computed by numerical integration1 from Eq. 6 (twenty parameter\\nsets). The entropy difference ?H = HI ? HDG was smaller than 0.002 percent of HI (Fig. 1 A,\\nnote scale) and probably within the range of the numerical integration accuracy. In addition, we\\ncomputed the Jensen-Shannon divergence DJS [PI kPDG ] = 12 (DKL [PI kM ] + DKL [PDG kM ]),\\nwhere M = 21 (PI + PDG ) [8]. We find that DJS [PI kPDG ] is extremly small up to 10 dimensions\\n(Fig. 1 B). Therefore, the distributions seem to be not only close in their respective entropy, but also\\nto have a very similar structure.\\nNext, we extended this analysis to networks of larger size and repeated the same analysis for up to\\ntwenty dimensions. Since the integration in Eq. 6 becomes too time-consuming for m ? 20 due\\nto the large number of states, we used a histogram based estimate of PDG (using 3 ? 106 samples\\nfor m &lt; 15 and 15 ? 106 samples for m ? 15). The estimate of ?H is still very small at high\\ndimensions (Fig. 1 C, below 0.5%). We also computed DJS , which scaled similarly to ?H (data\\nnot shown).\\nIn Fig. 1 C, ?H seems to increase with dimensionality. Therefore, we investigated how the estimate\\nof ?H is influenced by the number of samples used. We computed both quantities for varying numbers of samples from the DG distribution (for m = 7, 10). As ?H decreases according to a power\\nlaw with increasing m, the rise of ?H observed in Fig. 1 C is most likely due to undersampling of\\nthe distribution.\\n3.2\\n\\nSpecified covariance structure\\n\\nTo explore the relationship between the two techniques more systematically, we generated covariance matrices with varying eigenvalue spectra. We used a parametric Toeplitz form, where the nth\\ndiagonal is set to a constant value exp(?? ? n) (Fig. 2A and B, m = 7, 10). We varied the decay\\nparameter ?, which led to a widely varying covariance structure (For eigenvalue spectra, see Fig. 2E\\nand F). We fit the Ising models using the Boltzmann machine gradient descent procedure. The covariance matrix of the samples drawn from the Ising model resembles the original very closely (Fig.\\n2C and D). We also computed the entropy of the DG model using the desired covariance structure.\\nWe estimated ?H and DJS [PG kPDG ] averaged over 10 trials with 105 samples obtained by Gibbs\\nsampling from the Ising model. ?H is very close to zero (Fig. 2G, m = 7) except for small ?s\\nand never exceeded 0.05%. Moreover, the structure of both distributions seems to be very similar as\\nwell (Fig. 2H, m = 7). At m = 10, both quantities scaled qualitatively similair (data not shown).\\nWe also repeated this analysis using equations 1 and 6 as before, which lead to similar results (data\\nnot shown).\\nOur experiments demonstrate clearly that the dichotomized Gaussian distribution constitutes a good\\napproximation to the quadratic exponential distribution for a large parameter range. In the following\\nsection, we will exploit the similarity between the two models to study how the role of second-order\\ncorrelations may change between low-dimensional and high-dimensional statistics in case of natural\\nimages.\\n1\\nFor integration, we used the mvncdf function of Matlab. For m ? 4 this function employs Monte-Carlo\\nintegration.\\n\\n5\\n\\n\fFigure 4: A: Negative log probabilities of the DG model are plotted against ground truth (red dots). Identical\\ndistributions fall on the diagonal. Data points outside the area enclosed by the dashed lines indicate significant\\ndifferences between the model and ground truth. The DG model matches the true distribution very well. For\\ncomparison the independent model is shown as well (blue crosses). B: The multi-information of the true\\ndistribution (blue dots) accurately agrees with the multi-information of the DG model (red line). Similar to\\nthe analysis in [12], we observe a power law behavior of the entropy of the independent model (black solid\\nline) and the mutli-information. Linear extrapolation (in the log-log plot) to higher dimensions is indicated by\\ndashed lines. C: Different way of presentation of the same data as in B: the joint entropy H = Hindep ? I\\n(blue dots) is plotted instead of I and the axis are in linear scale. The dashed red line represents the same\\nextrapolation as in B.\\n\\n4\\n\\nNatural images: Second order and beyond\\n\\nWe now investigate to which extent the statistics of natural images with dichotomized pixel intensities can be characterized by pairwise correlations only. In particular, we would like to know how\\nthe role of pairwise correlations opposed to higher-order correlations changes depending on the dimensionality. Thanks to the DG model introduced above, we are in the position to study the effect\\nof pairwise correlations for high-dimensional binary random variables (N ? 1000 or even larger).\\nWe use the van Hateren image database in log-intensity scale, from which we sample small image\\npatches at random positions. The threshold for the dichotomization is set to the median of pixel\\nintensities. That is, each binary variable encodes whether the corresponding pixel intensity is above\\nor below the median over the ensemble. Up to patch sizes of 4 ? 4 pixel, the true joint statistics can\\nbe assessed using nonparametric histogram methods. Before we present quantitative comparisons, it\\nis instructive to look at random samples from the true distribution (Fig. 3, left), from the DG model\\nwith same mean and covariance (Fig. 3, middle), and from the corresponding independent model\\n(Fig. 3, right). By visual inspection, it seems that the DG model fits the true distribution well.\\nIn order to quantify how well the DG model matches the true distribution, we draw two independent\\nsets of samples from each (N = 2 ? 106 for each set) and generate a scatter plot as shown in\\nFig. 4 A for 4 ? 4 image patches. Each dot corresponds to one of the 216 = 65536 possible different\\nbinary patterns. The relative frequencies of these patterns according to the DG model (red dots) and\\naccording to the independent model (blue dots) are plotted against the relative frequencies obtained\\nfrom the natural image patches. The solid diagonal line corresponds to a perfect match between\\nmodel and ground truth. The dashed lines enclose the regions within which deviations are to be\\nexpected due to the finite sampling size. Since most of the red dots fall within this region, the DG\\nmodel fits the data distribution very well.\\nP\\nWe also systematically evaluated the JS-divergence and the multi-information I[S] = k H[Sk ] ?\\nH[S] as a function of dimensionality. That is, we started with the bivariate marginal distribution\\nof two randomly selected pixels. Then we incrementally added more pixels of random location\\nuntil the random vector contains all the 16 pixels of the 4 ? 4 image patches. Independent of the\\ndimension, the JS-divergence between the DG model and the true distribution is smaller than 0.015\\nbits. For comparison, the JS-divergence between the independent model and the true distribution\\nincreases with dimensionality from roughly 0.2 bits in the case of two pixels up to 0.839 bits in\\nthe case of 16 pixels. For two independent sets of samples both drawn from natural image data the\\nJS-divergence ranges between 0.006 and 0.007 bits for 4 ? 4 patches setting the gold standard for\\nthe minimal possible JS-divergence one could achieve with any model due to finite sampling size.\\nCarrying out the same type of analysis as in [12], we make qualitatively the same observations as it\\nwas reported there: as shown above, we find a quite accurate match between the two distributions.\\n6\\n\\n\fFigure 5: Random samples of dichotomized 32x32 patches from the van Hateren image data base (left) and\\nfrom the corresponding dichotomized Gaussian distribution with equal covariance matrix (right). For the latter, the percept of typical objects is missing due to the ignorance of higher-order correlations. This striking\\ndifference is not obvious, however, at the level of 4x4 patches, for which we found an excellent match of the\\ndichotomized Gaussian to the ensemble of natural images.\\n\\nFurthermore, the multi-information of the DG model (red solid line) and of the true distribution (blue\\ndots) increases linearly on a loglog-scale with the number of dimensions (Fig. 4 B). Both findings\\ncan be verified only up to a rather limited number of dimensions (less than 20). Nevertheless, in [12],\\ntwo claims about the higher-dimensional statistics have been based on these two observations: First,\\nthat pairwise correlations may be sufficient to determine the full statistics of binary responses, and\\nsecondly, that the convergent scaling behavior in the log-log plot may indicate a transition towards\\nstrong order.\\nUsing natural images instead of retinal ganglion cell data, we would like to verify to what extent\\nthe low-dimensional observations can be used to support these claims about the high-dimensional\\nstatistics [10]. To this end we study the same kind of extrapolation (Fig. 4 B) to higher dimensions\\n(dashed lines) as in [12]. The difference between the entropy of the independent model and the\\nmulti-information yields the joint entropy of the respective distribution. If the extrapolation is taken\\nseriously, this difference seems to vanish at the order of 50 dimensions suggesting that the joint\\nentropy of the neural responses approaches zero at this size?say for 7 ? 7 image patches (Fig. 4 C).\\nThough it was not taken literally, this point of ?freezing? has been pointed out in [12] as a critical\\nnetwork size at which a transition to strong order is to be expected. The meaning of this assertion,\\nhowever, is not clear. First of all, the joint entropy of a distribution can never be smaller than the\\njoint entropy of any of its marginals. Therefore, the joint entropy cannot decrease with increasing\\nnumber of dimensions as the extrapolation would suggest (Fig. 4 C). Instead it would be necessary to\\nask more precisely how the growth rate of the joint entropy can be characterized and whether there\\nis a critical number of dimensions at which the growth rate suddenly drops. In our study with natural\\nimages, visual inspection does not indicate anything special to happen at the ?critical patch size? of\\n7 ? 7 pixels. Rather, for all patch sizes, the DG model yields dichotomized pink noise. In Fig. 5\\n(right) we show a sample from the DG model for 32?32 image patches (i.e. 1024 dimensions) which\\nprovides no indication for a particularly interesting change in the statistics towards strong order. The\\nexact law according to which the multi-information grows with the number of dimensions for large\\nm, however, is not easily assessed and remains to be explored.\\nFinally, we point out that the sufficiency of pairwise correlations at the level of m = 16 dimensions\\ndoes not hold any more in the case of large m: the samples from the true distribution at the left\\nhand side of Fig. 5 clearly show much more structure than the samples from the DG model (Fig. 5,\\nright), indicating that pairwise correlations do not suffice to determine the full statistics of large\\nimage patches. Even if the match between the DG model and the Ising model may turn out to be\\nless accurate in high dimensions, this would not affect our conclusion. Any mismatch would only\\nintroduce more order in the DG model than justified by pairwise correlations only.\\n\\n5\\n\\nConclusion and Outlook\\n\\nWe proposed a new approach to maximum entropy modeling of binary variables, extending maximum entropy analysis to previously infeasible high dimensions: As both sampling and finding pa7\\n\\n\frameters is easy for the dichotomized Gaussian model, it overcomes the computational drawbacks of\\nMonte-Carlo methods. We verified numerically that the empirical entropy of the DG model is comparable to that obtained with Gibbs sampling at least up to 20 dimensions. For practical purposes,\\nthe DG distribution can even be superior to the Gibbs sampler in terms of entropy maximization due\\nto the lack of independence between consecutive samples in the Gibbs sampler.\\nAlthough the Ising model and the DG model are in principle different, the match between the two\\nturns out to be surprisingly good for a large region of the parameter space. Currently, we are trying\\nto determine where the close similarity between the Ising model and the DG model breaks down.\\nIn addition, we explore the possibility to use the dichotomized Gaussian distribution as a proposal\\ndensity for Monte-Carlo methods such as importance sampling. As it is a very close approximation\\nto the Ising model, we expect this combination to yield highly efficient sampling behaviour. In\\nsummary, by linking the DG model to the Ising model, we believe that maximum entropy modeling\\nof multivariate binary random variables will become much more practical in the future.\\nWe used the DG model to investigate the role of second-order correlations in the context of sensory coding of natural images. While for small image patches the DG model provided an excellent\\nfit to the true distribution, we were able to show that this agreement breakes down in the case\\nof larger image patches. Thus caution is required when extrapolating from low-dimensional measurements to higher-dimensional distributions because higher-order correlations may be invisible in\\nlow-dimensional marginal distributions. Nevertheless, the maximum entropy approach seems to be\\na promising tool for the analysis of correlated neural activities, and the DG model can facilitate its\\nuse significantly in practice.\\nAcknowledgments\\nWe thank Jakob Macke, Pierre Garrigues, and Greg Stephens for helpful comments and stimulating discussions, as well as Alexander Ecker and Andreas Hoenselaar for last minute advice. An implementation of the DG model in Matlab and R will be avaible at our website\\nhttp://www.kyb.tuebingen.mpg.de/bethgegroup/code/DGsampling.\\n\\nReferences\\n[1] D.H. Ackley, G.E. Hinton, and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive\\nScience, 9:147?169, 1985.\\n[2] H.B. Barlow. Sensory mechanisms, the reduction of redundancy, and intelligence. In The Mechanisation\\nof Thought Processes, pages 535?539, London: Her Majesty?s Stationery Office, 1959.\\n[3] M. Bethge. Factorial coding of natural images: How effective are linear model in removing higher-order\\ndependencies? J. Opt. Soc. Am. A, 23(6):1253?1268, June 2006.\\n[4] D.R. Cox and N. Wermuth. On some models for multivariate binary variables parallel in complexity with\\nthe multivariate gaussian distribution. Biometrika, 89:462?469, 2002.\\n[5] L.J. Emrich and M.R. Piedmonte. A method for generating high-dimensional multivariate binary variates.\\nThe American Statistician, 45(4):302?304, 1991.\\n[6] M. Huber. A bounding chain for swendsen-wang. Random Structures &amp; Algorithms, 22:53?59, 2002.\\n[7] E.T. Jaynes. Where do we stand on maximum entropy inference. In R.D. Levine and M. Tribus, editors,\\nThe Maximum Entropy Formalism. MIT Press, Cambridge, MA, 1978.\\n[8] J. Linn. Divergence measures based on the shannon entropy. IEEE Trans Inf Theory, 37:145?151, 1991.\\n[9] D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University Press,\\n2003.\\n[10] Sheila H Nirenberg and Jonathan D Victor. Analyzing the activity of large populations of neurons: how\\ntractable is the problem? Current Opinion in Neurobiology, 17:397?400, August 2007.\\n[11] Karl Pearson. On a new method of determining correlation between a measured character a, and a character b, of which only the percentage of cases wherein b exceeds (or falls short of) a given intensity is\\nrecorded for each grade of a. Biometrika, 7:96?105, 1909.\\n[12] Elad Schneidman, Michael J Berry, Ronen Segev, and William Bialek. Weak pairwise correlations imply\\nstrongly correlated network states in a neural population. Nature, 440(7087):1007?1012, Apr 2006.\\n[13] J Shlens, JD Field, JL Gauthier, MI Grivich, D Petrusca, A Sher, AM Litke, and EJ Chichilnisky. The\\nstructure of multi-neuron firing patterns in primate retina. J Neurosci, 26(32):8254?8266, Aug 2006.\\n[14] G. Tkacik, E. Schneidman, M.J. Berry, and W. Bialek. Ising models for networks of real neurons. arXiv:qbio.NC/0611072, 1:1?4, 2006.\\n\\n8\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions Nearest-Neighbor Sample Compression:\\nEfficiency, Consistency, Infinite Dimensions\\n\\nAryeh Kontorovich\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nkaryeh@cs.bgu.ac.il\\n\\nSivan Sabato\\nDepartment of Computer Science\\nBen-Gurion University of the Negev\\nsabatos@bgu.ac.il\\n\\nRoi Weiss\\nDepartment of Computer Science and Applied Mathematics\\nWeizmann Institute of Science\\nroiw@weizmann.ac.il\\n\\nAbstract\\nWe examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based\\nmulticlass learning algorithm. This algorithm is derived from sample compression\\nbounds and enjoys the statistical advantages of tight, fully empirical generalization\\nbounds, as well as the algorithmic advantages of a faster runtime and memory\\nsavings. We prove that this algorithm is strongly Bayes-consistent in metric\\nspaces with finite doubling dimension ? the first consistency result for an efficient\\nnearest-neighbor sample compression scheme. Rather surprisingly, we discover\\nthat this algorithm continues to be Bayes-consistent even in a certain infinitedimensional setting, in which the basic measure-theoretic conditions on which\\nclassic consistency proofs hinge are violated. This is all the more surprising, since\\nit is known that k-NN is not Bayes-consistent in this setting. We pose several\\nchallenging open problems for future research.\\n\\n1\\n\\nIntroduction\\n\\nThis paper deals with Nearest-Neighbor (NN) learning algorithms in metric spaces. Initiated by\\nFix and Hodges in 1951 [16], this seemingly naive learning paradigm remains competitive against\\nmore sophisticated methods [8, 46] and, in its celebrated k-NN version, has been placed on a solid\\ntheoretical foundation [11, 44, 13, 47].\\nAlthough the classic 1-NN is well known to be inconsistent in general, in recent years a series of\\npapers has presented variations on the theme of a regularized 1-NN classifier, as an alternative to the\\nBayes-consistent k-NN. Gottlieb et al. [18] showed that approximate nearest neighbor search can\\nact as a regularizer, actually improving generalization performance rather than just injecting noise.\\nIn a follow-up work, [27] showed that applying Structural Risk Minimization to (essentially) the\\nmargin-regularized data-dependent bound in [18] yields a strongly Bayes-consistent 1-NN classifier.\\nA further development has seen margin-based regularization analyzed through the lens of sample\\ncompression: a near-optimal nearest neighbor condensing algorithm was presented [20] and later\\nextended to cover semimetric spaces [21]; an activized version also appeared [25]. As detailed in\\n[27], margin-regularized 1-NN methods enjoy a number of statistical and computational advantages\\nover the traditional k-NN classifier. Salient among these are explicit data-dependent generalization\\nbounds, and considerable runtime and memory savings. Sample compression affords additional\\nadvantages, in the form of tighter generalization bounds and increased efficiency in time and space.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\fIn this work we study the Bayes-consistency of a compression-based 1-NN multiclass learning\\nalgorithm, in both finite-dimensional and infinite-dimensional metric spaces. The algorithm is\\nessentially the passive component of the active learner proposed by Kontorovich, Sabato, and Urner\\nin [25], and we refer to it in the sequel as KSU; for completeness, we present it here in full (Alg. 1).\\nWe show that in finite-dimensional metric spaces, KSU is both computationally efficient and Bayesconsistent. This is the first compression-based multiclass 1-NN algorithm proven to possess both of\\nthese properties. We further exhibit a surprising phenomenon in infinite-dimensional spaces, where\\nwe construct a distribution for which KSU is Bayes-consistent while k-NN is not.\\nMain results. Our main contributions consist of analyzing the performance of KSU in finite and\\ninfinite dimensional settings, and comparing it to the classical k-NN learner. Our key findings are\\nsummarized below.\\n? In Theorem 2, we show that KSU is computationally efficient and strongly Bayes-consistent\\nin metric spaces with a finite doubling dimension. This is the first (strong or otherwise)\\nBayes-consistency result for an efficient sample compression scheme for a multiclass (or\\neven binary)1 1-NN algorithm. This result should be contrasted with the one in [27], where\\nmargin-based regularization was employed, but not compression; the proof techniques\\nfrom [27] do not carry over to the compression-based scheme. Instead, novel arguments\\nare required, as we discuss below. The new sample compression technique provides a\\nBayes-consistency proof for multiple (even countably many) labels; this is contrasted with\\nthe multiclass 1-NN algorithm in [28], which is not compression-based, and requires solving\\na minimum vertex cover problem, thereby imposing a 2-approximation factor whenever\\nthere are more than two labels.\\n? In Theorem 4, we make the surprising discovery that KSU continues to be Bayes-consistent\\nin a certain infinite-dimensional setting, even though this setting violates the basic measuretheoretic conditions on which classic consistency proofs hinge, including Theorem 2. This\\nis all the more surprising, since it is known that k-NN is not Bayes-consistent for this\\nconstruction [9]. We are currently unaware of any separable2 metric probability space on\\nwhich KSU fails to be Bayes-consistent; this is posed as an intriguing open problem.\\nOur results indicate that in finite dimensions, an efficient, compression-based, Bayes-consistent\\nmulticlass 1-NN algorithm exists, and hence can be offered as an alternative to k-NN, which is well\\nknown to be Bayes-consistent in finite dimensions [12, 41]. In contrast, in infinite dimensions, our\\nresults show that the condition characterizing the Bayes-consistency of k-NN does not extend to all\\nNN algorithms. It is an open problem to characterize the necessary and sufficient conditions for the\\nexistence of a Bayes-consistent NN-based algorithm in infinite dimensions.\\nRelated work. Following the pioneering work of [11] on nearest-neighbor classification, it was\\nshown by [13, 47, 14] that the k-NN classifier is strongly Bayes consistent in Rd . These results\\nmade extensive use of the Euclidean structure of Rd , but in [41] a weak Bayes-consistency result was\\nshown for metric spaces with a bounded diameter and a bounded doubling dimension, and additional\\ndistributional smoothness assumptions. More recently, some of the classic results on k-NN risk\\ndecay rates were refined by [10] in an analysis that captures the interplay between the metric and the\\nsampling distribution. The worst-case rates have an exponential dependence on the dimension (i.e.,\\nthe so-called curse of dimensionality), and Pestov [33, 34] examines this phenomenon closely under\\nvarious distributional and structural assumptions.\\nConsistency of NN-type algorithms in more general (and in particular infinite-dimensional) metric\\nspaces was discussed in [1, 5, 6, 9, 30]. In [1, 9], characterizations of Bayes-consistency were\\ngiven in terms of Besicovitch-type conditions (see Eq. (3)). In [1], a generalized ?moving window?\\nclassification rule is used and additional regularity conditions on the regression function are imposed.\\nThe filtering technique (i.e., taking the first d coordinates in some basis representation) was shown to\\nbe universally consistent in [5]. However, that algorithm suffers from the cost of cross-validating\\nover both the dimension d and number of neighbors k. Also, the technique is only applicable in\\n1\\nAn efficient sample compression algorithm was given in [20] for the binary case, but no Bayes-consistency\\nguarantee is known for it.\\n2\\nC?rou and Guyader [9] gave a simple example of a nonseparable metric on which all known nearest-neighbor\\nmethods, including k-NN and KSU, obviously fail.\\n\\n2\\n\\n\fHilbert spaces (as opposed to more general metric spaces) and provides only asymptotic consistency,\\nwithout finite-sample bounds such as those provided by KSU. The insight of [5] is extended to the\\nmore general Banach spaces in [6] under various regularity assumptions.\\nNone of the aforementioned generalization results for NN-based techniques are in the form of\\nfully empirical, explicitly computable sample-dependent error bounds. Rather, they are stated in\\nterms of the unknown Bayes-optimal rate, and some involve additional parameters quantifying the\\nwell-behavedness of the unknown distribution (see [27] for a detailed discussion). As such, these\\nguarantees do not enable a practitioner to compute a numerical generalization error estimate for a\\ngiven training sample, much less allow for a data-dependent selection of k, which must be tuned via\\ncross-validation. The asymptotic expansions in [43, 37, 23, 40] likewise do not provide a computable\\nfinite-sample bound. The quest for such bounds was a key motivation behind the series of works\\n[18, 28, 20], of which KSU [25] is the latest development.\\nThe work of Devroye et al. [14, Theorem 21.2] has implications for 1-NN classifiers in Rd that\\nare defined based on data-dependent majority-vote partitions of the space. It is shown that under\\nsome conditions, a fixed mapping from each sample size to a data-dependent partition rule induces a\\nstrongly Bayes-consistent algorithm. This result requires the partition rule to have a bounded VC\\ndimension, and since this rule must be fixed in advance, the algorithm is not fully adaptive. Theorem\\n19.3 ibid. proves weak consistency for an inefficient compression-based algorithm, which selects\\namong all the possible compression sets of a certain size, and maintains a certain rate of compression\\nrelative to the sample size. The generalizing power of sample compression was independently\\ndiscovered by [31], and later elaborated upon by [22]. In the context of NN classification, [14] lists\\nvarious condensing heuristics (which have no known performance guarantees) and leaves open the\\nalgorithmic question of how to minimize the empirical risk over all subsets of a given size.\\nThe first compression-based 1-NN algorithm with provable optimality guarantees was given in [20];\\nit was based on constructing ?-nets in spaces with a finite doubling dimension. The compression\\nsize of this construction was shown to be nearly unimprovable by an efficient algorithm unless P=NP.\\nWith ?-nets as its algorithmic engine, KSU inherits this near-optimality. The compression-based\\n1-NN paradigm was later extended to semimetrics in [21], where it was shown to survive violations\\nof the triangle inequality, while the hierarchy-based search methods that have become standard for\\nmetric spaces (such as [4, 18] and related approaches) all break down.\\nIt was shown in [27] that a margin-regularized 1-NN learner (essentially, the one proposed in [18],\\nwhich, unlike [20], did not involve sample compression) becomes strongly Bayes-consistent when the\\nmargin is chosen optimally in an explicitly prescribed sample-dependent fashion. The margin-based\\ntechnique developed in [18] for the binary case was extended to multiclass in [28]. Since the algorithm\\nrelied on computing a minimum vertex cover, it was not possible to make it both computationally\\nefficient and Bayes-consistent when the number of lables exceeds two. An additional improvement\\nover [28] is that the generalization bounds presented there had an explicit (logarithmic) dependence\\non the number of labels, while our compression scheme extends seamlessly to countable label spaces.\\nPaper outline. After fixing the notation and setup in Sec. 2, in Sec. 3 we present KSU, the\\ncompression-based 1-NN algorithm we analyze in this work. Sec. 4 discusses our main contributions\\nregarding KSU, together with some open problems. High-level proof sketches are given in Sec. 5 for\\nthe finite-dimensional case, and Sec. 6 for the infinite-dimensional case. Full detailed proofs can be\\nfound in [26].\\n\\n2\\n\\nSetting and Notation\\n\\nOur instance space is the metric space (X , ?), where X is the instance domain and ? is the metric.\\n(See Appendix A in [26] for relevant background on metric measure spaces.) We consider a countable\\nlabel space Y. The unknown sampling distribution is a probability measure ?\\n? over X ? Y, with\\nmarginal ? over X . Denote by (X, Y ) ? ?\\n? a pair drawn according to ?\\n?. The generalization error of a\\nclassifier f : X ? Y is given by err?? (f ) := P?? (Y 6= f (X)),\\nP and its empirical error with respect to\\na labeled set S 0 ? X ? Y is given by err(f,\\nc S 0 ) := |S10 | (x,y)?S 0 1[y 6= f (x)]. The optimal Bayes\\nrisk of ?\\n? is R??? := inf err?? (f ), where the infimum is taken over all measurable classifiers f : X ? Y.\\nWe say that ?\\n? is realizable when R??? = 0. We omit the overline in ?\\n? in the sequel when there is no\\nambiguity.\\n3\\n\\n\fFor a finite labeled set S ? X ? Y and any x ? X , let Xnn (x, S) be the nearest neighbor of x with\\nrespect to S and let Ynn (x, S) be the nearest neighbor label of x with respect to S:\\n(Xnn (x, S), Ynn (x, S)) := argmin ?(x, x0 ),\\n(x0 ,y 0 )?S\\n\\nwhere ties are broken arbitrarily. The 1-NN classifier induced by S is denoted by hS (x) :=\\nYnn (x, S). The set of points in S, denoted by X = {X1 , . . . , X|S| } ? X , induces\\na Voronoi partition of X , V(X) := {V1 (X), . . . , V|S| (X)}, where each Voronoi cell is\\nVi (X) := {x ? X : argminj?{1,...,|S|} ?(x, Xj ) = i}. By definition, ?x ? Vi (X), hS (x) = Yi .\\nA 1-NN algorithm is a mapping from an i.i.d. labeled sample Sn ? ?\\n?n to a labeled set Sn0 ? X ? Y,\\nyielding the 1-NN classifier hSn0 . While the classic 1-NN algorithm sets Sn0 := Sn , in this work we\\nstudy a compression-based algorithm which sets Sn0 adaptively, as discussed further below.\\nA 1-NN algorithm is strongly Bayes-consistent on ?\\n? if err(hSn0 ) converges to R? almost surely,\\nthat is P[limn?? err(hSn0 ) = R? ] = 1. An algorithm is weakly Bayes-consistent on ?\\n? if err(hSn0 )\\nconverges to R? in expectation, limn?? E[err(hSn0 )] = R? . Obviously, the former implies the\\nlatter. We say that an algorithm is Bayes-consistent on a metric space if it is Bayes-consistent on all\\ndistributions in the metric space.\\nA convenient property that is used when studying the Bayes-consistency of algorithms in metric\\nspaces is the doubling dimension. Denote the open ball of radius r around x by Br (x) := {x0 ?\\n?r (x) denote the corresponding closed ball. The doubling dimension of a\\nX : ?(x, x0 ) &lt; r} and let B\\nmetric space (X , ?) is defined as follows. Let n be the smallest number such that every ball in X can\\nbe covered by n balls of half its radius, where all balls are centered at points of X . Formally,\\nn := min{n ? N : ?x ? X , r &gt; 0, ?x1 , . . . , xn ? X s.t. Br (x) ? ?ni=1 Br/2 (xi )}.\\nThen the doubling dimension of (X , ?) is defined by ddim(X , ?) := log2 n.\\nFor an integer n, let [n] := {1, . . . , n}. Denote the set of all index vectors of length d by In,d :=\\n[n]d . Given a labeled set Sn = (Xi , Yi )i?[n] and any i = {i1 , . . . , id } ? In,d , denote the subsample of Sn indexed by i by Sn (i) := {(Xi1 , Yi1 ), . . . , (Xid , Yid )}. Similarly, for a vector Y 0 =\\n{Y10 , . . . , Yd0 } ? Y d , denote by Sn (i, Y 0 ) := {(Xi1 , Y10 ), . . . , (Xid , Yd0 )}, namely the sub-sample\\nof Sn as determined by i where the labels are replaced with Y 0 . Lastly, for i, j ? In,d , we denote\\nSn (i; j) := {(Xi1 , Yj1 ), . . . , (Xid , Yjd )}.\\n\\n3\\n\\n1-NN majority-based compression\\n\\nIn this work we consider the 1-NN majority-based compression algorithm proposed in [25], which\\nwe refer to as KSU. This algorithm is based on constructing ?-nets at different scales; for ? &gt; 0\\nand A ? X , a set X ? A is said to be a ?-net of A if ?a ? A, ?x ? X : ?(a, x) ? ? and for all\\nx 6= x0 ? X, ?(x, x0 ) &gt; ?.3\\nThe algorithm (see Alg. 1) operates as follows. Given an input sample Sn , whose set of points is\\ndenoted Xn = {X1 , . . . , Xn }, KSU considers all possible scales ? &gt; 0. For each such scale it\\nconstructs a ?-net of Xn . Denote this ?-net by X(?) := {Xi1 , . . . , Xim }, where m ? m(?) denotes\\nits size and i ? i(?) := {i1 , . . . , im } ? In,m denotes the indices selected from Sn for this ?-net.\\nFor every such ?-net, the algorithm attaches the labels Y 0 ? Y 0 (?) ? Y m , which are the empirical\\nmajority-vote labels in the respective Voronoi cells in the partition V(X(?)) = {V1 , . . . , Vm }.\\nFormally, for i ? [m],\\nYi0 ? argmax |{j ? [n] | Xj ? Vi , Yj = y}|,\\n(1)\\ny?Y\\n\\nwhere ties are broken arbitrarily. This procedure creates a labeled set Sn0 (?) := Sn (i(?), Y 0 (?)) for\\nevery relevant ? ? {?(Xi , Xj ) | i, j ? [n]} \\ {0}. The algorithm then selects a single ?, denoted\\n? ? ? ?n? , and outputs hSn0 (? ? ) . The scale ? ? is selected so as to minimize a generalization error\\nbound, which upper bounds err(Sn0 (?)) with high probability. This error bound, denoted Q in the\\nalgorithm, can be derived using a compression-based analysis, as described below.\\n3\\nFor technical reasons, having to do with the construction in Sec. 6, we depart slightly from the standard\\ndefinition of a ?-net X ? A. The classic definition requires that (i) ?a ? A, ?x ? X : ?(a, x) &lt; ? and (ii)\\n?x 6= x0 ? X : ?(x, x0 ) ? ?. In our definition, the relations &lt; and ? in (i) and (ii) are replaced by ? and &gt;.\\n\\n4\\n\\n\fAlgorithm 1 KSU: 1-NN compression-based algorithm\\nRequire: Sample Sn = (Xi , Yi )i?[n] , confidence ?\\nEnsure: A 1-NN classifier\\n1: Let ? := {?(Xi , Xj ) | i, j ? [n]} \\ {0}\\n2: for ? ? ? do\\n3:\\nLet X(?) be a ?-net of {X1 , . . . , Xn }\\n4:\\nLet m(?) := |X(?)|\\n5:\\nFor each i ? [m(?)], let Yi0 be the majority label in Vi (X(?)) as defined in Eq. (1)\\n6:\\nSet Sn0 (?) := (X(?), Y 0 (?))\\n7: end for\\n8: Set ?(?) := err(h\\nc Sn0 (?) , Sn )\\n9: Find ?n? ? argmin??? Q(n, ?(?), 2m(?), ?), where Q is, e.g., as in Eq. (2)\\n10: Set Sn0 := Sn0 (?n? )\\n11: return hSn0\\n\\nm\\nWe say that a mapping Sn 7? Sn0 is a compression scheme if there is a function C : ??\\nm=0 (X ?Y) ?\\n2X ?Y , from sub-samples to subsets of X ? Y, such that for every Sn there exists an m and a sequence\\ni ? In,m such that Sn0 = C(Sn (i)). Given a compression scheme Sn 7? Sn0 and a matching function\\nC, we say that a specific Sn0 is an (?, m)-compression of a given Sn if Sn0 = C(Sn (i)) for some\\ni ? In,m and err(h\\nc Sn0 , Sn ) ? ?. The generalization power of compression was recognized by [17]\\nand [22]. Specifically, it was shown in [21, Theorem 8] that if the mapping Sn 7? Sn0 is a compression\\nscheme, then with probability at least 1 ? ?, for any Sn0 which is an (?, m)-compression of Sn ? ?\\n?n ,\\nwe have (omitting the constants, explicitly provided therein, which do not affect our analysis)\\ns\\nnm\\n? log(n) + log(1/?)\\nn\\nm log(n) + log(1/?)\\nerr(hSn0 ) ?\\n? + O(\\n) + O( n?m\\n). (2)\\nn?m\\nn?m\\nn?m\\n\\nDefining Q(n, ?, m, ?) as the RHS of Eq. (2) provides KSU with a compression bound. The following\\nproposition shows that KSU is a compression scheme, which enables us to use Eq. (2) with the\\nappropriate substitution.4\\nProposition 1. The mapping Sn 7? Sn0 defined by Alg. 1 is a compression scheme whose output Sn0\\nis a (err(h\\nc Sn0 ), 2|Sn0 |)-compression of Sn .\\n? i , Y?i )i?[2m] ) = (X\\n? i , Y?i+m )i?[m] , and observe that for all\\nProof. Define the function C by C((X\\n0\\nSn , we have Sn = C(Sn (i(?); j(?))), where i(?) is the ?-net index set as defined above, and\\nj(?) = {j1 , . . . , jm(?) } ? In,m(?) is some index vector such that Yi0 = Yji for every i ? [m(?)].\\nSince Yi0 is an empirical majority vote, clearly such a j exists. Under this scheme, the output Sn0 of\\nthis algorithm is a (err(h\\nc Sn0 ), 2|Sn0 |)-compression.\\nKSU is efficient, for any countable Y. Indeed, Alg. 1 has a naive runtime complexity of O(n4 ), since\\nO(n2 ) values of ? are considered and a ?-net is constructed for each one in time O(n2 ) (see [20,\\nAlgorithm 1]). Improved runtimes can be obtained, e.g., using the methods in [29, 18]. In this work\\nwe focus on the Bayes-consistency of KSU, rather than optimize its computational complexity. Our\\nBayes-consistency results below hold for KSU, whenever the generalization bound Q(n, ?, m, ?n )\\nsatisfies the following properties:\\nProperty 1 For any integer n and ? ? (0, 1), with probability 1 ? ? over the i.i.d. random sample\\nSn ? ?\\n?n , for all ? ? [0, 1] and m ? [n]: If Sn0 is an (?, m)-compression of Sn , then\\nerr(hSn0 ) ? Q(n, ?, m, ?).\\nProperty 2 Q is monotonically increasing in ? and in m.\\nProperty 3 There is a sequence {?n }?\\nn=1 , ?n ? (0, 1) such that\\nlim\\n\\nP?\\n\\nn=1 ?n\\n\\n&lt; ? and for all m,\\n\\nsup (Q(n, ?, m, ?n ) ? ?) = 0.\\n\\nn?? ??[0,1]\\n4\\n\\nIn [25] the analysis was based on compression with side information, and does not extend to infinite Y.\\n\\n5\\n\\n\fThe compression bound in Eq. (2) clearly\\nP?satisfies these properties. Note that Property 3 is satisfied\\nby Eq. (2) using any convergent series n=1 ?n &lt; ? such that ?n = e?o(n) ; in particular, the decay\\nof ?n cannot be too rapid.\\n\\n4\\n\\nMain results\\n\\nIn this section we describe our main results. The proofs appear in subsequent sections. First, we show\\nthat KSU is Bayes-consistent if the instance space has a finite doubling dimension. This contrasts\\nwith classical 1-NN, which is only Bayes-consistent if the distribution is realizable.\\nTheorem 2. Let (X , ?) be a metric space with a finite doubling-dimension. Let Q be a generalization\\nbound that satisfies Properties 1-3, and let ?n be as stipulated by Property 3 for Q. If the input\\nconfidence ? for input size n is set to ?n , then the 1-NN classifier hSn0 (?n? ) calculated by KSU is\\nstrongly Bayes consistent on (X , ?): P(limn?? err(hSn0 ) = R? ) = 1.\\nThe proof, provided in Sec. 5, closely follows the line of reasoning in [27], where the strong Bayesconsistency of an adaptive margin-regularized 1-NN algorithm was proved, but with several crucial\\ndifferences. In particular, the generalization bounds used by KSU are purely compression-based, as\\nopposed to the Rademacher-based generalization bounds used in [27]. The former can be much tighter\\nin practice and guarantee Bayes-consistency of KSU even for countably many labels. This however\\nrequires novel technical arguments, which are discussed in detail in Appendix B.1 in [26]. Moreover,\\nsince the compression-based bounds do not explicitly depend on ddim, they can be used even when\\nddim is infinite, as we do in Theorem 4 below. To underscore the subtle nature of Bayes-consistency,\\nwe note that the proof technique given here does not carry to an earlier algorithm, suggested in [20,\\nTheorem 4], which also uses ?-nets. It is an open question whether the latter is Bayes-consistent.\\nNext, we study Bayes-consistency of KSU in infinite dimensions (i.e., with ddim = ?) ? in particular, in a setting where k-NN was shown by [9] not to be Bayes-consistent. Indeed, a straightforward\\napplication of [9, Lemma A.1] yields the following result.\\nTheorem 3 (C?rou and Guyader [9]). There exists an infinite dimensional separable metric space\\n(X , ?) and a realizable distribution ?\\n? over X ? {0, 1} such that no kn -NN learner satisfying\\nkn /n ? 0 when n ? ? is Bayes-consistent under ?\\n?. In particular, this holds for any space and\\nrealizable distribution ?\\n? that satisfy the following condition: The set C of points labeled 1 by ?\\n?\\nsatisfies\\n?r (x))\\n?(C ? B\\n?(C) &gt; 0\\nand\\n?x ? C, lim\\n= 0.\\n(3)\\n?\\nr?0\\n?(Br (x))\\nSince ?(C) &gt; 0, Eq. (3) constitutes a violation of the Besicovitch covering property. In doubling\\nspaces, the Besicovitch covering theorem precludes such a violation [15]. In contrast, as [35, 36]\\nshow, in infinite-dimensional spaces this violation can in fact occur. Moreover, this is not an isolated\\npathology, as this property is shared by Gaussian Hilbert spaces [45].\\nAt first sight, Eq. (3) might appear to thwart any 1-NN algorithm applied to such a distribution.\\nHowever, the following result shows that this is not the case: KSU is Bayes-consistent on a distribution\\nwith this property.\\nTheorem 4. There is a metric space equipped with a realizable distribution for which KSU is weakly\\nBayes-consistent, while any k-NN classifier necessarily is not.\\nThe proof relies on a classic construction of Preiss [35] which satisfies Eq. (3). We show that the\\nstructure of the construction, combined with the packing and covering properties of ?-nets, imply that\\nthe majority-vote classifier induced by any ?-net with a sufficienlty small ? approaches the Bayes\\nerror. To contrast with Theorem 4, we next show that on the same construction, not all majority-vote\\nVoronoi partitions succeed. Indeed, if the packing property of ?-nets is relaxed, partition sequences\\nobstructing Bayes-consistency exist.\\nTheorem 5. For the example constructed in Theorem 4, there exists a sequence of Voronoi partitions\\nwith a vanishing diameter such that the induced true majority-vote classifiers are not Bayes consistent.\\nThe above result also stands in contrast to [14, Theorem 21.2], showing that, unlike in finite dimensions, the partitions? vanishing diameter is insufficient to establish consistency when ddim = ?. We\\nconclude the main results by posing intriguing open problems.\\n6\\n\\n\fOpen problem 1. Does there exist a metric probability space on which some k-NN algorithm is\\nconsistent while KSU is not? Does there exist any separable metric space on which KSU fails?\\nOpen problem 2. C?rou and Guyader [9] distill a certain Besicovitch condition which is necessary\\nand sufficient for k-NN to be Bayes-consistent in a metric space. Our Theorem 4 shows that the\\nBesicovitch condition is not necessary for KSU to be Bayes-consistent. Is it sufficient? What is a\\nnecessary condition?\\n\\n5\\n\\nBayes-consistency of KSU in finite dimensions\\n\\nIn this section we give a high-level proof of Theorem 2, showing that KSU is strongly Bayesconsistent in finite-dimensional metric spaces. A fully detailed proof is given in Appendix B in\\n[26].\\nRecall the optimal empirical error ?n? ? ?(?n? ) and the optimal compression size m?n ? m(?n? ) as\\ncomputed by KSU. As shown in Proposition 1, the sub-sample Sn0 (?n? ) is an (?n? , 2m?n )-compression\\nof Sn . Abbreviate the compression-based generalization bound used in KSU by\\nQn (?, m) := Q(n, ?, 2m, ?n ).\\nTo show Bayes-consistency, we start by a standard decomposition of the excess error over the optimal\\nBayes into two terms:\\n\u0001\\n\u0001\\nerr(hSn0 (?n? ) ) ? R? = err(hSn0 (?n? ) ) ? Qn (?n? , m?n ) + Qn (?n? , m?n ) ? R? =: TI (n) + TII (n),\\nand show that each term decays to zero with probability one. For the first term, Property 1 for Q,\\ntogether with the Borel-Cantelli lemma, readily imply lim supn?? TI (n) ? 0 with probability one.\\nThe main challenge is showing that lim supn?? TII (n) ? 0 with probability one. We do so in\\nseveral stages:\\n1. Loosely speaking, we first show (Lemma 10) that the Bayes error R? can be well approximated using 1-NN classifiers defined by the true (as opposed to empirical) majority-vote\\nlabels over fine partitions of X . In particular, this holds for any partition induced by a ?-net\\nof X with a sufficiently small ? &gt; 0. This approximation guarantee relies on the fact that in\\nfinite-dimensional spaces, the class of continuous functions with compact support is dense\\nin L1 (?) (Lemma 9).\\n2. Fix ?? &gt; 0 sufficiently small such that any true majority-vote classifier induced by a ?? -net\\nhas a true error close to R? , as guaranteed by stage 1. Since for bounded subsets of finitedimensional spaces the size of any ?-net is finite, the empirical error of any majority-vote\\n?-net almost surely converges to its true majority-vote error as the sample size n ? ?. Let\\nn(?\\n? ) sufficiently large such that Qn(?? ) (?(?\\n? ), m(?\\n? )) as computed by KSU for a sample of\\n0\\nsize n(?\\n? ) is a reliable estimate for the true error of hSn(?\\n(?\\n?).\\n?)\\n3. Let ?? and n(?\\n? ) be as in stage 2. Given a sample of size n = n(?\\n? ), recall that KSU\\nselects an optimal ? ? such that Qn (?(?), m(?)) is minimized over all ? &gt; 0. For margins\\n? \u001c ?? , which are prone to over-fitting, Qn (?(?), m(?)) is not a reliable estimate for\\nhSn0 (?) since compression may not yet taken place for samples of size n. Nevertheless, these\\nmargins are discarded by KSU due to the penalty term in Q. On the other hand, for ?-nets\\nwith margin ? \u001d ?? , which are prone to under-fitting, the true error is well estimated by\\nQn (?(?), m(?)). It follows that KSU selects ?n? ? ?? and Qn (?n? , m?n ) ? R? , implying\\nlim supn?? TII (n) ? 0 with probability one.\\nAs one can see, the assumption that X is finite-dimensional plays a major role in the proof. A simple\\nargument shows that the family of continuous functions with compact support is no longer dense\\nin L1 in infinite-dimensional spaces. In addition, ?-nets of bounded subsets in infinite dimensional\\nspaces need no longer be finite.\\n\\n6\\n\\nOn Bayes-consistency of NN algorithms in infinite dimensions\\n\\nIn this section we study the Bayes-consistency properties of 1-NN algorithms on a classic infinitedimensional construction of Preiss [35], which we describe below in detail. This construction was\\n7\\n\\n\fz1:k?2\\n?k?1\\nz1:k?1\\n?k\\n\\n?k\\nz1:k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\n?k\\n\\nz\\n\\nC = Z?\\n\\n?? (z) for some z ? C.\\nFigure 1: Preiss?s construction. Encircled is the closed ball B\\nk?1\\nfirst introduced as a concrete example showing that in infinite-dimensional spaces the Besicovich\\ncovering theorem [15] can be strongly violated, as manifested in Eq. (3).\\nExample 1 (Preiss?s construction). The construction (see Figure 1) defines an infinite-dimensional\\nmetric space (X , ?) and a realizable measure ?\\n? over X ? Y with the binary label set Y = {0, 1}.\\nIt relies on two sequences: a sequence of natural numbers {Nk }k?N and a sequence of positive\\nnumbers {ak }k?N . The two sequences should satisfy the following:\\nP?\\nlimk?? ak N1 . . . Nk+1 = ?; and limk?? Nk = ?. (4)\\nk=1 ak N1 . . . Nk = 1;\\nQ\\nThese properties are satisfied, for instance, by setting Nk := k! and ak := 2?k / i?[k] Ni . Let Z0\\nbe the set of all finite sequences (z1 , . . . , zk )k?N of natural numbers such that zi ? Ni , and let Z?\\nbe the set of all infinite sequences (z1 , z2 , . . . ) of natural numbers such that zi ? Ni .\\nDefine the example space X := Z0 ? Z? and denote ?k := 2?k , where ?? := 0. The metric ? over\\nX is defined as follows: for x, y ? X , denote by x ? y their longest common prefix. Then,\\n?(x, y) = (?|x?y| ? ?|x| ) + (?|x?y| ? ?|y| ).\\nIt can be shown (see [35]) that ?(x, y) is a metric; in fact, it embeds isometrically into the square\\nnorm metric of a Hilbert space.\\nTo define ?, the marginal measure over X , let ?? be the uniform product distribution measure\\nover Z? , that is: for all i ? N, each zi in the sequence z = (z1 , z2 , . . . ) ? Z? is independently\\ndrawn from a uniform distribution over [Ni ]. Let ?0 be an atomic measure on Z0 such that for all\\nz ? Z0 , ?0 (z) = a|z| . Clearly, the first condition in Eq. (4) implies ?0 (Z0 ) = 1. Define the marginal\\nprobability measure ? over X by\\n?A ? Z0 ? Z? ,\\n\\n?(A) := ??? (A) + (1 ? ?)?0 (A).\\n\\nIn words, an infinite sequence is drawn with probability ? (and all such sequences are equally likely),\\nor else a finite sequence is drawn (and all finite sequences of the same length are equally likely).\\nDefine the realizable distribution ?\\n? over X ? Y by setting the marginal over X to ?, and by setting\\nthe label of z ? Z? to be 1 with probability 1 and the label of z ? Z0 to be 0 with probability 1.\\nAs shown in [35], this construction satisfies Eq. (3) with C = Z? and ?(C) = ? &gt; 0. It follows\\nfrom Theorem 3 that no k-NN algorithm is Bayes-consistent on it. In contrast, the following theorem\\nshows that KSU is weakly Bayes-consistent on this distribution. Theorem 4 immediately follows\\nfrom the this result.\\nTheorem 6. Assume (X , ?), Y and ?\\n? as in Example 1. KSU is weakly Bayes-consistent on ?\\n?.\\nThe proof, provided in Appendix C in [26], first characterizes the Voronoi cells for which the true\\nmajority-vote yields a significant error for the cell (Lemma 15). In finite-dimensional spaces, the total\\nmeasure of all such ?bad? cells can be made arbitrarily close to zero by taking ? to be sufficiently\\nsmall, as shown in Lemma 10 of Theorem 2. However, it is not immediately clear whether this can\\nbe achieved for the infinite dimensional construction above.\\nIndeed, we expect such bad cells, due to the unintuitive property that for any x ? C, we have\\n?? (x) ? C)/?(B\\n?? (x)) ? 0 when ? ? 0, and yet ?(C) &gt; 0. Thus, if for example a significant\\n?(B\\n8\\n\\n\f?? (x) with\\nportion of the set C (whose label is 1) is covered by Voronoi cells of the form V = B\\nx ? C, then for all sufficiently small ?, each one of these cells will have a true majority-vote 0. Thus\\na significant portion of C would be misclassified. However, we show that by the structure of the\\nconstruction, combined with the packing and covering properties of ?-nets, we have that in any ?-net,\\nthe total measure of all these ?bad? cells goes to 0 when ? ? 0, thus yielding a consistent classifier.\\nLastly, the following theorem shows that on the same construction above, when the Voronoi partitions\\nare allowed to violate the packing property of ?-nets, Bayes-consistency does not necessarily hold.\\nTheorem 5 immediately follows from the following result.\\nTheorem 7. Assume (X , ?), Y and ?\\n? as in Example 1. There exists a sequence of Voronoi partitions\\n(Pk )k?N of X with maxV ?Pk diam(V ) ? ?k such that the sequence of true majority-vote classifiers\\n(hPk )k?N induced by these partitions is not Bayes consistent: lim inf k?? err(hPk ) = ? &gt; 0.\\nThe proof, provided in Appendix D, constructs a sequence of Voronoi partitions, where each partition\\nPk has all of its impure Voronoi cells (those with both 0 and 1 labels) being bad. In this case, C is\\nincorrectly classified by hPk , yielding a significant error. Thus, in infinite-dimensional metric spaces,\\nthe shape of the Voronoi cells plays a fundamental role in the consistency of the partition.\\nAcknowledgments. We thank Fr?d?ric C?rou for the numerous fruitful discussions and helpful\\nfeedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science\\nFoundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel\\nScience Foundation (grant No. 555/15).\\n\\nReferences\\n[1] Christophe Abraham, G?rard Biau, and Beno?t Cadre. On the kernel rule for function classification. Ann. Inst. Statist. Math., 58(3):619?633, 2006.\\n[2] Daniel Berend and Aryeh Kontorovich. The missing mass problem. Statistics &amp; Probability\\nLetters, 82(6):1102?1110, 2012.\\n[3] Daniel Berend and Aryeh Kontorovich. On the concentration of the missing mass. Electronic\\nCommunications in Probability, 18(3):1?7, 2013.\\n[4] Alina Beygelzimer, Sham Kakade, and John Langford. Cover trees for nearest neighbor. In\\nICML ?06: Proceedings of the 23rd international conference on Machine learning, pages\\n97?104, New York, NY, USA, 2006. ACM.\\n[5] G?rard Biau, Florentina Bunea, and Marten H. Wegkamp. Functional classification in Hilbert\\nspaces. IEEE Trans. Inform. Theory, 51(6):2163?2172, 2005.\\n[6] G?rard Biau, Fr?d?ric C?rou, and Arnaud Guyader. Rates of convergence of the functional\\nk-nearest neighbor estimate. IEEE Trans. Inform. Theory, 56(4):2034?2040, 2010.\\n[7] V. I. Bogachev. Measure theory. Vol. I, II. Springer-Verlag, Berlin, 2007.\\n[8] Oren Boiman, Eli Shechtman, and Michal Irani. In defense of nearest-neighbor based image\\nclassification. In CVPR, 2008.\\n[9] Fr?d?ric C?rou and Arnaud Guyader. Nearest neighbor classification in infinite dimension.\\nESAIM: Probability and Statistics, 10:340?355, 2006.\\n[10] Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for nearest neighbor classification. In NIPS, 2014.\\n[11] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classification. IEEE Transactions\\non Information Theory, 13:21?27, 1967.\\n[12] Luc Devroye. On the inequality of Cover and Hart in nearest neighbor discrimination. IEEE\\nTrans. Pattern Anal. Mach. Intell., 3(1):75?78, 1981.\\n[13] Luc Devroye and L?szl? Gy?rfi. Nonparametric density estimation: the L1 view. Wiley Series\\nin Probability and Mathematical Statistics: Tracts on Probability and Statistics. John Wiley &amp;\\nSons, Inc., New York, 1985.\\n9\\n\\n\f[14] Luc Devroye, L?szl? Gy?rfi, and G?bor Lugosi. A probabilistic theory of pattern recognition,\\nvolume 31. Springer Science &amp; Business Media, 2013.\\n[15] Herbert Federer. Geometric measure theory. Die Grundlehren der mathematischen Wissenschaften, Band 153. Springer-Verlag New York Inc., New York, 1969.\\n[16] Evelyn Fix and Jr. Hodges, J. L. Discriminatory analysis. nonparametric discrimination:\\nConsistency properties. International Statistical Review / Revue Internationale de Statistique,\\n57(3):pp. 238?247, 1989.\\n[17] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the VapnikChervonenkis dimension. Machine learning, 21(3):269?304, 1995.\\n[18] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient classification for metric\\ndata (extended abstract COLT 2010). IEEE Transactions on Information Theory, 60(9):5750?\\n5759, 2014.\\n[19] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality\\nreduction. Theoretical Computer Science, 620:105?118, 2016.\\n[20] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample compression\\nfor nearest neighbors. In Neural Information Processing Systems (NIPS), 2014.\\n[21] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Nearly optimal classification for\\nsemimetrics (extended abstract AISTATS 2016). Journal of Machine Learning Research, 2017.\\n[22] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. PAC-Bayesian compression bounds on\\nthe prediction error of learning algorithms for classification. Machine Learning, 59(1):55?76,\\n2005.\\n[23] Peter Hall and Kee-Hoon Kang. Bandwidth choice for nonparametric classification. Ann.\\nStatist., 33(1):284?306, 02 2005.\\n[24] Olav Kallenberg. Foundations of modern probability. Second edition. Probability and its\\nApplications. Springer-Verlag, 2002.\\n[25] Aryeh Kontorovich, Sivan Sabato, and Ruth Urner. Active nearest-neighbor learning in metric\\nspaces. In Advances in Neural Information Processing Systems, pages 856?864, 2016.\\n[26] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compression:\\nEfficiency, consistency, infinite dimensions. CoRR, abs/1705.08184, 2017.\\n[27] Aryeh Kontorovich and Roi Weiss. A Bayes consistent 1-NN classifier. In Artificial Intelligence\\nand Statistics (AISTATS 2015), 2014.\\n[28] Aryeh Kontorovich and Roi Weiss. Maximum margin multiclass nearest neighbors. In International Conference on Machine Learning (ICML 2014), 2014.\\n[29] Robert Krauthgamer and James R. Lee. Navigating nets: Simple algorithms for proximity\\nsearch. In 15th Annual ACM-SIAM Symposium on Discrete Algorithms, pages 791?801, January\\n2004.\\n[30] Sanjeev R. Kulkarni and Steven E. Posner. Rates of convergence of nearest neighbor estimation\\nunder arbitrary sampling. IEEE Trans. Inform. Theory, 41(4):1028?1039, 1995.\\n[31] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. unpublished, 1986.\\n[32] James R. Munkres. Topology: a first course. Prentice-Hall, Inc., Englewood Cliffs, N.J., 1975.\\n[33] Vladimir Pestov. On the geometry of similarity search: dimensionality curse and concentration\\nof measure. Inform. Process. Lett., 73(1-2):47?51, 2000.\\n[34] Vladimir Pestov. Is the k-NN classifier in high dimensions affected by the curse of dimensionality? Comput. Math. Appl., 65(10):1427?1437, 2013.\\n10\\n\\n\f[35] David Preiss. Invalid Vitali theorems. Abstracta. 7th Winter School on Abstract Analysis, pages\\n58?60, 1979.\\n[36] David Preiss. Gaussian measures and the density theorem. Comment. Math. Univ. Carolin.,\\n22(1):181?193, 1981.\\n[37] Demetri Psaltis, Robert R. Snapp, and Santosh S. Venkatesh. On the finite sample performance\\nof the nearest neighbor classifier. IEEE Transactions on Information Theory, 40(3):820?837,\\n1994.\\n[38] Walter Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., New York, third\\nedition, 1976. International Series in Pure and Applied Mathematics.\\n[39] Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1987.\\n[40] Richard J. Samworth. Optimal weighted nearest neighbour classifiers. Ann. Statist., 40(5):2733?\\n2763, 10 2012.\\n[41] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to\\nAlgorithms. Cambridge University Press, 2014.\\n[42] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural\\nrisk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory,\\n44(5):1926?1940, 1998.\\n[43] Robert R. Snapp and Santosh S. Venkatesh. Asymptotic expansions of the k nearest neighbor\\nrisk. Ann. Statist., 26(3):850?878, 1998.\\n[44] Charles J. Stone. Consistent nonparametric regression. The Annals of Statistics, 5(4):595?620,\\n1977.\\n[45] Jaroslav Ti?er. Vitali covering theorem in Hilbert space. Trans. Amer. Math. Soc., 355(8):3277?\\n3289, 2003.\\n[46] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest\\nneighbor classification. Journal of Machine Learning Research, 10:207?244, 2009.\\n[47] Lin Cheng Zhao. Exponential bounds of mean error for the nearest neighbor estimates of\\nregression functions. J. Multivariate Anal., 21(1):168?178, 1987.\\n\\n11\\n\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df2.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac4e9f8",
      "metadata": {
        "id": "4ac4e9f8"
      },
      "source": [
        "# Preprocess Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a23137",
      "metadata": {
        "id": "24a23137"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    if isinstance(text, str):\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace newline and multiple whitespaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove digits\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Keep tokens with more than 2 characters\n",
        "        tokens = [word for word in tokens if len(word) > 2]\n",
        "\n",
        "        # Lemmatize using spaCy\n",
        "        lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(tokens)).doc]\n",
        "\n",
        "        return lemmatized_tokens\n",
        "\n",
        "    else:\n",
        "        return []  # Return an empty list if the text is not a valid string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a0a836",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27a0a836",
        "outputId": "82160c54-40cf-433f-9d48-1e8a765a473e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9min 22s, sys: 41.2 s, total: 10min 3s\n",
            "Wall time: 10min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "processed_text = df2.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e577f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "82e577f2",
        "outputId": "7c68ae85-a260-423c-bdca-b0ed9924ed09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                               [independent, component, analysis, identification, artifact, magnetoencephalographic, recording, independent, component, analysis, identification, artifact, magnetoencephalographic, recording, ricardo, vigario, veikko, ousmiiki, matti, hiimiiliiinen, riitta, hari, erkki, oja, lab, computer, info, science, helsinki, university, technology, box, fin, hut, finland, ricardovigario, erkkiojahutfi, brain, research, unit, low, temperature, lab, helsinki, university, technology, box, fin, hut, finland, veikko, msh, harineurohutfi, abstract, study, application, independent, component, analysis, ica, approach, identification, possible, removal, artifact, magnetoencephalographic, meg, recording, statistical, technique, separate, component, accord, kurtosis, amplitude, distribution, time, thus, distinguish, strictly, periodical, signal, regularly, irregularly, occur, signal, many, artifact, belong, last, category, order, assess, effectiveness, method, control, artifact, produce, include, saccadic, ...]\n",
              "1                                                                                                  [nearmaximum, entropy, model, binary, neural, representation, natural, image, nearmaximum, entropy, model, binary, neural, representation, natural, image, matthias, bethge, philipp, berens, max, planck, institute, biological, cybernetics, spemannstrasse, tubingen, germany, mbethgeberenstuebingenmpgde, abstract, maximum, entropy, analysis, binary, variable, provide, elegant, way, study, role, pairwise, correlation, neural, population, unfortunately, approach, suffer, poor, scalability, high, dimension, sensory, coding, however, highdimensional, datum, ubiquitous, introduce, new, approach, use, nearmaximum, entropy, model, make, type, analysis, feasible, highdimensional, datathe, model, parameter, derive, closed, form, sampling, easy, therefore, nearmaxent, approach, serve, tool, testing, prediction, pairwise, maximum, entropy, model, lowdimensional, marginal, also, high, dimensional, measurement, thousand, unit, demonstrate, usefulness, study, natural, ...]\n",
              "2    [nearestneighbor, sample, compression, efficiency, consistency, infinite, dimension, nearestneighbor, sample, compression, efficiency, consistency, infinite, dimensions, aryeh, kontorovich, department, computer, science, bengurion, university, negev, karyehcsbguacil, sivan, sabato, department, computer, science, bengurion, university, negev, sabatosbguacil, roi, weiss, department, computer, science, applied, mathematics, weizmann, institute, science, roiwweizmannacil, abstract, examine, bayesconsistency, recently, propose, nearestneighborbase, multiclass, learning, algorithm, algorithm, derive, sample, compression, bound, enjoy, statistical, advantage, tight, fully, empirical, generalization, bound, well, algorithmic, advantage, fast, runtime, memory, saving, prove, algorithm, strongly, bayesconsistent, metric, space, finite, double, dimension, first, consistency, result, efficient, nearestneighbor, sample, compression, scheme, rather, surprisingly, discover, algorithm, continue, bayesconsistent, even, certain, infinitedimensional, set, basic, ...]\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[independent, component, analysis, identification, artifact, magnetoencephalographic, recording, independent, component, analysis, identification, artifact, magnetoencephalographic, recording, ricardo, vigario, veikko, ousmiiki, matti, hiimiiliiinen, riitta, hari, erkki, oja, lab, computer, info, science, helsinki, university, technology, box, fin, hut, finland, ricardovigario, erkkiojahutfi, brain, research, unit, low, temperature, lab, helsinki, university, technology, box, fin, hut, finland, veikko, msh, harineurohutfi, abstract, study, application, independent, component, analysis, ica, approach, identification, possible, removal, artifact, magnetoencephalographic, meg, recording, statistical, technique, separate, component, accord, kurtosis, amplitude, distribution, time, thus, distinguish, strictly, periodical, signal, regularly, irregularly, occur, signal, many, artifact, belong, last, category, order, assess, effectiveness, method, control, artifact, produce, include, saccadic, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[nearmaximum, entropy, model, binary, neural, representation, natural, image, nearmaximum, entropy, model, binary, neural, representation, natural, image, matthias, bethge, philipp, berens, max, planck, institute, biological, cybernetics, spemannstrasse, tubingen, germany, mbethgeberenstuebingenmpgde, abstract, maximum, entropy, analysis, binary, variable, provide, elegant, way, study, role, pairwise, correlation, neural, population, unfortunately, approach, suffer, poor, scalability, high, dimension, sensory, coding, however, highdimensional, datum, ubiquitous, introduce, new, approach, use, nearmaximum, entropy, model, make, type, analysis, feasible, highdimensional, datathe, model, parameter, derive, closed, form, sampling, easy, therefore, nearmaxent, approach, serve, tool, testing, prediction, pairwise, maximum, entropy, model, lowdimensional, marginal, also, high, dimensional, measurement, thousand, unit, demonstrate, usefulness, study, natural, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[nearestneighbor, sample, compression, efficiency, consistency, infinite, dimension, nearestneighbor, sample, compression, efficiency, consistency, infinite, dimensions, aryeh, kontorovich, department, computer, science, bengurion, university, negev, karyehcsbguacil, sivan, sabato, department, computer, science, bengurion, university, negev, sabatosbguacil, roi, weiss, department, computer, science, applied, mathematics, weizmann, institute, science, roiwweizmannacil, abstract, examine, bayesconsistency, recently, propose, nearestneighborbase, multiclass, learning, algorithm, algorithm, derive, sample, compression, bound, enjoy, statistical, advantage, tight, fully, empirical, generalization, bound, well, algorithmic, advantage, fast, runtime, memory, saving, prove, algorithm, strongly, bayesconsistent, metric, space, finite, double, dimension, first, consistency, result, efficient, nearestneighbor, sample, compression, scheme, rather, surprisingly, discover, algorithm, continue, bayesconsistent, even, certain, infinitedimensional, set, basic, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "processed_text.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Eag4Q_5GkFKr",
      "metadata": {
        "id": "Eag4Q_5GkFKr"
      },
      "source": [
        "# Generate Gensim Dictionary Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e0b6d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03e0b6d3",
        "outputId": "9184dbbb-5224-43a6-c373-f29c5672aa7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.85 s, sys: 35.3 ms, total: 2.89 s\n",
            "Wall time: 2.89 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Map each word in to its unique integer id\n",
        "dictionary = Dictionary(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636327d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "636327d6",
        "outputId": "86148221-e948-4e03-d1fb-e05b26c40d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98496 words in the Dictionary object.\n"
          ]
        }
      ],
      "source": [
        "print(\"{} words in the Dictionary object.\".format(len(dictionary)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IGR6AF6IouGp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGR6AF6IouGp",
        "outputId": "1be0a81c-89b3-4e49-f981-46c349467e48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14214 words remaining in the Dictionary object.\n"
          ]
        }
      ],
      "source": [
        "# Remove very rare and very common words\n",
        "#  Filter out tokens that appear in:\n",
        "#   < 5 documents (remove words that appear in fewer than 5 documents)\n",
        "#   > 40% documents (remove words that appear in more than 40% of documents)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.4)\n",
        "print(\"{} words remaining in the Dictionary object.\".format(len(dictionary)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I00GQvDGq-kZ",
      "metadata": {
        "id": "I00GQvDGq-kZ"
      },
      "outputs": [],
      "source": [
        "# Convert documents into the bag-of-words format: list of (token_id, token_count) 2-tuples\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LDA Model (BOW Corpus)"
      ],
      "metadata": {
        "id": "Ke_S33GjhG24"
      },
      "id": "Ke_S33GjhG24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X2Ixcf1-q_B6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2Ixcf1-q_B6",
        "outputId": "0bc35e36-48b1-49af-fb57-d2d5f1d91e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23.1 s, sys: 1.5 s, total: 24.6 s\n",
            "Wall time: 39.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Train model with Bag of Words corpus\n",
        "lda_model_bow = LdaMulticore(corpus=bow_corpus, id2word=dictionary, num_topics=10,\n",
        "                             chunksize=10, passes=10, gamma_threshold=0.001,\n",
        "                             per_word_topics=True, workers=multiprocessing.cpu_count(), random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate LDA Model (BOW Corpus)"
      ],
      "metadata": {
        "id": "E_fFq7oyiflh"
      },
      "id": "E_fFq7oyiflh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PFZujTElq_TX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFZujTElq_TX",
        "outputId": "c0ecea94-da35-4309-c13a-710f17f8c3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA BOW Coherence Score (c_v):  0.565995761119147\n",
            "CPU times: user 1min 5s, sys: 368 ms, total: 1min 5s\n",
            "Wall time: 1min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Compute Coherence Score: c_v\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_bow, texts=processed_text,\n",
        "                                     dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print('LDA BOW Coherence Score (c_v): ', coherence_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQ86YB6Hq_3t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ86YB6Hq_3t",
        "outputId": "2146b616-3638-4660-92f8-1460657f989c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA BOW Coherence Score (u_mass):  -1.451321881876052\n",
            "CPU times: user 1.65 s, sys: 90.8 ms, total: 1.74 s\n",
            "Wall time: 1.74 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Compute Coherence Score: u_mass\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_bow, texts=processed_text,\n",
        "                                     dictionary=dictionary, coherence='u_mass')\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print('LDA BOW Coherence Score (u_mass): ', coherence_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9JWRwrXZrAHZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JWRwrXZrAHZ",
        "outputId": "0e80fdc2-ebfe-4bc0-aa47-81c1098af2d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.034*\"kernel\" + 0.028*\"label\" + 0.017*\"classifier\" + 0.011*\"risk\" + 0.011*\"hypothesis\" + 0.010*\"regression\" + 0.008*\"generalization\" + 0.008*\"svm\" + 0.008*\"margin\" + 0.007*\"unlabeled\"')\n",
            "(1, '0.024*\"object\" + 0.010*\"human\" + 0.008*\"detection\" + 0.007*\"face\" + 0.007*\"visual\" + 0.007*\"pixel\" + 0.007*\"pose\" + 0.006*\"shape\" + 0.006*\"similarity\" + 0.006*\"vision\"')\n",
            "(2, '0.013*\"bayesian\" + 0.012*\"topic\" + 0.011*\"likelihood\" + 0.011*\"posterior\" + 0.011*\"inference\" + 0.010*\"mixture\" + 0.010*\"density\" + 0.008*\"covariance\" + 0.007*\"variational\" + 0.007*\"latent\"')\n",
            "(3, '0.024*\"neuron\" + 0.018*\"cell\" + 0.016*\"spike\" + 0.014*\"response\" + 0.013*\"stimulus\" + 0.010*\"activity\" + 0.009*\"subject\" + 0.009*\"filter\" + 0.008*\"field\" + 0.008*\"frequency\"')\n",
            "(4, '0.013*\"convex\" + 0.012*\"sparse\" + 0.011*\"norm\" + 0.009*\"rank\" + 0.007*\"objective\" + 0.006*\"min\" + 0.006*\"lasso\" + 0.005*\"regression\" + 0.005*\"projection\" + 0.005*\"sparsity\"')\n",
            "(5, '0.043*\"layer\" + 0.035*\"deep\" + 0.021*\"arxiv\" + 0.016*\"batch\" + 0.014*\"convolutional\" + 0.011*\"architecture\" + 0.010*\"preprint\" + 0.010*\"generative\" + 0.008*\"objective\" + 0.007*\"adversarial\"')\n",
            "(6, '0.043*\"graph\" + 0.039*\"cluster\" + 0.030*\"node\" + 0.025*\"tree\" + 0.016*\"edge\" + 0.013*\"belief\" + 0.012*\"vertex\" + 0.011*\"partition\" + 0.011*\"inference\" + 0.010*\"message\"')\n",
            "(7, '0.026*\"policy\" + 0.024*\"action\" + 0.018*\"regret\" + 0.015*\"online\" + 0.015*\"reward\" + 0.010*\"game\" + 0.009*\"round\" + 0.009*\"agent\" + 0.009*\"arm\" + 0.009*\"strategy\"')\n",
            "(8, '0.016*\"memory\" + 0.011*\"hide\" + 0.009*\"dynamic\" + 0.009*\"net\" + 0.008*\"circuit\" + 0.008*\"layer\" + 0.008*\"architecture\" + 0.005*\"trajectory\" + 0.005*\"parallel\" + 0.005*\"rule\"')\n",
            "(9, '0.057*\"word\" + 0.022*\"code\" + 0.020*\"language\" + 0.017*\"speech\" + 0.016*\"sentence\" + 0.012*\"neighbor\" + 0.011*\"text\" + 0.010*\"decoder\" + 0.010*\"context\" + 0.009*\"embed\"')\n"
          ]
        }
      ],
      "source": [
        "# Display topics identified by LDA model\n",
        "all_topics = lda_model_bow.print_topics()\n",
        "for topic in all_topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display topics using 'show_topics' which provides more options\n",
        "#   Returns the top 5 topics with the top 5 words per topic\n",
        "top_topics = lda_model_bow.show_topics(num_topics=5, num_words=5, formatted=False)\n",
        "for topic in top_topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV908deaizV9",
        "outputId": "0ec00f9e-2ffc-4608-dd1d-724b145315c3"
      },
      "id": "sV908deaizV9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, [('policy', 0.026056727), ('action', 0.023655426), ('regret', 0.01807593), ('online', 0.015356237), ('reward', 0.015254039)])\n",
            "(5, [('layer', 0.04332829), ('deep', 0.035299692), ('arxiv', 0.021327922), ('batch', 0.015569905), ('convolutional', 0.0142770335)])\n",
            "(2, [('bayesian', 0.012753932), ('topic', 0.011678577), ('likelihood', 0.011361216), ('posterior', 0.011206238), ('inference', 0.010802712)])\n",
            "(6, [('graph', 0.04257518), ('cluster', 0.03853856), ('node', 0.029979745), ('tree', 0.024852268), ('edge', 0.016040184)])\n",
            "(8, [('memory', 0.015948422), ('hide', 0.010934705), ('dynamic', 0.008899505), ('net', 0.008893962), ('circuit', 0.007909868)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA Model (BOW Corpus) Visualization"
      ],
      "metadata": {
        "id": "4SmAq3extFJZ"
      },
      "id": "4SmAq3extFJZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Topic Distance Visualization\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "u5MiJRLbizkU",
        "outputId": "5f9312e1-5465-4de8-dfd8-f92207980f58"
      },
      "id": "u5MiJRLbizkU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "2      0.080539 -0.021906       1        1  16.373917\n",
              "4      0.110350 -0.125521       2        1  15.275599\n",
              "1     -0.040252  0.105068       3        1  12.894464\n",
              "3     -0.009363  0.269203       4        1  10.421259\n",
              "0      0.119324 -0.078182       5        1  10.324308\n",
              "7      0.120301 -0.078713       6        1   9.144024\n",
              "8     -0.026961  0.112059       7        1   8.802756\n",
              "6      0.137264  0.000568       8        1   7.988753\n",
              "5     -0.242689 -0.217349       9        1   4.989102\n",
              "9     -0.248514  0.034772      10        1   3.785817, topic_info=                Term         Freq        Total Category  logprob  loglift\n",
              "2723           graph  5452.000000  5452.000000  Default  30.0000  30.0000\n",
              "1009          kernel  6609.000000  6609.000000  Default  29.0000  29.0000\n",
              "2429         cluster  5341.000000  5341.000000  Default  28.0000  28.0000\n",
              "1262            word  4019.000000  4019.000000  Default  27.0000  27.0000\n",
              "1681           layer  4987.000000  4987.000000  Default  26.0000  26.0000\n",
              "...              ...          ...          ...      ...      ...      ...\n",
              "86    dimensionality   299.117322  1019.192508  Topic10  -5.2892   2.0480\n",
              "294       similarity   324.338812  1950.237822  Topic10  -5.2082   1.4800\n",
              "2268           digit   269.562229   593.993046  Topic10  -5.3932   2.4838\n",
              "676        reduction   289.366997  1125.012775  Topic10  -5.3223   1.9161\n",
              "299           source   298.600593  2417.557885  Topic10  -5.2909   1.1825\n",
              "\n",
              "[656 rows x 6 columns], token_table=      Topic      Freq      Term\n",
              "term                           \n",
              "1273     10  0.990563       acl\n",
              "3704      7  0.016337  acoustic\n",
              "3704     10  0.980202  acoustic\n",
              "2948      3  0.003713    action\n",
              "2948      4  0.028564    action\n",
              "...     ...       ...       ...\n",
              "9492      6  0.855082    worker\n",
              "8555      7  0.994356       wta\n",
              "2222      1  0.995891       xed\n",
              "8997      7  0.988351       xtl\n",
              "2224      9  0.993921    yoshua\n",
              "\n",
              "[1545 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 5, 2, 4, 1, 8, 9, 7, 6, 10])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el70471342640969149286059718778\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el70471342640969149286059718778_data = {\"mdsDat\": {\"x\": [0.08053936428029364, 0.11034988183618274, -0.0402518402380451, -0.009363102425359114, 0.11932434310719471, 0.1203005294975235, -0.026960521039565605, 0.13726384163303168, -0.2426886203052013, -0.2485138763460553], \"y\": [-0.021905592842357133, -0.1255209961805684, 0.1050679685705689, 0.2692026113712684, -0.07818169233614712, -0.07871290346347228, 0.1120589965997006, 0.000567924772473741, -0.21734862112363748, 0.0347723046321706], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.373917113159674, 15.275599477700105, 12.894463734087253, 10.421258968019414, 10.324308196994735, 9.14402403726298, 8.802755526445043, 7.988753315250039, 4.989102338425046, 3.7858172926557]}, \"tinfo\": {\"Term\": [\"graph\", \"kernel\", \"cluster\", \"word\", \"layer\", \"object\", \"node\", \"label\", \"policy\", \"deep\", \"neuron\", \"action\", \"tree\", \"regret\", \"cell\", \"spike\", \"classifier\", \"bayesian\", \"arxiv\", \"inference\", \"code\", \"reward\", \"topic\", \"convex\", \"posterior\", \"likelihood\", \"edge\", \"norm\", \"stimulus\", \"mixture\", \"dirichlet\", \"sampler\", \"lda\", \"mcmc\", \"dene\", \"riemannian\", \"multinomial\", \"classier\", \"ard\", \"quadrature\", \"blei\", \"boolean\", \"laplace\", \"regularisation\", \"bishop\", \"gamma\", \"registration\", \"geodesic\", \"condence\", \"logconcave\", \"sin\", \"elbo\", \"neal\", \"representer\", \"integrand\", \"rasmussen\", \"resample\", \"stein\", \"optimisation\", \"xed\", \"posterior\", \"bayesian\", \"likelihood\", \"mixture\", \"density\", \"topic\", \"variational\", \"carlo\", \"monte\", \"ghahramani\", \"document\", \"nonparametric\", \"gibbs\", \"loglikelihood\", \"covariance\", \"divergence\", \"mix\", \"latent\", \"sampling\", \"integral\", \"hyperparameter\", \"inference\", \"estimator\", \"probabilistic\", \"conditional\", \"exp\", \"manifold\", \"uncertainty\", \"expectation\", \"entropy\", \"predictive\", \"joint\", \"exponential\", \"discrete\", \"family\", \"continuous\", \"marginal\", \"kernel\", \"regression\", \"lasso\", \"recovery\", \"proximal\", \"lowrank\", \"subgradient\", \"pursuit\", \"music\", \"subgaussian\", \"sdp\", \"nonsmooth\", \"glm\", \"primal\", \"frankwolfe\", \"haar\", \"bci\", \"spam\", \"nuclear\", \"aij\", \"svd\", \"subexponential\", \"admm\", \"psd\", \"duality\", \"nesterov\", \"mallow\", \"frobenius\", \"rnp\", \"geometrically\", \"orthonormal\", \"boyd\", \"norm\", \"sparsity\", \"semidefinite\", \"nonconvex\", \"eigenvalue\", \"dual\", \"subspace\", \"convex\", \"singular\", \"sparse\", \"siam\", \"penalty\", \"solver\", \"entry\", \"permutation\", \"outlier\", \"projection\", \"rank\", \"pca\", \"dictionary\", \"operator\", \"min\", \"highdimensional\", \"decomposition\", \"eigenvector\", \"factorization\", \"tensor\", \"spectral\", \"column\", \"objective\", \"coefficient\", \"principal\", \"descent\", \"lemma\", \"regression\", \"regularization\", \"recover\", \"group\", \"row\", \"estimator\", \"programming\", \"smooth\", \"basis\", \"object\", \"pixel\", \"cnn\", \"scene\", \"video\", \"segmentation\", \"texture\", \"fidelity\", \"participant\", \"annotation\", \"groundtruth\", \"saliency\", \"illumination\", \"appearance\", \"camera\", \"bag\", \"iccv\", \"clutter\", \"warp\", \"crf\", \"autoencoder\", \"synthesize\", \"rigid\", \"rbm\", \"eccv\", \"rcnn\", \"occlusion\", \"judgment\", \"foreground\", \"pretraine\", \"person\", \"pose\", \"contour\", \"face\", \"flow\", \"patch\", \"motion\", \"detection\", \"category\", \"cvpr\", \"human\", \"signature\", \"frame\", \"vision\", \"shape\", \"segment\", \"invariance\", \"detector\", \"ground\", \"fragment\", \"similarity\", \"extract\", \"visual\", \"rotation\", \"color\", \"invariant\", \"location\", \"question\", \"region\", \"boundary\", \"group\", \"transformation\", \"score\", \"label\", \"target\", \"fig\", \"spike\", \"stimulus\", \"cortex\", \"cortical\", \"fire\", \"eeg\", \"neuronal\", \"amplitude\", \"auditory\", \"recording\", \"pathway\", \"sensory\", \"inhibitory\", \"firing\", \"auto\", \"excitatory\", \"rod\", \"lateral\", \"disparity\", \"inhibition\", \"rat\", \"familiarity\", \"deg\", \"membrane\", \"fmri\", \"synchronization\", \"hippocampus\", \"modulation\", \"assembly\", \"hippocampal\", \"cell\", \"activity\", \"neuron\", \"neuroscience\", \"movement\", \"response\", \"receptive\", \"brain\", \"synaptic\", \"eye\", \"cue\", \"frequency\", \"filter\", \"subject\", \"temporal\", \"trial\", \"field\", \"channel\", \"visual\", \"correlation\", \"orientation\", \"source\", \"fig\", \"spatial\", \"dynamic\", \"population\", \"phase\", \"event\", \"hyperplane\", \"svms\", \"auc\", \"adaboost\", \"multiclass\", \"mtl\", \"pac\", \"rkhs\", \"cancer\", \"excess\", \"disagreement\", \"agnostic\", \"multilabel\", \"vcdimension\", \"rademacher\", \"pacbayesian\", \"transductive\", \"vapnik\", \"erm\", \"magic\", \"misclassification\", \"labeler\", \"diagnosis\", \"classifi\", \"tsybakov\", \"breast\", \"voting\", \"clinician\", \"realizable\", \"monitoring\", \"risk\", \"boost\", \"margin\", \"unlabeled\", \"classifier\", \"kernelbase\", \"svm\", \"hypothesis\", \"kernel\", \"fuzzy\", \"label\", \"predictor\", \"knn\", \"query\", \"semisupervise\", \"generalization\", \"patient\", \"weak\", \"ensemble\", \"active\", \"metric\", \"regression\", \"logistic\", \"learner\", \"decision\", \"regularization\", \"selection\", \"target\", \"rule\", \"negative\", \"domain\", \"rank\", \"policy\", \"regret\", \"reward\", \"agent\", \"arm\", \"reinforcement\", \"bandit\", \"player\", \"mdp\", \"privacy\", \"planning\", \"adversary\", \"episode\", \"sutton\", \"mdps\", \"private\", \"actorcritic\", \"dqn\", \"thread\", \"auction\", \"horizon\", \"opponent\", \"cesabianchi\", \"ucb\", \"stock\", \"multiarme\", \"hedge\", \"ftl\", \"jacobian\", \"barto\", \"game\", \"action\", \"discount\", \"round\", \"online\", \"exploration\", \"utility\", \"cumulative\", \"worker\", \"strategy\", \"expert\", \"outcome\", \"transition\", \"play\", \"decision\", \"environment\", \"search\", \"return\", \"user\", \"setting\", \"dynamic\", \"markov\", \"learner\", \"analog\", \"chip\", \"rumelhart\", \"vlsi\", \"device\", \"command\", \"clock\", \"pid\", \"touretzky\", \"pulse\", \"adjustable\", \"receiver\", \"mcclelland\", \"mateo\", \"wta\", \"hopfield\", \"ssp\", \"syllable\", \"cmos\", \"atkeson\", \"primitive\", \"xtl\", \"climate\", \"vsi\", \"planar\", \"transistor\", \"orbit\", \"clause\", \"teacher\", \"wire\", \"gate\", \"mozer\", \"hardware\", \"prime\", \"memory\", \"processor\", \"connectionist\", \"circuit\", \"asynchronous\", \"net\", \"delay\", \"trajectory\", \"force\", \"digital\", \"hide\", \"module\", \"feedforward\", \"morgan\", \"parallel\", \"backpropagation\", \"architecture\", \"store\", \"activation\", \"dynamic\", \"threshold\", \"position\", \"simulation\", \"recurrent\", \"operation\", \"layer\", \"connection\", \"implementation\", \"stable\", \"rule\", \"vol\", \"distribute\", \"fig\", \"nonlinear\", \"target\", \"continuous\", \"message\", \"bethe\", \"clique\", \"protein\", \"kikuchi\", \"parent\", \"vertice\", \"hypergraph\", \"leaf\", \"subtree\", \"loopy\", \"undirected\", \"suffix\", \"taxonomy\", \"motif\", \"bob\", \"alice\", \"friend\", \"mmsb\", \"lbp\", \"adjacency\", \"ancestor\", \"identifiability\", \"jit\", \"mixedmembership\", \"lift\", \"dag\", \"snps\", \"ise\", \"subgraph\", \"vertex\", \"graph\", \"tree\", \"causal\", \"child\", \"node\", \"cluster\", \"affinity\", \"edge\", \"submodular\", \"clustering\", \"belief\", \"partition\", \"community\", \"site\", \"graphical\", \"propagation\", \"assignment\", \"marginal\", \"pairwise\", \"hierarchical\", \"inference\", \"link\", \"cover\", \"connect\", \"potential\", \"energy\", \"markov\", \"path\", \"region\", \"sgd\", \"dropout\", \"gpu\", \"discriminator\", \"cifar\", \"relu\", \"lstm\", \"imagenet\", \"minibatch\", \"deep\", \"gan\", \"hti\", \"dnn\", \"sutskever\", \"iclr\", \"vgg\", \"yoshua\", \"endtoend\", \"krizhevsky\", \"convolutional\", \"goodfellow\", \"coadaptation\", \"courville\", \"kingma\", \"gin\", \"mixability\", \"wgan\", \"alexnet\", \"ilya\", \"confusion\", \"mnist\", \"batch\", \"salakhutdinov\", \"generator\", \"arxiv\", \"adversarial\", \"layer\", \"bengio\", \"preprint\", \"rnn\", \"lecun\", \"generative\", \"architecture\", \"recurrent\", \"convolution\", \"objective\", \"net\", \"attention\", \"icml\", \"normalization\", \"nips\", \"activation\", \"hinton\", \"validation\", \"hyperparameter\", \"sentence\", \"decoder\", \"hash\", \"lsh\", \"grammar\", \"phoneme\", \"token\", \"english\", \"contextdependent\", \"phone\", \"apple\", \"mds\", \"pronunciation\", \"speaker\", \"professor\", \"acl\", \"quantize\", \"phonetic\", \"bleu\", \"constituent\", \"syntactic\", \"bigram\", \"vowel\", \"utterance\", \"kohonen\", \"quantizer\", \"preprocessor\", \"linguistic\", \"tagging\", \"qij\", \"speech\", \"acoustic\", \"language\", \"quantization\", \"word\", \"string\", \"parse\", \"ham\", \"vocabulary\", \"decode\", \"character\", \"corpus\", \"code\", \"alignment\", \"text\", \"neighbor\", \"symbol\", \"compression\", \"distortion\", \"embed\", \"near\", \"hmm\", \"length\", \"context\", \"bit\", \"dimensionality\", \"similarity\", \"digit\", \"reduction\", \"source\"], \"Freq\": [5452.0, 6609.0, 5341.0, 4019.0, 4987.0, 4850.0, 4051.0, 5749.0, 3731.0, 2767.0, 4223.0, 3500.0, 3266.0, 2589.0, 3000.0, 2534.0, 2864.0, 3305.0, 2200.0, 4300.0, 2380.0, 2185.0, 3061.0, 3337.0, 2896.0, 2942.0, 2350.0, 2760.0, 2065.0, 2714.0, 691.2770768475895, 538.9047560856529, 501.07053127567207, 456.1165224306686, 371.9762712293416, 366.27655781273415, 354.7556204284637, 334.3653679479772, 247.93428020801446, 246.15976650588914, 247.04569512813285, 245.603925131035, 225.87280247097559, 218.85097904233083, 217.61114459747225, 199.54191974439385, 184.37062154372995, 183.0957905080798, 182.15844898379896, 179.48339614242144, 178.61557023430575, 175.45901206017768, 158.24604707774654, 155.9678361748506, 152.58874743982977, 148.43519683592237, 146.40912554983836, 141.26153745217584, 139.82228959744208, 136.66771630458277, 2873.178257320222, 3269.993093964331, 2912.9133715810804, 2682.9792334036392, 2662.4799766164824, 2994.2819226395513, 1845.6462354046184, 742.5279051209579, 720.4465934951506, 320.867548515682, 1612.8141108790855, 896.2137695652455, 422.05461670571617, 557.517450384474, 2011.6879708231238, 779.886594165914, 486.4477468054791, 1758.641218499593, 1259.3167260879948, 509.4568784950448, 1063.5756748862773, 2769.718007588101, 1618.2176441929853, 1259.321859916239, 1239.4194405102921, 1182.1183956513048, 936.1823536198535, 938.5250507781058, 860.7491646639595, 804.2407590075834, 680.0077268951163, 970.6387582279552, 922.1926716548038, 886.6054518324331, 894.1418519990285, 952.0781185597017, 866.5944459895569, 1115.3473479437084, 913.564198768479, 1341.121281398624, 972.6224008563003, 666.56021215535, 535.8066462722715, 410.8183711193414, 354.1517037817304, 351.00730858153224, 322.3839059731006, 298.0156367747882, 263.9722026070273, 239.17697487446438, 225.22371769286906, 221.75177320431314, 220.99733499602274, 208.5809252639212, 203.67429929875124, 194.4252992603298, 194.2995618794253, 192.95906857486517, 187.6172835088017, 180.76722070929097, 159.83890125904963, 152.94835077227611, 143.38026654654774, 140.25920606569358, 135.97541940234643, 130.6882258843426, 127.24909758522215, 126.98343542543495, 125.58796073263521, 2716.444466998167, 1212.9052966298902, 437.0916778942674, 600.7444413254353, 1007.7759606279851, 915.6999426691734, 978.3145130155226, 3032.820044734634, 339.1707304256845, 2906.8292677131794, 455.71392560561276, 1104.1789818186664, 333.4108139208741, 1078.3840367902928, 669.987297177482, 471.2162545109167, 1234.5134695082752, 2044.506353868652, 787.8285766195349, 752.0664763269141, 1087.486582226817, 1534.7540077273438, 659.3081266702434, 774.2867539793091, 512.6036929090761, 578.1474109848248, 750.3844273332444, 717.8182925274866, 1015.2396179479078, 1608.945383441044, 1020.7108577994491, 699.3293980034188, 946.3193269990606, 960.2900852201641, 1261.8515133518856, 942.619855074037, 793.9278490010107, 969.6950352534378, 854.4510381647198, 902.8542632074688, 773.5187688247647, 769.940314429895, 771.3164502651955, 4849.8678375030195, 1434.4460265702462, 1168.246663818632, 1119.3169643723672, 962.597209450711, 837.5277438262214, 356.08232574518104, 311.66853554444674, 335.39743807473724, 277.06806406689145, 269.0326771759247, 267.341855541626, 254.91339747698404, 252.62970468187248, 247.62514624464626, 247.0608103726264, 232.73691807982794, 226.3941862272174, 224.1759553590322, 221.38998036769456, 210.70887389579215, 201.98431512063104, 191.59185226189055, 178.92987023171412, 178.00374262567246, 174.71240625949923, 173.02174916146214, 172.9325234973284, 169.57379777945036, 166.6588381116285, 303.9150227747371, 1341.1943011468452, 541.1746666023608, 1469.9762689567779, 941.3569883922736, 786.2244442704847, 1132.1278517192684, 1534.3141781647967, 1004.4517556285572, 622.4514429083794, 2016.5285917220751, 419.5895618840513, 961.5043125909737, 1199.7515030605668, 1294.04855085893, 920.3255873549542, 910.0744141345456, 553.927026050502, 602.9052871991115, 415.00981256902986, 1221.3856709395739, 722.773714584875, 1463.1028841554098, 592.0147730765855, 547.7799043077232, 711.3548980316139, 933.8958800645618, 839.1049414193128, 1101.9529355744592, 746.7741019467633, 909.7690348330636, 737.9948231120386, 774.7741307757221, 1058.2265308642222, 855.8322618952885, 745.5019075722703, 2533.6481193739282, 2065.0824317641745, 1003.6060474484857, 700.092855364197, 692.1439856472869, 639.6677526210358, 512.8121262159801, 484.67778695363234, 428.2554243034163, 421.05822003503476, 406.29523555224586, 404.7755674057515, 404.71067433124244, 380.68427944949536, 336.8990072006405, 329.6240300262168, 319.3312209382962, 311.24031213529014, 301.50190570734367, 281.4565945646366, 268.3960844508246, 249.27852011133703, 241.94138540184497, 237.05367930304396, 232.6199967641861, 231.22407378377835, 219.88340111567086, 217.12738312280732, 213.36027935779245, 210.11028753107104, 2912.2671425329304, 1652.8978837450102, 3922.2963885625036, 515.8727009332373, 678.1690264914254, 2300.286088539661, 607.4570796560682, 1041.373664838331, 537.074423380881, 546.8627534036194, 534.3520297269755, 1251.7235020384767, 1457.501340986796, 1460.5639293650306, 987.771909306661, 845.2803953820218, 1374.0629807886196, 681.6802433379283, 1108.321129467304, 912.593282487943, 635.5478022617788, 964.4009800697792, 1050.1356736539778, 731.8644742489382, 831.1791454886497, 605.0927757136662, 620.8508886552025, 590.7993161809842, 639.6555241937388, 576.1351869054283, 510.9747556970094, 471.49163911750486, 441.7833663702085, 304.21429231268473, 301.0404755145862, 423.0304452378383, 242.1627298368387, 225.84904048558778, 217.04862465704616, 214.75735627527845, 210.01546996934803, 203.20249129398954, 200.32213586527402, 192.6134535438535, 178.41673989548508, 173.5928921797129, 168.59821467814643, 168.68096653097635, 153.85061872725703, 151.54321213448145, 142.51991961339073, 108.3864270772703, 102.74431089816716, 96.807829986266, 95.71476025409355, 93.52826320839685, 92.89643577987596, 92.9499977023581, 1798.4258236165078, 1052.2310058682867, 1228.4640340886049, 1066.4155429306236, 2685.327849365458, 269.91640645510205, 1241.1808314423902, 1745.042164848263, 5445.580058130278, 273.08263876452867, 4512.083450798086, 621.8837310103634, 341.38754628012913, 797.5106023792039, 598.0569939772568, 1295.338672657306, 518.4036023805568, 691.2317786093854, 764.7569033591686, 1038.850537525601, 1034.9107438318513, 1658.8457873168682, 612.2556837330956, 694.5042878605747, 1055.716257203713, 873.5146316200827, 878.5486242502735, 981.0903841327296, 868.0352808262876, 730.9545499748659, 758.3938373046317, 685.6891170962544, 3730.8465408763695, 2588.1423911567726, 2184.0992907534633, 1323.4102328612928, 1308.2986644393995, 1134.3652106487639, 979.2098715131967, 698.4875919395761, 650.2669616819566, 548.8419732687638, 541.4323721865676, 452.13178246854693, 363.63218295223294, 356.1335718463734, 331.98401896555936, 295.92132451315143, 293.45698032991754, 287.7284685598061, 283.2090929538392, 272.67512336125674, 239.52950740907102, 231.67970675479185, 221.10779955661806, 216.5293506315758, 199.17216265763025, 195.22466811942786, 194.83234060501553, 190.05443588537506, 183.74439196066376, 174.84092264986313, 1367.8679060286452, 3387.024088752336, 398.4981092999641, 1348.0967462107, 2198.732138598651, 562.6158243799697, 418.84986783405884, 437.4754986508369, 422.63686096442353, 1228.7534399665205, 767.0101751225438, 536.7599460118224, 864.8739958361939, 599.3192243173154, 1032.8703143630535, 723.0818282307437, 822.1173808963473, 559.3390553740118, 644.2869549236372, 680.5052335634743, 637.8463629642197, 499.288026196009, 462.81331867901156, 541.6239458279346, 384.73248620513823, 253.58620615506882, 249.51013531089538, 239.21755778595377, 226.41149657728374, 218.91374512697703, 190.7260071752504, 182.2699136493087, 178.04738686480542, 176.6226387015592, 176.34413653213167, 174.45250088780318, 165.39888573919652, 154.9775515072091, 146.7285855453934, 142.98096946648917, 137.98924010111023, 123.83220427862605, 117.5310044398584, 112.74880960596047, 112.41935193762667, 107.39884326590591, 99.37503305370544, 95.23904476884047, 92.89082346370934, 91.3033161679151, 90.98010056825538, 280.49465700403897, 84.95415349298597, 507.6035162098749, 259.91595262666726, 178.31427140513262, 248.67491763869538, 2198.2977913508994, 334.68743072414287, 327.64284704099464, 1090.279959108771, 269.18033788333054, 1225.9254709104796, 546.556752632462, 754.4704643895031, 597.6863234178437, 223.43444702742306, 1507.2172834096107, 437.6180840940653, 447.4195439998839, 363.0594460965716, 751.7991799259781, 447.5943861085542, 1038.1863818512454, 467.1511884256688, 652.9363655718748, 1226.6895386276644, 625.0994992747175, 678.0034222084408, 670.1905345455629, 527.630672041063, 528.2445129023952, 1039.4687498722164, 598.4378107189863, 554.4390893267575, 462.6036889553366, 678.7388630605498, 462.437800765227, 494.9762445363375, 554.0003152243822, 477.59187467519075, 483.46461827831166, 455.8800342528608, 1216.6017064061443, 626.7429464285715, 618.1442476491169, 408.605574699024, 337.88967112819904, 311.00449520679507, 278.8842346285304, 262.11579340099394, 252.58960179341867, 244.85755324456733, 238.8643826377384, 238.02796439192366, 207.64369835828836, 196.73973792564635, 185.98754904669684, 180.17014917994356, 178.1752168118441, 172.44688188678944, 153.1374976757106, 150.25793024983366, 149.205286324761, 145.54354638685405, 136.72793748532604, 134.83178332186063, 133.2466574489118, 124.15833449192428, 120.51556963888216, 118.99523283159999, 118.2750533635221, 116.8431989934147, 1514.2150342390812, 5325.809969454101, 3108.8173237839096, 796.3252166931342, 305.5106911656607, 3750.223074087158, 4820.861495909166, 295.2122733108004, 2006.496946912618, 750.4500146658592, 692.6183830322267, 1598.232923741161, 1435.7180061132694, 634.3368831548149, 428.4151846699701, 946.3743045085499, 802.1515434417157, 725.2849454781962, 939.3419603937626, 665.8409995729636, 760.4361809707028, 1383.9791387974335, 529.0619473156686, 489.91176001639076, 537.5817167321397, 649.6975856995319, 525.4059159220955, 548.574392603766, 476.12807196912775, 495.1580870105619, 564.269262977745, 377.0310039900361, 365.47302793449063, 361.28504597121974, 325.4487939274392, 299.2198211816015, 290.5646354179023, 281.85751959459236, 273.05298261566935, 2757.6775880798054, 240.00920359174236, 229.88080478141842, 211.0988754212042, 210.16313394426012, 204.15501703350432, 190.65823993278545, 183.21487075803455, 274.60337227978215, 157.1304109101329, 1115.3483999493344, 146.31960928144932, 138.40528630514265, 130.18073806379527, 118.97920593580226, 118.18553091440626, 118.63745885174036, 113.71631721149559, 112.52103457147413, 108.9462274731904, 107.77741915953092, 517.6841979718846, 1216.3499103552865, 220.30060914678924, 479.90665296763586, 1666.1769266367496, 569.7640271486079, 3384.8864834673677, 500.5127188911369, 760.7414323199188, 311.017948752748, 226.50647863480887, 759.5247228461944, 893.4377770629843, 530.4872949733476, 330.8832064563447, 621.9846768232319, 456.84050278411564, 377.6590757793038, 369.1264304049558, 327.71450879446354, 321.19409411525345, 333.90860989419303, 310.39664333201796, 311.23596405729944, 313.5248882959746, 941.7914632019806, 605.6399941449728, 473.5892686479123, 408.9949233679444, 390.3042176046074, 345.47836832799476, 234.32361718085517, 232.65680204222224, 206.68487015670266, 174.68659908210435, 168.88867335837327, 156.6890838118577, 144.96405708962436, 149.34767987174558, 142.04352820929208, 120.25216037713706, 119.40192786285363, 107.91726045491693, 101.3106055881212, 101.25422338944183, 101.0923574812381, 98.81696655428937, 98.4047346992204, 95.3021127106441, 94.3057350529784, 93.66065230860214, 91.14081660081843, 89.85980890614759, 89.53896866149611, 83.23661894167307, 1013.8287279841871, 300.0009975565324, 1211.1903288621313, 220.40610275722207, 3355.7611390859447, 225.92540242348917, 325.1504670584278, 192.27816252185954, 268.3346292314183, 459.7397027099826, 274.9822077748565, 350.4496633880524, 1322.2472220438763, 364.99927953822623, 647.1655041757062, 718.2185579852253, 405.67978810811564, 323.073697436276, 239.92835278587998, 543.095973372672, 521.2152083554873, 284.76242820457657, 516.096726082035, 565.7874274526796, 332.83948071059376, 299.1173221860923, 324.33881183797763, 269.5622291126767, 289.3669974338115, 298.600593482504], \"Total\": [5452.0, 6609.0, 5341.0, 4019.0, 4987.0, 4850.0, 4051.0, 5749.0, 3731.0, 2767.0, 4223.0, 3500.0, 3266.0, 2589.0, 3000.0, 2534.0, 2864.0, 3305.0, 2200.0, 4300.0, 2380.0, 2185.0, 3061.0, 3337.0, 2896.0, 2942.0, 2350.0, 2760.0, 2065.0, 2714.0, 692.1731684893695, 539.800891411271, 501.970604062385, 457.0125769333406, 372.8729600931267, 367.17290606227954, 355.6518074362647, 335.26316699418146, 248.8305593001759, 247.05611847206683, 247.9477190268809, 246.50069329822432, 226.76890962989552, 219.74744641603579, 218.50732857997158, 200.4380391007803, 185.2669247443398, 183.99203352865985, 183.05721815444332, 180.3796674797534, 179.51190104902042, 176.35513720169547, 159.14212863489664, 156.86488369293315, 153.48491530165282, 149.3313013440994, 147.3055723865311, 142.15772114981965, 140.71852368923857, 137.56528240236915, 2896.0560431881136, 3305.9816625663593, 2942.0619909531943, 2714.7441839575167, 2701.7251722258115, 3061.3499730131684, 1929.2774967987511, 767.5367979508893, 745.438826544091, 327.1678660864662, 1847.8426473108932, 991.1462037865723, 445.3213916595876, 599.4720264285588, 2388.4691545116584, 896.779620357389, 532.6729244140569, 2339.658518687147, 1630.7363406801858, 572.3376409542132, 1377.9011927568906, 4300.275362851497, 2522.704276764815, 1903.4943580131728, 1981.102865470996, 1855.862286885848, 1460.2704086556657, 1469.247944206441, 1321.1764361223716, 1206.957791108779, 928.772401998973, 1754.1345638222056, 1662.7851996629738, 1574.6960792291216, 1676.3861869514208, 2007.88278273527, 1857.9821399748507, 6609.435138232711, 3834.956275448441, 1342.0201503655846, 973.5215095374398, 667.4590870617016, 536.705535565833, 411.7173641153965, 355.0507070616793, 351.9069744920722, 323.283076441869, 298.91455379297344, 264.87114178842637, 240.07642924796662, 226.1226348604672, 222.6507988847624, 221.89651785718374, 209.4814883764761, 204.573485772285, 195.32433457359255, 195.19882059065836, 193.857988648779, 188.51772518691573, 181.66646823553802, 160.73786685862223, 153.84732077181366, 144.279122938054, 141.15852909004343, 136.87436598221583, 131.58708146089472, 128.14811259764508, 127.88237608090344, 126.51879717112138, 2760.789228647127, 1226.1360631000496, 443.9281866905547, 613.1384039121243, 1047.4496183069703, 950.2660442276109, 1025.5592301418344, 3337.499340539413, 348.8118294867223, 3232.2517897868843, 479.8619883399161, 1251.383342267772, 346.9736271307462, 1239.9397365413458, 741.3602381784619, 514.7429311038542, 1547.209642953548, 2750.5832670045793, 934.5853288292453, 895.4057608998952, 1402.8144973805406, 2135.5114204764977, 796.2633936343487, 1008.9660483361824, 597.828216923325, 703.48151434786, 1027.3223402318363, 965.4255305090294, 1627.6553057239873, 3281.7450793014273, 1683.8559975887288, 949.3063035875944, 1563.9067712747485, 1939.8161794731386, 3834.956275448441, 1937.9366115505843, 1313.4338258084545, 2553.0639263664575, 1811.213834107384, 2522.704276764815, 1379.2283036226393, 1324.7752831502467, 1630.3840223207374, 4850.777603851211, 1435.3558030575552, 1169.156549685942, 1120.2268439827224, 963.5069756366454, 838.4375256427345, 356.9920515642927, 312.5797296153408, 336.43337977361233, 277.97793708980834, 269.9428655190769, 268.25156191224664, 255.8232772581338, 253.53948682440597, 248.5348658606064, 247.97154834585643, 233.64669808451944, 227.30387433298804, 225.08614744371755, 222.30000282815627, 211.61876686196635, 202.89445169159413, 192.50172032797508, 179.8401456849274, 178.91350809914704, 175.6232423928165, 173.93139130340745, 173.84230599865475, 170.48350794231953, 167.5686193076286, 306.70025347503304, 1382.557970704421, 553.5801460515132, 1544.761129522033, 994.1507287862934, 827.2469032808091, 1207.7287738286095, 1670.7328508839405, 1077.7570878796655, 659.9757785719913, 2389.008043190928, 443.1655489962198, 1093.0447705620234, 1392.2754005972781, 1514.6591537999113, 1064.8989679474912, 1072.8829750358345, 661.7829201789452, 743.899818278863, 469.0422621382149, 1950.2378222999646, 983.0044967810397, 2572.2323068866867, 774.5807193795964, 695.6079403512565, 1056.5832125650406, 1663.6860566830885, 1529.5734948257386, 2939.6959491662456, 1378.3189728614104, 2553.0639263664575, 1486.0566241771942, 1771.5193195120366, 5749.821467851752, 3105.7495758245213, 3018.312431124778, 2534.5465937279223, 2065.980900412523, 1004.5045469072162, 700.9913030354743, 693.0426414771409, 640.5662179765486, 513.7105738248742, 485.59483949209914, 429.15394962591233, 421.9568256748328, 407.19389768386344, 405.6741166117599, 405.60926031835345, 381.58272044265306, 337.80364973845684, 330.52249190961766, 320.23706728833406, 312.1387967363763, 302.4010972357777, 282.3550778907437, 269.2947539196623, 250.17728039550857, 242.84073684759477, 237.95212078050088, 233.518469759145, 232.124151332884, 220.78186556376727, 218.02605452113815, 214.25901511817509, 211.0087344964116, 3000.9919902414435, 1727.8706587829176, 4223.788357457218, 527.540321489938, 719.7512624812483, 2609.881709130455, 660.7069314069479, 1195.8638116617667, 600.4097696462064, 613.1526084765399, 604.8890734933485, 1675.6853873632806, 2172.022202621545, 2244.5972652827136, 1431.2453421314788, 1206.102849148073, 2635.3329493248407, 959.1514577344103, 2572.2323068866867, 1855.2654821821068, 992.7528749533193, 2417.5578847993734, 3018.312431124778, 1423.9181401703602, 3245.537838484764, 1092.2812992483498, 1562.7447130364822, 1279.9484558889249, 640.5553782837247, 577.0348325692742, 511.87443772887366, 472.3911166624623, 442.84353662598556, 305.1148400407164, 301.94019116879656, 424.3970875302922, 243.06232645344622, 226.74893771359248, 217.94856770139606, 215.65720082699636, 210.9150345392432, 204.1020708438221, 201.2216345138478, 193.51312712802954, 179.31628892888583, 174.49244508935823, 169.49773873463562, 169.58417758040608, 154.75018729655392, 152.44377882637906, 143.41967957852688, 109.2861259073891, 103.64398255761216, 97.70738316177386, 96.6145418367571, 94.42865415179011, 93.79606808259425, 93.85049823603056, 1821.2049690933893, 1063.8966211658392, 1249.6401947631725, 1115.2535280504565, 2864.0730563450115, 278.6615541348766, 1345.9679179362065, 1937.8207193366359, 6609.435138232711, 283.62627053844545, 5749.821467851752, 696.1261074827864, 367.72793622636857, 1046.1648766162693, 765.3484129047368, 2075.042633767461, 686.6187176188685, 1017.3563790682488, 1220.9896737230968, 1932.3156528425934, 1922.6549268615486, 3834.956275448441, 950.824218418761, 1158.1272172084732, 2326.4926145569693, 1937.9366115505843, 2281.5390331015874, 3105.7495758245213, 2511.7698782781613, 1782.1885925195747, 2071.7547018121463, 2750.5832670045793, 3731.75651779533, 2589.0523421921644, 2185.0092631228185, 1324.3202086387191, 1309.2087294037517, 1135.2751992444064, 980.1197916311199, 699.3975374790134, 651.1769051076253, 549.7519405881646, 542.3425612001053, 453.0417500310156, 364.54217626156606, 357.0437700364001, 332.8939487906884, 296.8314756591194, 294.3669394658447, 288.6385280885482, 284.11965065297505, 273.58505926456655, 240.43953812957142, 232.5898289233288, 222.0178669078926, 217.43927253449576, 200.0825309690733, 196.1345878470444, 195.74228750632716, 190.9667411133675, 184.65531726431806, 175.75088470650797, 1375.287080129614, 3500.850838987311, 405.09890789483035, 1411.6945470432102, 2808.049804429841, 641.3824261153752, 479.54743161789906, 509.416269167867, 494.68930162264417, 1982.1511123961498, 1105.0389285307879, 699.1524410339255, 1568.2007241163944, 944.5243473435529, 2326.4926145569693, 1286.5113410669687, 2537.4096607300517, 1054.9805916649866, 1514.0640900121232, 1756.3770473255215, 3245.537838484764, 1890.4769158118793, 1158.1272172084732, 542.5230676669813, 385.6315558476714, 254.48535967113054, 250.4091906407505, 240.11675759705807, 227.31079157992735, 219.81296772587612, 191.62606506728386, 183.16909873805326, 178.9464763003432, 177.52282150064397, 177.2439313049071, 175.3517129296629, 166.29799527739266, 155.8797180756763, 147.6276842594168, 143.88129756668806, 138.8888332109903, 124.73124974539652, 118.43098387538026, 113.64845092096523, 113.32004104841359, 108.3021117840918, 100.27639753829577, 96.13879873824797, 93.78987127170944, 92.2040208448728, 91.88205028946798, 283.3078993855819, 85.85332730170416, 513.7689374180627, 263.54273773233984, 183.20208353779222, 261.2918219815502, 2563.978267757246, 357.2515157206796, 359.94960518430923, 1449.7943985464865, 302.3025210380674, 1725.415786729757, 706.0883496476544, 1046.0867978708104, 795.2967721902395, 248.05750886117647, 2764.5890522818513, 589.9362173266547, 619.1877945523435, 474.95898100873654, 1288.2495079245477, 634.331741253005, 2080.6958592929423, 700.0833125251218, 1227.449569245857, 3245.537838484764, 1516.1382280677683, 1802.6836842806406, 1810.0427166487764, 1195.721148467055, 1216.0959981076435, 4987.2775091554495, 1612.8426387087295, 1369.0566750033452, 936.0872936223257, 2511.7698782781613, 1056.5990891347155, 1633.7783727929373, 3018.312431124778, 2080.4858584729373, 3105.7495758245213, 2007.88278273527, 1217.493988422391, 627.6350556218958, 619.0365074911243, 409.4977800132771, 338.7818040771374, 311.8967243938934, 279.7765679373663, 263.00915937876823, 253.4817620318968, 245.74981619446478, 239.75650543344858, 238.92031247391876, 208.5361970518478, 197.6323884335122, 186.8797418809329, 181.06409323244205, 179.06916278309123, 173.34070031387066, 154.02959863146845, 151.15000898453312, 150.0978920875554, 146.43580209599003, 137.62102061127322, 135.72639865007875, 134.13881553964146, 125.05081115794843, 121.40765647088236, 119.8874718870426, 119.16732202557232, 117.73535099856697, 1525.9957314646304, 5452.878377138275, 3266.5229714511242, 821.3181697617492, 312.6394708058274, 4051.109303283252, 5341.4022820513765, 309.1756096595071, 2350.7683734678585, 826.0361656538224, 767.6932262662461, 1961.7736265677752, 1830.9929402542837, 774.7158257566211, 493.2042533605011, 1333.0300168978067, 1120.8684168299333, 1001.7258110930583, 1857.9821399748507, 1137.1123822194004, 1397.8117311471212, 4300.275362851497, 891.3989407821491, 960.9224085970185, 1207.1547742313164, 1938.4568298796582, 1220.4036956724296, 1890.4769158118793, 1167.8567225403006, 2939.6959491662456, 565.1736387195617, 377.9353958782644, 366.37744499153365, 362.18940094964023, 326.3531514890687, 300.12419272725793, 291.4689625532402, 282.7619359302662, 273.9573333700538, 2767.1265634927677, 240.91349794727125, 230.78675889777796, 212.00321149743692, 211.06752833901535, 205.05938958194616, 191.56312437741934, 184.11935219555357, 276.1166414012622, 158.03477328653796, 1122.1455308393768, 147.22393142598006, 139.31079426797828, 131.08512993663743, 119.8835471961902, 119.08989498903847, 119.54760194145659, 114.6206185744887, 113.42539503112478, 109.85162606041725, 108.68447246248513, 564.056382471854, 1497.4652360586263, 236.41938435163004, 557.6791317994432, 2200.2766355746357, 680.2611206998314, 4987.2775091554495, 595.3075697996719, 980.9352405708887, 399.62663946750166, 261.96054493431734, 1561.2152548888066, 2080.6958592929423, 1195.721148467055, 544.9400047188394, 3281.7450793014273, 1725.415786729757, 1232.3653631689224, 1163.943734436704, 844.2417647780097, 898.025217297617, 1227.449569245857, 747.3870265546, 880.8545050891394, 1377.9011927568906, 942.6823859444231, 606.5310875182787, 474.4804049066117, 409.8861443741101, 391.19512797310836, 346.36925387387697, 235.21455394062178, 233.54808466123868, 207.57594722278785, 175.57753702473045, 169.78060204716328, 157.5800245594706, 145.85546841046434, 150.28238893036433, 142.9421334657779, 121.14317641883497, 120.2928648272947, 108.80811282998127, 102.20156798891313, 102.14608334877286, 101.98328918074586, 99.70786244720972, 99.29562816675737, 96.19303246559018, 95.19677987262604, 94.55159791346446, 92.03619553129778, 90.75079303931932, 90.430131537618, 84.12885040631768, 1043.7801253763103, 306.0594087777602, 1261.4404914561494, 229.25669234540464, 4019.4106692858445, 237.83727120788205, 352.6251105441739, 204.5774728458651, 297.33249834866683, 555.4420858957965, 324.6226791875233, 438.43417629033377, 2380.382833734089, 499.38562158364255, 1056.564872744493, 1244.9818835744966, 580.629175453033, 443.3898112200244, 318.8403327277062, 1435.792102255983, 1402.1778941765274, 441.6818025482354, 1393.31740288322, 1905.5614629742233, 757.7965853110795, 1019.1925082912296, 1950.2378222999646, 593.9930459201323, 1125.0127749654962, 2417.5578847993734], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.9159, -6.1649, -6.2377, -6.3317, -6.5356, -6.5511, -6.583, -6.6422, -6.9413, -6.9485, -6.9449, -6.9507, -7.0345, -7.0661, -7.0717, -7.1584, -7.2375, -7.2444, -7.2496, -7.2644, -7.2692, -7.2871, -7.3903, -7.4048, -7.4267, -7.4543, -7.4681, -7.5038, -7.5141, -7.5369, -4.4913, -4.3619, -4.4775, -4.5598, -4.5674, -4.45, -4.9339, -5.8444, -5.8746, -6.6834, -5.0687, -5.6563, -6.4093, -6.131, -4.8477, -5.7953, -6.2673, -4.9822, -5.3161, -6.2211, -5.4851, -4.528, -5.0654, -5.3161, -5.3321, -5.3794, -5.6126, -5.6101, -5.6967, -5.7646, -5.9324, -5.5765, -5.6277, -5.6671, -5.6586, -5.5958, -5.6899, -5.4375, -5.6371, -5.1838, -5.505, -5.8829, -6.1013, -6.3669, -6.5153, -6.5242, -6.6093, -6.6879, -6.8092, -6.9078, -6.9679, -6.9835, -6.9869, -7.0447, -7.0685, -7.115, -7.1156, -7.1225, -7.1506, -7.1878, -7.3109, -7.3549, -7.4195, -7.4415, -7.4726, -7.5122, -7.5389, -7.541, -7.552, -4.4779, -5.2843, -6.3049, -5.9869, -5.4695, -5.5653, -5.4992, -4.3678, -6.5585, -4.4102, -6.2632, -5.3782, -6.5757, -5.4018, -5.8778, -6.2297, -5.2666, -4.7621, -5.7157, -5.7622, -5.3934, -5.0489, -5.8938, -5.7331, -6.1455, -6.0252, -5.7644, -5.8088, -5.4621, -5.0017, -5.4568, -5.8349, -5.5324, -5.5178, -5.2447, -5.5364, -5.708, -5.508, -5.6346, -5.5795, -5.7341, -5.7387, -5.7369, -3.7289, -4.947, -5.1523, -5.1951, -5.3459, -5.4851, -6.3404, -6.4736, -6.4003, -6.5913, -6.6207, -6.627, -6.6746, -6.6836, -6.7037, -6.7059, -6.7657, -6.7933, -6.8031, -6.8156, -6.8651, -6.9074, -6.9602, -7.0286, -7.0338, -7.0524, -7.0621, -7.0627, -7.0823, -7.0996, -6.4988, -5.0143, -5.9218, -4.9226, -5.3682, -5.5483, -5.1837, -4.8797, -5.3034, -5.7819, -4.6064, -6.1763, -5.3471, -5.1257, -5.05, -5.3908, -5.402, -5.8985, -5.8138, -6.1873, -5.1078, -5.6325, -4.9273, -5.832, -5.9097, -5.6484, -5.3762, -5.4832, -5.2107, -5.5998, -5.4024, -5.6116, -5.563, -5.2512, -5.4635, -5.6015, -4.1652, -4.3697, -5.0913, -5.4514, -5.4628, -5.5417, -5.7627, -5.8191, -5.9429, -5.9598, -5.9955, -5.9993, -5.9994, -6.0606, -6.1828, -6.2047, -6.2364, -6.2621, -6.2938, -6.3626, -6.4102, -6.484, -6.5139, -6.5343, -6.5532, -6.5592, -6.6095, -6.6221, -6.6396, -6.655, -4.0259, -4.5923, -3.7282, -5.7568, -5.4832, -4.2618, -5.5933, -5.0543, -5.7165, -5.6984, -5.7216, -4.8703, -4.7181, -4.716, -5.1072, -5.2629, -4.7771, -5.4781, -4.992, -5.1863, -5.5481, -5.1311, -5.0459, -5.407, -5.2798, -5.5972, -5.5715, -5.6211, -5.5323, -5.6369, -5.757, -5.8374, -5.9025, -6.2755, -6.286, -5.9458, -6.5037, -6.5734, -6.6131, -6.6238, -6.6461, -6.6791, -6.6933, -6.7326, -6.8091, -6.8366, -6.8658, -6.8653, -6.9573, -6.9724, -7.0338, -7.3076, -7.361, -7.4205, -7.4319, -7.455, -7.4618, -7.4612, -4.4986, -5.0346, -4.8798, -5.0212, -4.0977, -6.3952, -4.8695, -4.5287, -3.3907, -6.3835, -3.5788, -5.5605, -6.1603, -5.3118, -5.5996, -4.8267, -5.7425, -5.4548, -5.3537, -5.0474, -5.0512, -4.5794, -5.5761, -5.4501, -5.0313, -5.2207, -5.215, -5.1046, -5.227, -5.3989, -5.3621, -5.4628, -3.6475, -4.0132, -4.1829, -4.6839, -4.6954, -4.838, -4.9851, -5.323, -5.3945, -5.5641, -5.5777, -5.7579, -5.9757, -5.9966, -6.0668, -6.1818, -6.1901, -6.2099, -6.2257, -6.2636, -6.3932, -6.4265, -6.4732, -6.4941, -6.5777, -6.5977, -6.5997, -6.6246, -6.6583, -6.708, -4.6509, -3.7442, -5.8842, -4.6654, -4.1762, -5.5393, -5.8344, -5.7908, -5.8254, -4.7581, -5.2294, -5.5863, -5.1093, -5.4761, -4.9318, -5.2883, -5.16, -5.5451, -5.4037, -5.349, -5.4138, -5.6587, -5.7345, -5.5393, -5.8813, -6.2981, -6.3143, -6.3565, -6.4115, -6.4452, -6.583, -6.6283, -6.6518, -6.6598, -6.6614, -6.6722, -6.7255, -6.7906, -6.8452, -6.8711, -6.9067, -7.0149, -7.0671, -7.1087, -7.1116, -7.1573, -7.2349, -7.2774, -7.3024, -7.3196, -7.3232, -6.1973, -7.3917, -5.6041, -6.2735, -6.6503, -6.3177, -4.1384, -6.0206, -6.0419, -4.8396, -6.2385, -4.7224, -5.5302, -5.2078, -5.4408, -6.4247, -4.5158, -5.7525, -5.7303, -5.9393, -5.2114, -5.7299, -4.8886, -5.6872, -5.3524, -4.7218, -5.3959, -5.3147, -5.3263, -5.5654, -5.5643, -4.8874, -5.4395, -5.5159, -5.697, -5.3136, -5.6973, -5.6293, -5.5167, -5.6651, -5.6529, -5.7116, -4.633, -5.2963, -5.3101, -5.7241, -5.9141, -5.997, -6.106, -6.168, -6.205, -6.2361, -6.2609, -6.2644, -6.401, -6.4549, -6.5111, -6.5429, -6.554, -6.5867, -6.7055, -6.7245, -6.7315, -6.7563, -6.8188, -6.8328, -6.8446, -6.9152, -6.945, -6.9577, -6.9638, -6.976, -4.4142, -3.1565, -3.6948, -5.0568, -6.0148, -3.5072, -3.2561, -6.0491, -4.1327, -5.1161, -5.1963, -4.3602, -4.4674, -5.2842, -5.6767, -4.8842, -5.0495, -5.1502, -4.8916, -5.2358, -5.1029, -4.5041, -5.4657, -5.5426, -5.4497, -5.2603, -5.4726, -5.4295, -5.5711, -5.5319, -4.9305, -5.3337, -5.3648, -5.3764, -5.4808, -5.5648, -5.5942, -5.6246, -5.6564, -3.3439, -5.7853, -5.8285, -5.9137, -5.9181, -5.9471, -6.0155, -6.0554, -5.6507, -6.2089, -4.2491, -6.2802, -6.3358, -6.3971, -6.4871, -6.4938, -6.49, -6.5323, -6.5429, -6.5752, -6.586, -5.0167, -4.1624, -5.871, -5.0924, -3.8477, -4.9208, -3.1389, -5.0504, -4.6317, -5.5262, -5.8433, -4.6333, -4.4709, -4.9922, -5.4643, -4.8331, -5.1417, -5.332, -5.3549, -5.4739, -5.494, -5.4552, -5.5282, -5.5255, -5.5181, -4.1422, -4.5837, -4.8297, -4.9763, -5.0231, -5.1451, -5.5333, -5.5405, -5.6588, -5.827, -5.8608, -5.9358, -6.0135, -5.9838, -6.0339, -6.2004, -6.2075, -6.3087, -6.3718, -6.3724, -6.374, -6.3968, -6.4009, -6.433, -6.4435, -6.4504, -6.4776, -6.4918, -6.4954, -6.5683, -4.0685, -5.2862, -3.8907, -5.5946, -2.8716, -5.5698, -5.2057, -5.7311, -5.3978, -4.8594, -5.3733, -5.1308, -3.8029, -5.0901, -4.5174, -4.4133, -4.9845, -5.2122, -5.5097, -4.6927, -4.7339, -5.3384, -4.7437, -4.6518, -5.1824, -5.2892, -5.2082, -5.3932, -5.3223, -5.2909], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.8082, 1.8078, 1.8077, 1.8075, 1.8071, 1.807, 1.807, 1.8068, 1.8059, 1.8058, 1.8058, 1.8058, 1.8055, 1.8054, 1.8054, 1.805, 1.8046, 1.8046, 1.8046, 1.8045, 1.8045, 1.8044, 1.8038, 1.8037, 1.8036, 1.8035, 1.8034, 1.8032, 1.8031, 1.8029, 1.8015, 1.7985, 1.7995, 1.7977, 1.7948, 1.7873, 1.7652, 1.7764, 1.7754, 1.79, 1.6734, 1.7088, 1.7558, 1.7369, 1.6378, 1.6698, 1.7187, 1.524, 1.551, 1.6931, 1.5506, 1.3695, 1.3655, 1.3964, 1.3405, 1.3584, 1.3649, 1.3613, 1.381, 1.4035, 1.4977, 1.2177, 1.22, 1.2351, 1.1809, 1.0633, 1.0468, 0.0301, 0.3749, 1.8782, 1.878, 1.8776, 1.8772, 1.8767, 1.8764, 1.8764, 1.8761, 1.8759, 1.8755, 1.8752, 1.8749, 1.8749, 1.8749, 1.8746, 1.8745, 1.8743, 1.8743, 1.8743, 1.8741, 1.874, 1.8733, 1.8731, 1.8727, 1.8725, 1.8723, 1.8721, 1.8719, 1.8719, 1.8715, 1.8627, 1.8681, 1.8634, 1.8585, 1.8403, 1.8419, 1.8318, 1.7832, 1.8509, 1.7728, 1.8273, 1.7538, 1.839, 1.7393, 1.7777, 1.7906, 1.6531, 1.5823, 1.7081, 1.7045, 1.6243, 1.5486, 1.6902, 1.6142, 1.7251, 1.6827, 1.5648, 1.5826, 1.4069, 1.1661, 1.3783, 1.5733, 1.3766, 1.1758, 0.7673, 1.1582, 1.3755, 0.9108, 1.1276, 0.8514, 1.3006, 1.3362, 1.1304, 2.0482, 2.0477, 2.0476, 2.0476, 2.0474, 2.0473, 2.0458, 2.0455, 2.0453, 2.0451, 2.045, 2.045, 2.0448, 2.0448, 2.0447, 2.0447, 2.0445, 2.0444, 2.0443, 2.0443, 2.0441, 2.0439, 2.0436, 2.0433, 2.0433, 2.0432, 2.0431, 2.0431, 2.043, 2.0429, 2.0392, 2.018, 2.0257, 1.9987, 1.9938, 1.9975, 1.9837, 1.9632, 1.9779, 1.9898, 1.8789, 1.9937, 1.9201, 1.8995, 1.891, 1.9025, 1.8838, 1.8705, 1.8382, 1.926, 1.5804, 1.7409, 1.4842, 1.7796, 1.8095, 1.6527, 1.4709, 1.448, 1.0671, 1.4355, 1.0165, 1.3484, 1.2214, 0.3558, 0.7594, 0.65, 2.261, 2.2609, 2.2604, 2.26, 2.26, 2.2599, 2.2596, 2.2594, 2.2592, 2.2592, 2.2591, 2.2591, 2.2591, 2.259, 2.2586, 2.2586, 2.2585, 2.2584, 2.2583, 2.2581, 2.258, 2.2577, 2.2576, 2.2575, 2.2575, 2.2574, 2.2572, 2.2572, 2.2571, 2.2571, 2.2313, 2.217, 2.1873, 2.239, 2.2018, 2.1351, 2.1773, 2.123, 2.1498, 2.1469, 2.1373, 1.9696, 1.8624, 1.8316, 1.8905, 1.9058, 1.6101, 1.9198, 1.4194, 1.5518, 1.8153, 1.3423, 1.2055, 1.5958, 0.8991, 1.6707, 1.3382, 1.4882, 2.2693, 2.2691, 2.2689, 2.2688, 2.2683, 2.2677, 2.2677, 2.2674, 2.267, 2.2667, 2.2665, 2.2665, 2.2664, 2.2663, 2.2662, 2.266, 2.2656, 2.2655, 2.2653, 2.2653, 2.2648, 2.2647, 2.2644, 2.2624, 2.262, 2.2614, 2.2613, 2.2611, 2.261, 2.261, 2.2581, 2.2596, 2.2536, 2.2259, 2.2062, 2.2388, 2.1896, 2.1659, 2.077, 2.2328, 2.0283, 2.1579, 2.1963, 1.9993, 2.024, 1.7995, 1.9896, 1.8842, 1.8028, 1.6501, 1.6513, 1.4326, 1.8305, 1.7593, 1.4805, 1.4738, 1.3163, 1.1183, 1.2082, 1.3794, 1.2657, 0.8815, 2.3918, 2.3917, 2.3917, 2.3914, 2.3914, 2.3913, 2.3911, 2.3908, 2.3907, 2.3904, 2.3904, 2.3901, 2.3896, 2.3895, 2.3893, 2.389, 2.389, 2.3889, 2.3889, 2.3887, 2.3883, 2.3881, 2.388, 2.3879, 2.3875, 2.3874, 2.3874, 2.3873, 2.3871, 2.3869, 2.3867, 2.359, 2.3756, 2.346, 2.1475, 2.261, 2.2567, 2.2398, 2.2347, 1.9139, 2.0269, 2.1278, 1.797, 1.9372, 1.58, 1.8159, 1.2651, 1.7575, 1.5377, 1.4439, 0.7651, 1.0607, 1.4748, 2.4284, 2.4278, 2.4266, 2.4265, 2.4264, 2.4261, 2.426, 2.4254, 2.4252, 2.4251, 2.425, 2.425, 2.425, 2.4247, 2.4243, 2.424, 2.4238, 2.4236, 2.4229, 2.4225, 2.4222, 2.4221, 2.4217, 2.4211, 2.4207, 2.4205, 2.4203, 2.4202, 2.4201, 2.4196, 2.418, 2.4162, 2.4031, 2.3806, 2.2762, 2.3649, 2.3361, 2.1451, 2.3141, 2.0883, 2.174, 2.1033, 2.1445, 2.3256, 1.8235, 2.1314, 2.1052, 2.1614, 1.8915, 2.0814, 1.7349, 2.0256, 1.7989, 1.4571, 1.5441, 1.4522, 1.4366, 1.612, 1.5963, 0.8619, 1.4387, 1.5262, 1.7253, 1.1216, 1.6038, 1.236, 0.7348, 0.9585, 0.5701, 0.9475, 2.5264, 2.5257, 2.5257, 2.525, 2.5245, 2.5243, 2.5239, 2.5237, 2.5236, 2.5235, 2.5234, 2.5234, 2.5228, 2.5226, 2.5223, 2.5222, 2.5221, 2.522, 2.5213, 2.5212, 2.5212, 2.521, 2.5206, 2.5205, 2.5205, 2.52, 2.5198, 2.5197, 2.5196, 2.5195, 2.5194, 2.5036, 2.4777, 2.4962, 2.5041, 2.45, 2.4246, 2.4809, 2.3688, 2.4312, 2.4242, 2.3222, 2.2839, 2.3272, 2.3863, 2.1846, 2.1926, 2.2042, 1.8451, 1.9919, 1.9184, 1.3934, 2.0054, 1.8535, 1.7182, 1.434, 1.6844, 1.2899, 1.6299, 0.746, 2.9963, 2.9955, 2.9954, 2.9954, 2.9951, 2.9949, 2.9948, 2.9947, 2.9946, 2.9945, 2.9942, 2.994, 2.9936, 2.9936, 2.9935, 2.9932, 2.993, 2.9924, 2.9922, 2.9918, 2.9918, 2.9914, 2.991, 2.9903, 2.9903, 2.9903, 2.99, 2.9899, 2.9896, 2.9895, 2.9121, 2.79, 2.9273, 2.8477, 2.7199, 2.8207, 2.6103, 2.8245, 2.7437, 2.7472, 2.8525, 2.2774, 2.1525, 2.1852, 2.499, 1.3347, 1.669, 1.8152, 1.8495, 2.0516, 1.9698, 1.6961, 2.1192, 1.9576, 1.5175, 3.273, 3.2724, 3.272, 3.2717, 3.2716, 3.2713, 3.2701, 3.2701, 3.2696, 3.2688, 3.2686, 3.2682, 3.2678, 3.2677, 3.2676, 3.2665, 3.2665, 3.2657, 3.2652, 3.2651, 3.2651, 3.2649, 3.2649, 3.2646, 3.2645, 3.2644, 3.2641, 3.264, 3.264, 3.2632, 3.2448, 3.2539, 3.2333, 3.2345, 3.0935, 3.2225, 3.1928, 3.2119, 3.1713, 3.0848, 3.108, 3.0499, 2.686, 2.9604, 2.7837, 2.7238, 2.9154, 2.9573, 2.9896, 2.3017, 2.2843, 2.835, 2.2808, 2.0596, 2.4512, 2.048, 1.48, 2.4838, 1.9161, 1.1825]}, \"token.table\": {\"Topic\": [10, 7, 10, 3, 4, 6, 4, 7, 9, 3, 4, 5, 6, 7, 8, 3, 4, 7, 6, 5, 8, 7, 2, 6, 9, 6, 3, 8, 6, 5, 2, 9, 8, 3, 8, 10, 4, 7, 8, 3, 3, 10, 3, 7, 9, 10, 1, 6, 2, 9, 4, 1, 3, 6, 8, 6, 7, 7, 2, 3, 4, 5, 6, 7, 9, 5, 6, 4, 4, 3, 7, 9, 3, 6, 6, 1, 2, 3, 4, 5, 6, 7, 1, 6, 9, 1, 8, 2, 1, 6, 8, 1, 9, 10, 8, 10, 1, 5, 7, 10, 1, 10, 8, 1, 5, 9, 1, 3, 4, 5, 7, 8, 10, 2, 1, 4, 7, 5, 3, 5, 1, 6, 3, 5, 4, 8, 4, 6, 7, 10, 6, 3, 4, 7, 10, 3, 7, 10, 3, 8, 7, 9, 4, 7, 1, 5, 3, 5, 9, 7, 7, 5, 8, 7, 1, 2, 3, 8, 2, 3, 8, 3, 7, 3, 9, 1, 2, 3, 4, 7, 8, 9, 10, 1, 2, 3, 4, 5, 10, 2, 3, 4, 8, 1, 2, 3, 4, 7, 7, 1, 2, 5, 6, 8, 9, 8, 9, 10, 1, 1, 5, 8, 9, 9, 3, 4, 7, 8, 9, 2, 4, 5, 7, 8, 9, 7, 10, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 3, 2, 5, 6, 1, 3, 4, 9, 3, 9, 1, 10, 1, 2, 3, 4, 5, 7, 8, 10, 4, 4, 9, 1, 2, 4, 1, 2, 3, 4, 5, 6, 8, 9, 3, 3, 4, 1, 5, 6, 3, 9, 8, 3, 4, 5, 6, 7, 4, 8, 10, 10, 1, 2, 3, 6, 8, 3, 9, 4, 4, 6, 7, 1, 1, 3, 4, 5, 2, 5, 6, 7, 9, 2, 3, 4, 5, 3, 4, 7, 7, 5, 2, 3, 10, 1, 3, 5, 9, 10, 1, 3, 7, 1, 2, 3, 6, 10, 1, 5, 3, 6, 1, 2, 3, 4, 6, 7, 8, 9, 10, 9, 4, 1, 4, 10, 1, 2, 4, 6, 7, 8, 9, 10, 1, 5, 9, 9, 1, 3, 5, 1, 3, 5, 6, 7, 8, 9, 10, 6, 9, 2, 5, 2, 1, 4, 6, 7, 8, 3, 3, 4, 7, 8, 4, 1, 2, 4, 1, 2, 3, 1, 1, 2, 3, 8, 9, 10, 3, 9, 1, 3, 4, 7, 8, 10, 1, 3, 4, 5, 9, 10, 1, 3, 5, 8, 1, 2, 3, 8, 3, 4, 6, 7, 6, 5, 1, 2, 5, 1, 3, 4, 5, 6, 8, 5, 4, 1, 2, 4, 5, 6, 8, 1, 2, 5, 6, 8, 9, 1, 3, 6, 7, 1, 3, 6, 7, 1, 2, 4, 5, 6, 8, 1, 2, 3, 4, 7, 10, 3, 4, 2, 3, 6, 1, 2, 8, 4, 1, 2, 5, 6, 8, 4, 7, 9, 3, 1, 3, 4, 7, 8, 1, 3, 4, 6, 7, 8, 9, 1, 3, 4, 7, 9, 4, 4, 3, 7, 4, 1, 3, 4, 6, 7, 8, 10, 3, 3, 8, 3, 4, 10, 2, 1, 2, 3, 4, 6, 7, 10, 8, 2, 6, 1, 5, 6, 9, 1, 9, 3, 7, 1, 2, 5, 6, 7, 8, 9, 1, 3, 9, 10, 1, 4, 7, 9, 1, 2, 1, 8, 1, 8, 9, 2, 9, 9, 10, 2, 8, 1, 2, 8, 1, 2, 3, 8, 3, 1, 2, 3, 4, 5, 8, 2, 5, 10, 3, 7, 10, 6, 1, 3, 7, 8, 9, 10, 1, 3, 8, 10, 1, 2, 9, 1, 3, 7, 9, 10, 4, 4, 1, 10, 7, 6, 9, 3, 4, 6, 8, 1, 9, 5, 3, 4, 5, 3, 9, 1, 2, 5, 6, 8, 9, 8, 3, 9, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 3, 8, 9, 4, 4, 1, 3, 4, 5, 8, 1, 1, 3, 4, 5, 1, 2, 3, 4, 5, 9, 8, 6, 8, 1, 3, 4, 5, 6, 8, 3, 1, 2, 5, 9, 1, 5, 6, 8, 9, 5, 10, 10, 9, 3, 5, 8, 9, 10, 5, 3, 6, 8, 10, 1, 2, 1, 2, 3, 9, 4, 3, 4, 7, 9, 8, 1, 8, 5, 6, 3, 9, 10, 2, 5, 6, 8, 1, 2, 3, 4, 6, 7, 8, 10, 8, 1, 3, 8, 10, 10, 1, 2, 3, 4, 5, 7, 8, 1, 3, 4, 6, 7, 8, 1, 1, 2, 5, 9, 10, 1, 9, 8, 2, 10, 9, 5, 2, 1, 2, 3, 5, 9, 1, 5, 8, 1, 6, 8, 10, 7, 7, 1, 6, 6, 10, 4, 2, 3, 4, 6, 7, 9, 10, 8, 1, 2, 3, 5, 6, 9, 10, 1, 2, 5, 6, 9, 9, 5, 1, 3, 6, 9, 8, 1, 3, 10, 8, 1, 5, 9, 4, 3, 7, 5, 1, 6, 5, 6, 7, 8, 10, 8, 3, 4, 7, 3, 4, 7, 4, 7, 5, 6, 5, 5, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 7, 8, 10, 2, 5, 7, 9, 10, 4, 7, 9, 4, 1, 4, 1, 2, 3, 5, 6, 9, 7, 8, 2, 9, 1, 2, 3, 4, 5, 7, 10, 1, 3, 5, 2, 2, 5, 9, 1, 2, 3, 4, 8, 9, 10, 2, 3, 1, 2, 5, 6, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 6, 7, 8, 9, 1, 2, 3, 7, 6, 1, 7, 3, 4, 2, 2, 4, 5, 6, 8, 2, 3, 5, 5, 5, 1, 2, 4, 5, 8, 1, 2, 5, 7, 8, 9, 8, 3, 10, 3, 1, 2, 3, 5, 6, 8, 10, 3, 4, 1, 2, 6, 7, 8, 9, 10, 4, 1, 4, 5, 1, 2, 3, 10, 2, 5, 6, 9, 1, 2, 3, 8, 3, 6, 2, 3, 4, 6, 7, 10, 10, 10, 7, 3, 7, 6, 1, 2, 3, 4, 5, 6, 8, 9, 6, 6, 1, 2, 4, 5, 8, 2, 3, 6, 1, 3, 4, 6, 7, 8, 10, 1, 4, 1, 3, 4, 5, 6, 7, 8, 9, 1, 4, 5, 6, 9, 1, 2, 4, 5, 2, 9, 10, 3, 2, 3, 7, 7, 1, 2, 3, 4, 7, 10, 6, 6, 1, 3, 5, 8, 2, 7, 10, 2, 5, 6, 7, 8, 10, 1, 2, 4, 10, 10, 1, 7, 8, 9, 8, 2, 2, 7, 2, 10, 1, 9, 10, 10, 10, 3, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 5, 2, 3, 5, 1, 4, 3, 3, 5, 7, 3, 4, 4, 1, 2, 3, 4, 8, 2, 4, 7, 9, 10, 1, 2, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 1, 2, 5, 6, 1, 2, 3, 5, 9, 6, 9, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 6, 1, 3, 5, 6, 1, 5, 2, 9, 10, 2, 4, 2, 3, 4, 10, 2, 6, 8, 1, 2, 3, 4, 7, 9, 1, 2, 4, 5, 6, 7, 8, 7, 1, 9, 10, 3, 1, 1, 2, 3, 6, 8, 9, 3, 2, 3, 5, 8, 9, 10, 2, 1, 2, 3, 5, 6, 7, 8, 10, 3, 5, 6, 8, 10, 3, 1, 2, 3, 5, 6, 1, 2, 5, 9, 4, 10, 1, 2, 3, 5, 6, 8, 9, 9, 1, 3, 4, 7, 1, 2, 6, 3, 4, 1, 2, 3, 5, 8, 10, 1, 2, 4, 6, 7, 8, 1, 1, 2, 1, 4, 8, 1, 2, 3, 4, 6, 7, 10, 8, 2, 8, 1, 3, 4, 5, 7, 9, 10, 2, 1, 2, 3, 4, 1, 2, 3, 3, 4, 10, 1, 2, 4, 10, 7, 10, 4, 7, 1, 2, 4, 6, 7, 8, 1, 4, 6, 2, 3, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 10, 2, 2, 2, 8, 2, 3, 4, 5, 6, 8, 10, 6, 8, 2, 3, 4, 10, 8, 8, 9, 6, 2, 3, 5, 5, 7, 1, 2, 3, 4, 6, 7, 10, 4, 7, 4, 10, 3, 10, 1, 3, 4, 5, 6, 7, 9, 10, 8, 5, 7, 3, 4, 6, 7, 2, 9, 1, 3, 5, 10, 3, 6, 2, 3, 4, 5, 7, 8, 10, 1, 3, 7, 6, 7, 5, 1, 2, 3, 4, 5, 9, 10, 7, 1, 2, 4, 6, 7, 8, 10, 5, 6, 8, 10, 1, 2, 4, 5, 6, 7, 5, 6, 1, 4, 6, 8, 8, 3, 5, 2, 3, 6, 3, 5, 6, 8, 10, 1, 2, 3, 5, 9, 5, 1, 8, 9, 5, 2, 8, 8, 9, 3, 2, 3, 4, 8, 9, 3, 4, 7, 1, 10, 1, 2, 4, 5, 7, 10, 5, 10, 7, 3, 1, 2, 4, 5, 6, 8, 9, 9, 7, 1, 2, 3, 5, 6, 8, 10, 3, 6, 7, 1, 7, 9], \"Freq\": [0.990563427073411, 0.016336697571126345, 0.9802018542675807, 0.0037133830025618867, 0.028564484635091435, 0.9674790945905469, 0.19552738133873443, 0.5319974167258066, 0.27210893902973876, 0.09263496868985999, 0.09263496868985999, 0.5376968294344386, 0.06158414119605217, 0.170779551215943, 0.044506186074457874, 0.013889928553393683, 0.9566688291149898, 0.028937351152903506, 0.9953563417538492, 0.9970551591395477, 0.9926854929653843, 0.9970549054131496, 0.9963313635036163, 0.1617026119129599, 0.8379135344580648, 0.9977005429831923, 0.042047301254833164, 0.9541502977058295, 0.9990031046644858, 0.9969525672016694, 0.9938584639649419, 0.9962495609470168, 0.9940293305308725, 0.2663272514299327, 0.002002460537067163, 0.7308980960295146, 0.9987750292142287, 0.9990358609648975, 0.9970239375224349, 0.9964819614821, 0.997872178290005, 0.995402289556339, 0.07064944131236615, 0.4988715651852794, 0.4291833407615169, 0.0009612168886036212, 0.996662149124642, 0.9990767481329718, 0.2422422668960437, 0.7571775171647445, 0.9941238639714615, 0.10681565635534963, 0.16172090027632374, 0.006987940135396705, 0.7237509425946588, 0.1058542280432071, 0.8898371044882096, 0.9963608857979785, 0.060047127427953116, 0.40247804330087494, 0.13388886521097654, 0.023531982370414057, 0.05842423209206249, 0.014606058023015622, 0.30672721848332807, 0.9982916948680746, 0.9978615087163778, 0.9973111056605253, 0.9976209560225916, 0.9970760303013676, 0.7062550568808977, 0.2932219655800156, 0.9960820168590415, 0.9988574951340832, 0.9957275622949956, 0.1496579926321181, 0.47289472262034044, 0.006746876717021718, 0.18400572864604686, 0.036187793300389216, 0.009200286432302343, 0.14168441105745608, 0.08480998218981553, 0.10284045084434323, 0.8120388845890999, 0.9891161941477837, 0.010586870579563435, 0.9977015230309478, 0.03364302542667495, 0.1513936144200373, 0.8145690095731299, 0.0335960787576248, 0.8415817728785012, 0.12430549140321176, 0.9989881769410305, 0.9929006356185351, 0.9976782079426416, 0.023753076153821547, 0.5357638288028638, 0.43943190884569866, 0.9961777465402771, 0.9882431550458847, 0.9941231129074498, 0.997968795578118, 0.9888178786085413, 0.010339350441724291, 0.03337398737572576, 0.5419645341231987, 0.0065296931822072125, 0.2089501818306308, 0.11318134849159169, 0.061669324498623675, 0.034099508840415446, 0.9958994459106366, 0.002508646863250435, 0.870500461547901, 0.12626855878360524, 0.9927601872153033, 0.9978479242389018, 0.9956294072020673, 0.9680317634067894, 0.031268859114082025, 0.9315642748174617, 0.06680540616220841, 0.029221318708877463, 0.9691737371777692, 0.9703458088089453, 0.0043319009321827915, 0.0023325620404061184, 0.022659174106802293, 0.99541538290558, 0.12302540860306369, 0.7110451581973681, 0.15117529023257825, 0.014596234919007555, 0.046207492457219904, 0.10473698290303178, 0.8471373617156982, 0.019191434736423447, 0.9787631715575958, 0.9983622817217768, 0.9958537201712483, 0.24762131813995208, 0.7518307430990189, 0.996232312050541, 0.9882315719702701, 0.05027804709135656, 0.9374760863909192, 0.011871205563236965, 0.9904001893004222, 0.9879770416048058, 0.9954605500243489, 0.9983256116907787, 0.9963015479282826, 0.010109704745784695, 0.05017409021982034, 0.03706891740121055, 0.9025719736931114, 0.08206403006368428, 0.014328640169849636, 0.902704330700527, 0.9942637390725776, 0.9941373974293598, 0.9990107828705637, 0.9905908635804859, 0.028986933959593467, 0.06763617923905142, 0.04705125512281838, 0.2465989889026285, 0.0012603014765040638, 0.005041205906016255, 0.0474713556149864, 0.5553728506461241, 0.1573768780581465, 0.6063463867825192, 0.0029693750577008773, 0.1698482533004902, 0.054636501061696145, 0.008908125173102632, 0.041690150899306964, 0.7878000928558695, 0.06181643064379998, 0.10781935577406973, 0.10075843418643984, 0.6235964067026613, 0.1628108845085766, 0.09768653070514595, 0.014745136710210709, 0.9942334828416343, 0.029688305357047792, 0.04775944774829428, 0.043887060093027175, 0.007744775310534207, 0.8183645911464479, 0.05163183540356138, 0.0180428142405603, 0.2503440475877742, 0.7284786249626222, 0.9942246573770647, 0.6254092210933402, 0.16707865389983503, 0.0873250970533881, 0.1196303352696704, 0.9937022055959154, 0.0347925559311518, 0.08863817820555339, 0.23692073800736702, 0.4456760735942778, 0.19384424018784574, 0.04464167691997806, 0.3366726467715012, 0.05270197969719632, 0.37077392775204, 0.04898183995386481, 0.14570547328048394, 0.9112386714024878, 0.08890133379536466, 0.9887799579660865, 0.0865886528490485, 0.050903632280955784, 0.3484537302531406, 0.03516024085385606, 0.037784139425039345, 0.114926757417828, 0.008921255142023177, 0.019941629140992987, 0.29702531825794815, 0.9972253662792169, 0.4741312631323642, 0.08367022290571133, 0.020419518685322408, 0.03536062991848515, 0.10558385271435002, 0.2271048907440736, 0.007470555616581369, 0.003486259287737972, 0.043329222576171944, 0.021677077990588454, 0.9772749327423629, 0.9087642245076222, 0.09048690926518362, 0.0005992510547363153, 0.1119389280870888, 0.16332073114345744, 0.11744412127169973, 0.6074063147020721, 0.005346900054498222, 0.9936322601275863, 0.1984334358605933, 0.7982954316230765, 0.256028048040499, 0.04581554543882614, 0.025872308012513584, 0.49211285865468546, 0.054439648109664, 0.008085096253910495, 0.09055307804379754, 0.026950320846368316, 0.9994977156561713, 0.9985858554433105, 0.9917219448372065, 0.8423805667322379, 0.1532362263538763, 0.004186782140816292, 0.010406667500449242, 0.014569334500628939, 0.05099267075220129, 0.009366000750404318, 0.22062135100952393, 0.1717100137574125, 0.5099267075220129, 0.013528667750584015, 0.9941520341357746, 0.11572369723218308, 0.8828064903140823, 0.07655821449068877, 0.06281699650518052, 0.8578446085238716, 0.9424588298465125, 0.056062663511770035, 0.9966422507218058, 0.0017193263262353894, 0.030518042290678164, 0.4539021501261428, 0.4440160237502893, 0.06963271621253327, 0.12602574017614562, 0.045009192920052, 0.8281691497289568, 0.9991243853296099, 0.06343127214789641, 0.7671219475386223, 0.06937795391176171, 0.028742295192015564, 0.07036906753907259, 0.0032524713971304126, 0.9967017903650753, 0.9965379085135851, 0.20252423095687463, 0.021243800450021815, 0.7746905897441289, 0.997658827036134, 0.9852963681746045, 0.002220803234052452, 0.0014805354893683014, 0.010733882297920185, 0.6048953923442064, 0.00767309165764321, 0.044120277031448454, 0.1668897435537398, 0.17520225951618662, 0.06643791072957765, 0.918159955488037, 0.014364953671260032, 0.0005985397363025013, 0.8371325144659205, 0.11786342261433538, 0.043821016100201615, 0.9953491059589764, 0.9970737657498594, 0.839842709124665, 0.07370959947104773, 0.08599453271622234, 0.02693634228531245, 0.038720992035136646, 0.1919214387828512, 0.2878821581742768, 0.4545507760646476, 0.004031323238675441, 0.09272043448953515, 0.8989850822246235, 0.34733379329241115, 0.32084223278705776, 0.02452922269014203, 0.012755195798873856, 0.2933695033740987, 0.9983050939522405, 0.9956477451932804, 0.014811197668194376, 0.982476111990227, 0.563283297456499, 0.05651884269856642, 0.019686338468039986, 0.00190512952916516, 0.05588379952217803, 0.0527085836402361, 0.2108343345609444, 0.00381025905833032, 0.03492737470136127, 0.9967160801875437, 0.9986736250647101, 0.07527278558103975, 0.16936376755733945, 0.7527278558103975, 0.21728773370474294, 0.18668382754914534, 0.036724687386717116, 0.10527743717525574, 0.3029786709404162, 0.07344937477343423, 0.07222521852721032, 0.00612078123111952, 0.8697789092142287, 0.0825174862587858, 0.04794935012334851, 0.9952679419790343, 0.8729098239762718, 0.07684637012066374, 0.04978778909226101, 0.06130069350821995, 0.261613983318545, 0.3658734305451238, 0.15542380558777028, 0.017859257163812113, 0.08736555531486466, 0.04344143634440784, 0.00724023939073464, 0.9977877932901865, 0.9975249847236703, 0.963940578077308, 0.035779453771428466, 0.9944924567580192, 0.16176055437551312, 0.2560438489258122, 0.1965775879839569, 0.37805752422619926, 0.0077028835416911015, 0.9948941356700647, 0.12549088346157022, 0.015314141710564501, 0.005530106728814959, 0.8533380075386775, 0.9991160664414411, 0.01622989755581533, 0.9623374550742269, 0.02100339683693749, 0.0016727213130661844, 0.8581060336029526, 0.1405085902975595, 0.9923158620542728, 0.0013929593266723695, 0.192228387080787, 0.20824741933751922, 0.04039582047349871, 0.17969175314073565, 0.3781884571915483, 0.00362165784331255, 0.9959559069109513, 0.28842914950864, 0.11881314397372955, 0.025401430780590456, 0.13683996581801955, 0.4301855212841932, 0.9976532256214661, 0.020475193638426828, 0.09254787524568926, 0.17772468078154488, 0.6265409253358609, 0.07780573582602195, 0.004095038727685366, 0.6661376279458792, 0.025684411027763997, 0.13090764330279714, 0.17647675964237844, 0.0975853877685331, 0.8693970910287494, 0.020162270200110143, 0.012097362120066086, 0.07850688662894771, 0.28604489385596793, 0.5619849409181109, 0.07306581527842658, 0.9985127200722667, 0.9970634491153013, 0.6413752158358281, 0.3579492088379189, 0.00039640000978728566, 0.12578631526859838, 0.10078530850713784, 0.4617373436257245, 0.027344851145347477, 0.1281301596524853, 0.15625629225912843, 0.9966970618643494, 0.998419194086917, 0.6369007055924422, 0.14764026508657288, 0.0899851250710134, 0.04795614449892331, 0.06304347085813514, 0.014548493274954263, 0.651691913706105, 0.034817454158514324, 0.10596616483026099, 0.19830810846805985, 0.002270703532077021, 0.006812110596231063, 0.1547456796181418, 0.03981760177308912, 0.6940931945445308, 0.11130829586568095, 0.021827850951253856, 0.035860040848488484, 0.8777914346825658, 0.06392442064295772, 0.5544913439131393, 0.2549938501292528, 0.02044762005753442, 0.07517507374093538, 0.05172045073376354, 0.04269944188485129, 0.030518680329783252, 0.059002781970914284, 0.7355001959477764, 0.09053875164502365, 0.027466812296804925, 0.055950913937935956, 0.10600949763795549, 0.8921106955071023, 0.03107276528562818, 0.951603436872363, 0.01683108119638193, 0.1563651606424541, 0.821627844103077, 0.021322521905789194, 0.9952942153913921, 0.533290006180364, 0.2171337385342869, 0.1712016015366493, 0.0011930425194191588, 0.07635472124282616, 0.03876045395460576, 0.7219134549045323, 0.2374077804719603, 0.9981453384195635, 0.09524413226962648, 0.17455099937859833, 0.5213762459699872, 0.1070073517929668, 0.1016949300727486, 0.15405959807372133, 0.24715797884515295, 0.34787651177937073, 0.0016565548179970037, 0.183546273834068, 0.0460522239403167, 0.01954734685236464, 0.04650044547339202, 0.2536806480776139, 0.6712638564376789, 0.0009208009004632084, 0.028084427464127856, 0.9984955594147589, 0.9984728856642746, 0.946536548988721, 0.05230595169757012, 0.997779748387013, 0.06789918164924086, 0.10939312599044361, 0.016346099285928357, 0.04023655208843903, 0.7519205671527044, 0.007544353516582318, 0.007544353516582318, 0.9971639019623932, 0.8847816785381911, 0.11299621436752802, 0.8801103357416525, 0.00274462682663717, 0.11710407793651927, 0.9970770422202742, 0.08653175655375259, 0.0023870829394138644, 0.013725726901629721, 0.7471569600365396, 0.017306351310750517, 0.008354790287948525, 0.12472508358437442, 0.9922655192263384, 0.9936119084392366, 0.9949376466931821, 0.035257664887725915, 0.9625342514349176, 0.9947014116289618, 0.005089846404534161, 0.9978145909691322, 0.9962081910932563, 0.009732001364518877, 0.9887713386351179, 0.05638438367291472, 0.014457534275106338, 0.6240835628754237, 0.04433643844365944, 0.19903205518729727, 0.02265013703099993, 0.038553424733616906, 0.38431599878437867, 0.1114516396474698, 0.4868002651268796, 0.017294219945297038, 0.041242353691425974, 0.05020808275477945, 0.04841493694210875, 0.8607099900819334, 0.9946082799910719, 0.9910407373595124, 0.9811477020642483, 0.01528267448698206, 0.9476302012515605, 0.04940252234012874, 0.9908481320842647, 0.9955163059891448, 0.9916866000376072, 0.996240366293385, 0.9969449313458972, 0.023107062231255204, 0.9767318527275016, 0.24155495068997032, 0.04801092187626739, 0.7096614389835774, 0.047049346081275256, 0.01075413624714863, 0.810593019628828, 0.13039390199667714, 0.9965071663692098, 0.019975998044272882, 0.37993564907734695, 0.35643447490761415, 0.07559544357930718, 0.0763788160516316, 0.09165457926195793, 0.995959747967921, 0.05865748478104022, 0.9385197564966435, 0.02183381281891836, 0.971604670441867, 0.9989875137062694, 0.9962078326774271, 0.22281792640810852, 0.005425761195002642, 0.5451081413912655, 0.037980328365018495, 0.09947228857504845, 0.08934420101104351, 0.16811992256435426, 0.23894491121912478, 0.5437069836123798, 0.04793206302898611, 0.15070390144684345, 0.8276155921122486, 0.020093853526245792, 0.09365964020367724, 0.022745912620893045, 0.4121024168961799, 0.4147784066162849, 0.05619578412220634, 0.995219465683167, 0.9964586513399968, 0.3531954431900406, 0.6452609058279588, 0.9957481940967534, 0.9981719390538234, 0.9965909703765699, 0.8442834697642759, 0.1439928178477496, 0.011720345638770314, 0.9961630257244581, 0.7721888953961639, 0.2278828131150333, 0.9991329738184187, 0.05366853546472647, 0.0454118377009224, 0.9004960998648817, 0.9972321539751205, 0.9948337426337515, 0.08505565782173441, 0.18987172099599298, 0.24313890064192764, 0.13144965299722589, 0.032647626234605126, 0.31702563369919184, 0.9954874581767027, 0.9967818516479129, 0.9922474878983688, 0.9973053801327273, 0.1621554491156895, 0.16580759887054738, 0.061356115881612244, 0.0029217198038862975, 0.019721608676232506, 0.40465819283825216, 0.06427783568549854, 0.11248621244962245, 0.006573869558744169, 0.6441447968492936, 0.018603459836802703, 0.32183985517668673, 0.01534785436536223, 0.9952008021216886, 0.9984979131939067, 0.8893351818541668, 0.0628999342765226, 0.01572498356913065, 0.020966644758840868, 0.008736101982850361, 0.9968406321839525, 0.0027962043110058665, 0.8481819743384462, 0.006524476725680356, 0.14167435175763057, 0.13155613144993397, 0.15048507122690288, 0.6729238090712449, 0.0227147277323627, 0.012303810855029795, 0.009464469888484458, 0.9902043445658549, 0.996451132444889, 0.9946480665714007, 0.5535493228548104, 0.2502658627531017, 0.01368196060609212, 0.02736392121218424, 0.03306473813138929, 0.12256756376290857, 0.9951547697563258, 0.16869822862020528, 0.005295460091217206, 0.8239735901933972, 0.0019668851767378195, 0.007177165168008675, 0.9689172976811711, 0.021531495504026023, 0.9976923079465053, 0.9926299545112369, 0.9273160029649877, 0.07070444597387003, 0.9874283576164306, 0.993452243041082, 0.18400571320613368, 0.7847200170000711, 0.004869716417553633, 0.01826143656582612, 0.008000248400266682, 0.9970889016934926, 0.003963722453707054, 0.01109842287037975, 0.02378233472224232, 0.9600135782878484, 0.996609281090823, 0.9992398397555307, 0.7518191163157553, 0.00042741280063431227, 0.12736901458902505, 0.12053040977887607, 0.9963516334775324, 0.037495406994439893, 0.07519132418671101, 0.2083300955466473, 0.6787270196587114, 0.9923916049211032, 0.9980664125458144, 0.998099421323116, 0.6001067841883676, 0.399783368459301, 0.07634737515534906, 0.8665427080132118, 0.05726053136651179, 0.4948922532756375, 0.2221860012102081, 0.21909292462723534, 0.06340806995094106, 0.2232082936425264, 0.0028708462204826545, 0.022249058208740572, 0.024402192874102564, 0.10119732927201357, 0.1988061007684238, 0.05669921285453242, 0.37033916244226245, 0.9915969264955733, 0.9901218971447374, 0.0030590789819095903, 0.00577826029916256, 0.0006797953293132423, 0.9917268707614045, 0.05496977588621032, 0.028045804023576693, 0.0314113005064059, 0.06843176181752714, 0.1110613839333637, 0.11218321609430677, 0.5934492131388829, 0.08054404222582562, 0.5614039958128443, 0.2620686747049252, 0.03846879628696149, 0.042676320880847905, 0.015026873549594333, 0.9923513137648495, 0.15670614727061735, 0.17668881034539405, 0.6436520948296498, 0.0031551573275963225, 0.01998266307477671, 0.9308190797898704, 0.06839351661538474, 0.9968446927766114, 0.9986854326645073, 0.9978380718980799, 0.9983910377656264, 0.996555235348362, 0.9917927092502896, 0.6409771741260494, 0.2924115954613495, 0.06574124862831277, 0.9826828595512057, 0.016004606833081525, 0.4666352713227564, 0.027449133607220965, 0.5053869893564801, 0.38403007935604117, 0.2639545586758465, 0.2904029112485766, 0.06136017796873385, 0.9921947629300789, 0.9922914187316487, 0.9977843565266076, 0.9981926491888855, 0.9973146138764737, 0.9963191745838846, 0.9959986875621119, 0.026521285634562228, 0.0019500945319531051, 0.03432166376237465, 0.007020340315031178, 0.857261556246585, 0.04836234439243701, 0.024571191102609125, 0.9995942580192686, 0.09414065804073114, 0.1066233972284524, 0.18932154434710574, 0.5383181274704792, 0.001560342398465157, 0.06293381007142801, 0.007281597859504067, 0.050105093784103966, 0.718797373444856, 0.12643341422157076, 0.09178129328676989, 0.013111613326681412, 0.9965055384417811, 0.9951522688944066, 0.912379769507907, 0.009386623143085464, 0.0769703097733008, 0.9954193816307185, 0.9915101714961475, 0.9883067494369799, 0.009577329662825746, 0.0018417941659280282, 0.9933155793391901, 0.010637234479479354, 0.06914202411661581, 0.9183479100617177, 0.9952938903408048, 0.2576549727507843, 0.7424531451634443, 0.9909377333949619, 0.96587402528786, 0.03219580084292867, 0.06316334925657122, 0.006316334925657121, 0.7642765260045117, 0.08632324398398065, 0.08000690905832354, 0.9952924705905608, 0.937296539198497, 0.04636802667413059, 0.01573200905015145, 0.012504319852074558, 0.9419920955229502, 0.04445980391848732, 0.011383352946142913, 0.9865572553323858, 0.9963461625119, 0.9942152587185223, 0.9980951813536392, 0.9956615964279543, 0.9981672877161422, 0.997422686795619, 0.9928232162992057, 0.05776727784427793, 0.039937871102216846, 0.11767408449760319, 0.049922338877771054, 0.22679005375901706, 0.004992233887777106, 0.13051125735188718, 0.37156483650455313, 0.09819387282273713, 0.04432751973140705, 0.17787118677032954, 0.05049970602312195, 0.41016983447669053, 0.024127637322158266, 0.11222156894027101, 0.03198314714797724, 0.02356652947745691, 0.028055392235067752, 0.038554777891374674, 0.17590617412939696, 0.00883546993344003, 0.19919968577210248, 0.5767152192918128, 0.991134386514096, 0.016227972535865928, 0.7105533688918438, 0.26486369460324033, 0.008113986267932964, 0.9285503126773854, 0.05587401167564074, 0.01538902863947732, 0.9986167817812595, 0.020851486705191703, 0.9781242854435381, 0.15589762659593803, 0.21714312275862796, 0.1213774378496946, 0.08797080357913646, 0.05790483273563412, 0.35745098669497216, 0.07405379058937332, 0.9256723823671665, 0.9802028321261964, 0.01957143757989078, 0.269167895431424, 0.2062018341787159, 0.0076905112980406864, 0.1893788407142519, 0.04758503865662675, 0.2297540250289655, 0.04998832343726446, 0.9040038660057659, 0.03329478524351593, 0.06255383894236326, 0.9967110732315179, 0.9837766577099132, 0.001811076321262727, 0.01412639530584927, 0.23926795430822492, 0.0402728239924735, 0.017767422349620664, 0.2167625526653721, 0.07699216351502287, 0.3885143020450385, 0.021320906819544794, 0.9932198178148991, 0.9998396950108384, 0.043269661892881395, 0.49028792947638145, 0.08074972113812373, 0.09903267686751023, 0.046012105252289374, 0.18953330772797344, 0.051192276042282214, 0.9946450649510259, 0.05448621308590565, 0.062320831961003194, 0.02599668990373276, 0.0064101427159889, 0.04237816573348217, 0.7831057684699773, 0.012108047352423478, 0.01317640447175496, 0.07647422583802306, 0.21708812495954935, 0.03782596116719421, 0.03700365766355955, 0.055094334743521996, 0.4341762499190987, 0.10525484846523606, 0.03782596116719421, 0.07556237848834083, 0.7748708058191176, 0.04989968390739488, 0.09908651518754126, 0.9974640811850666, 0.9948938940631203, 0.9869417750566596, 0.35859881042070985, 0.6406428186167737, 0.9931000962920393, 0.03575758094047319, 0.07008485864332746, 0.08152728454427888, 0.7680728386013641, 0.044339400366186756, 0.9150198507630819, 0.019427173052294733, 0.062166953767343144, 0.9968861675381567, 0.9973483601053584, 0.006155943871033658, 0.25239369871238, 0.017588411060096166, 0.1371896062687501, 0.5856940883012023, 0.048903570009118054, 0.10867460002026233, 0.0007762471430018738, 0.5837378515374091, 0.16145940574438974, 0.09625464573223236, 0.9971249316720591, 0.0765685686941959, 0.9216586972449506, 0.9957394840708824, 0.11414571591465263, 0.002730758753939058, 0.0065538210094537395, 0.0469690505677518, 0.010923035015756232, 0.7842739141312975, 0.034953712050419944, 0.9501395494897273, 0.04835315773484618, 0.09847098345236573, 0.07278290081261814, 0.1644037288943845, 0.169541345422334, 0.4075842445506616, 0.0625076677567191, 0.02483181321842266, 0.9970679872889687, 0.1136001655627696, 0.131077114110888, 0.7544216123271109, 0.03851975725437204, 0.8431546865679213, 0.08131948753700764, 0.03744976399730615, 0.8822236661703663, 0.06712571373035396, 0.008790272036117779, 0.040754897622000616, 0.026977438187325006, 0.9037441792753876, 0.018884206731127503, 0.04990826064655126, 0.9911957898813643, 0.006521024933430028, 0.1574153461840937, 0.012158095843486911, 0.39737776414765114, 0.14397745077813448, 0.2892347011187413, 0.9967106440008376, 0.9960468377069763, 0.9925730461730919, 0.9967328814738015, 0.999055423711203, 0.9881546394047577, 0.9975245144007611, 0.05717163369252833, 0.07728757888064015, 0.042349358290761724, 0.11222579947051857, 0.032820752675340334, 0.6341816404041568, 0.035996954547147464, 0.006352403743614259, 0.9980017981132007, 0.9997972756819148, 0.28838724989319725, 0.0439447237932491, 0.5538866228107439, 0.041198178556171035, 0.0714101761640298, 0.010126157670529302, 0.9699412454414139, 0.019529018364592227, 0.010539841329723874, 0.35890933370164985, 0.19914752828267743, 0.034947894935400216, 0.3761059169238309, 0.002773642455190493, 0.018306040204257256, 0.9920388131844533, 0.007596538075202914, 0.06706365496314434, 0.04746043274314831, 0.23678628944679428, 0.20428621050311663, 0.06551603215630256, 0.03198420467473038, 0.33531827481572174, 0.011865108185787078, 0.7321492311102843, 0.1431997760848056, 0.006460140274502509, 0.033377391418262965, 0.08398182356853262, 0.0028730426549178883, 0.060333895753275656, 0.04165911849630938, 0.8935162656794633, 0.2232563281879296, 0.7757902545708422, 0.9887414345484825, 0.9966066480109578, 0.9950352831278482, 0.04592566238390469, 0.9529574944660223, 0.9942942388065088, 0.0474030350688046, 0.7363271447354315, 0.034762225717123375, 0.03897582883435045, 0.06531084831701967, 0.07689825688939413, 0.9986322184013391, 0.9971988292101668, 0.6614151466748331, 0.11347551364715167, 0.03519842321462575, 0.18912585607858612, 0.06158126426873134, 0.9377147059102272, 0.9934089869590237, 0.5611833791164487, 0.05510327753598204, 0.28059168955822433, 0.07467944192376513, 0.018851121262309648, 0.010150603756628272, 0.0045242737672170534, 0.7982111575018658, 0.09436342428766996, 0.10276564699821591, 0.9941348211363807, 0.05263775757627751, 0.21322752645305637, 0.7155166368843147, 0.017843307652975426, 0.9987844134020434, 0.9993121869630653, 0.9954095019857939, 0.9947108413649083, 0.9970406844972237, 0.9865818871782311, 0.9957251879508249, 0.03489538263051869, 0.9596230223392639, 0.9892523564955339, 0.9941661703700735, 0.14338084115876853, 0.7627860749646487, 0.06021995328668279, 0.03441140187810445, 0.04118795220570782, 0.06472392489468372, 0.548518919056966, 0.026804857784666992, 0.14840738334437578, 0.1078732081578062, 0.035957736052602066, 0.026804857784666992, 0.993928910691938, 0.7434786739712234, 0.007271185075513187, 0.24940164809010232, 0.99108491433399, 0.9951920566560738, 0.9953283751982759, 0.996451253351635, 0.9915127776796224, 0.9929818115872909, 0.07870357873992356, 0.9187129287525693, 0.9977324085863463, 0.22155665118559098, 0.6045222716197912, 0.0974544719991603, 0.0022840891874803195, 0.07385221706186366, 0.9994643060966495, 0.08279522372496341, 0.44157452653313817, 0.44324715731546066, 0.03177998486412737, 0.09688778867719347, 0.4453282763970085, 0.008888787952036098, 0.08799900072515737, 0.05955487927864185, 0.023999727470497462, 0.019555333494479413, 0.2568859718138432, 0.09490777441766717, 0.006123082220494656, 0.3748687003880617, 0.13402746638193858, 0.08300178121114977, 0.04592311665370992, 0.09218640454189177, 0.16838476106360303, 0.001020513703415776, 0.9931616247957475, 0.23833387771627756, 0.3290780674813373, 0.43259945637998304, 0.9995935415537898, 0.9965986115960561, 0.48660002312742606, 0.008772216747790289, 0.4509951433863949, 0.05366532598648177, 0.9988767487872059, 0.996254241562327, 0.9944864416268833, 0.9911369789657022, 0.01954111553086131, 0.04061486757394704, 0.8812659945290395, 0.03831591280561042, 0.02030743378697352, 0.026540772618205203, 0.12891232414556814, 0.03601961998184992, 0.009478847363644715, 0.19431637095471666, 0.5298675676277396, 0.07488289417279324, 0.9995380966388325, 0.9968055756758899, 0.9973936839259396, 0.987258452789671, 0.012079914327793526, 0.0023562838421425855, 0.9967080652263136, 0.1251167841729082, 0.778226397555489, 0.09508875597141023, 0.9955384567058037, 0.9961370265509575, 0.13813925046533831, 0.7642844511727129, 0.08649840916988474, 0.011619189291477055, 0.026209635843317796, 0.9548807869403348, 0.018417581943953047, 0.1037977937556178, 0.4715069992941362, 0.2666719914040606, 0.05355524465050493, 0.05631582427166498, 0.04803408540818484, 0.013934397534854103, 0.003583130794676769, 0.20742346044739965, 0.3455730588643817, 0.10032766225094954, 0.27032731217616957, 0.05892259529024021, 0.9980927795934597, 0.05921680254093545, 0.9305497542146999, 0.004229771610066818, 0.9953343723207991, 0.9985163206952529, 0.7720438728156795, 0.05948233174195466, 0.013490838127041263, 0.08155824867711309, 0.05028403301897198, 0.023302356764889454, 0.9989048254027189, 0.0056448721105420915, 0.4374775885670121, 0.23256873095433417, 0.12305821200981759, 0.1292675713314139, 0.07225436301493877, 0.9969404173153548, 0.01142897831943158, 0.059509507801178224, 0.22621495018461127, 0.13478312362915862, 0.3239524199507848, 0.04926283758375681, 0.11980722100369656, 0.07487951312731035, 0.8639317228123786, 0.04319658614061893, 0.020659236849861227, 0.010329618424930614, 0.061038654329135446, 0.9994781654812037, 0.16304783508099482, 0.33354678090493833, 0.07451110742948688, 0.38526625547364096, 0.043391762561877656, 0.013515699565574938, 0.984393451692708, 0.7813434899935348, 0.2168946811687739, 0.998338280446901, 0.999276123162374, 0.11102399698113301, 0.19471901008998713, 0.01878867641219174, 0.19870448690469444, 0.3877299586879568, 0.022774153226899078, 0.0666143981886798, 0.997923401519185, 0.009903218134831622, 0.8543176177648079, 0.1287418357528111, 0.006602145423221081, 0.006251797543661478, 0.9502732266365446, 0.041678650291076515, 0.9477270987135838, 0.05189934112002959, 0.005127579767787884, 0.0789647284239334, 0.6260774896469006, 0.0220485930014879, 0.10050056344864251, 0.16613358447632742, 0.17347656887421906, 0.11104710300547142, 0.2414307662357762, 0.10110258631841429, 0.3701570100182381, 0.0022098925971238094, 0.9971483726369728, 0.02580187722774061, 0.971870708911563, 0.040551150692087125, 0.09124008905719604, 0.8677946248106645, 0.2551376103547566, 0.5812306508081734, 0.05359399507451988, 0.003019380004198303, 0.016606590023090666, 0.06038760008396606, 0.029438955040933457, 0.9925974593252015, 0.9597271203396659, 0.03746682451776473, 0.3036121718595014, 0.08314175278441387, 0.39874950091629335, 0.052532351261793835, 0.03350488545043544, 0.004550046172281356, 0.12367852777382958, 0.9971966759518222, 0.044241602851561444, 0.8993730034230009, 0.024750547049824583, 0.031247565650403538, 0.007340131548896155, 0.9892866187567817, 0.003262280688398291, 0.4852807057555482, 0.5140744958220858, 0.9914668049963024, 0.21130578543167297, 0.7437134997055941, 0.029002754863170797, 0.014501377431585399, 0.02778362922894775, 0.9714689668328627, 0.9997843426002604, 0.9938748288930354, 0.16985595369500894, 0.2766835975283479, 0.03525312246500186, 0.010682764383333897, 0.4946119909483594, 0.013887593698334066, 0.991856079708822, 0.9995252132232552, 0.9945895777915733, 0.04285204269730895, 0.005713605692974526, 0.10998690958975962, 0.667063464654776, 0.012855612809192684, 0.15998095940328674, 0.0519637475447001, 0.03632417304095541, 0.12562109843330413, 0.001009004806693206, 0.0867744133756157, 0.620033453712975, 0.0025225120167330145, 0.04136919707442144, 0.01917109132717091, 0.015135072100398087, 0.046250110187252495, 0.9502295365744603, 0.9972537055260857, 0.9960311054448292, 0.9982576296801623, 0.9937542038790378, 0.12340743895770141, 0.11449715455642333, 0.6508962755133638, 0.0775194742911193, 0.013810940821981024, 0.000445514220063904, 0.019602625682811778, 0.09079505609858639, 0.9079505609858638, 0.9536260522610118, 0.03217756618058629, 0.008775699867432624, 0.0039003110521922775, 0.9969488636611169, 0.997428757887464, 0.9949422426679452, 0.997076632827696, 0.995574138291853, 0.0772678149412839, 0.9220130609820512, 0.9982066376051052, 0.9936003983153919, 0.020667235659725607, 0.015500426744794207, 0.020667235659725607, 0.022389505298036076, 0.034445392766209344, 0.18772739057584095, 0.6992414731540497, 0.8943891774386502, 0.10492833925257906, 0.99515711171617, 0.9903583303829007, 0.9955915418872384, 0.9952434931775028, 0.05956694043848844, 0.27561784332619516, 0.10239074086183418, 0.31586577605490357, 0.039603965805049074, 0.15551801206372928, 0.03670611464858207, 0.014489255782335026, 0.996800178156401, 0.007059457234822814, 0.988324012875194, 0.13414890819072603, 0.690307923398111, 0.06777314632552305, 0.1068999112144848, 0.7300532370694355, 0.26865959124155225, 0.1722563419387999, 0.10032512222809226, 0.11452207348678456, 0.6123618309582612, 0.9972210822063247, 0.9960592283905677, 0.11212697948831177, 0.06397833535509553, 0.13916936865902224, 0.2499772072121774, 0.4122315422364403, 0.02242539589766235, 0.9948363997028501, 0.9779999106254165, 0.021559116266291745, 0.9936173800815323, 0.2772236496916525, 0.7207814891982964, 0.9926594012359476, 0.1756326076366784, 0.07334848364903428, 0.4966163388347458, 0.036337780890347254, 0.06661926496563664, 0.04710453078378348, 0.10497581146100318, 0.991578288134961, 0.2601707764354517, 0.015304163319732455, 0.008927428603177266, 0.5515875529820239, 0.1243463269728262, 0.0038260408299331137, 0.035072040941053546, 0.012245436615506288, 0.01653133943093349, 0.9517765609402263, 0.019286562669422403, 0.004145583441355549, 0.0514052346728088, 0.7006036015890877, 0.033993784219115496, 0.12353838655239535, 0.0862281355801954, 0.9937865900004933, 0.9979797921075823, 0.6391024766804527, 0.026544192322191327, 0.23549462931995382, 0.09937056612922907, 0.9961480358685735, 0.04303954104848918, 0.9558364741185306, 0.2747571934003593, 0.299194732236449, 0.42534527055247934, 0.052132486489719895, 0.06672958270684147, 0.8737404735677055, 0.008341197838355184, 0.9875975168366072, 0.04427518934702314, 0.027246270367398854, 0.057898324530722564, 0.5165438757152699, 0.3530662535108768, 0.9971778429197549, 0.9568348788927806, 0.0015549862603891343, 0.04146630027704358, 0.9946003936203792, 0.007208408105730641, 0.9921390792796537, 0.9972243281734011, 0.9970603717221177, 0.9994738225571118, 0.055305150092408044, 0.8618984429985669, 0.022983958479961784, 0.030884694207448646, 0.02872994809995223, 0.5687666685793045, 0.4307542507080447, 0.9983659120509776, 0.09417066804169456, 0.9013478226847907, 0.19117941902204796, 0.048268071139229936, 0.24607251953332906, 0.056785966046152866, 0.43725193855537703, 0.019875088116153503, 0.9936392407905277, 0.9869518105612718, 0.9872712066884103, 0.9951745255936324, 0.014744095882838842, 0.11008924925853003, 0.10910630953300744, 0.6792113503361094, 0.030471131491200277, 0.050129926001652066, 0.003931758902090358, 0.9945854543256947, 0.9900606379679915, 0.09976586942564103, 0.008956536906042586, 0.00870774421420807, 0.01144446382438775, 0.01542514689374001, 0.02064979342226485, 0.8349482737966367, 0.1435244299141116, 0.8550821669530874, 0.9943564301595079, 0.995890806223072, 0.9883512127581243, 0.9939205076369989], \"Term\": [\"acl\", \"acoustic\", \"acoustic\", \"action\", \"action\", \"action\", \"activation\", \"activation\", \"activation\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"activity\", \"activity\", \"activity\", \"actorcritic\", \"adaboost\", \"adjacency\", \"adjustable\", \"admm\", \"adversarial\", \"adversarial\", \"adversary\", \"affinity\", \"affinity\", \"agent\", \"agnostic\", \"aij\", \"alexnet\", \"alice\", \"alignment\", \"alignment\", \"alignment\", \"amplitude\", \"analog\", \"ancestor\", \"annotation\", \"appearance\", \"apple\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"ard\", \"arm\", \"arxiv\", \"arxiv\", \"assembly\", \"assignment\", \"assignment\", \"assignment\", \"assignment\", \"asynchronous\", \"asynchronous\", \"atkeson\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"auc\", \"auction\", \"auditory\", \"auto\", \"autoencoder\", \"backpropagation\", \"backpropagation\", \"bag\", \"bandit\", \"barto\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"batch\", \"batch\", \"batch\", \"bayesian\", \"bayesian\", \"bci\", \"belief\", \"belief\", \"belief\", \"bengio\", \"bengio\", \"bengio\", \"bethe\", \"bigram\", \"bishop\", \"bit\", \"bit\", \"bit\", \"blei\", \"bleu\", \"bob\", \"boolean\", \"boost\", \"boost\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boundary\", \"boyd\", \"brain\", \"brain\", \"brain\", \"breast\", \"camera\", \"cancer\", \"carlo\", \"carlo\", \"category\", \"category\", \"causal\", \"causal\", \"cell\", \"cell\", \"cell\", \"cell\", \"cesabianchi\", \"channel\", \"channel\", \"channel\", \"channel\", \"character\", \"character\", \"character\", \"child\", \"child\", \"chip\", \"cifar\", \"circuit\", \"circuit\", \"classier\", \"classifi\", \"classifier\", \"classifier\", \"classifier\", \"clause\", \"climate\", \"clinician\", \"clique\", \"clock\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"clustering\", \"clustering\", \"clustering\", \"clutter\", \"cmos\", \"cnn\", \"coadaptation\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"coefficient\", \"color\", \"color\", \"color\", \"color\", \"column\", \"column\", \"column\", \"column\", \"column\", \"command\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"compression\", \"compression\", \"compression\", \"condence\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"confusion\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connection\", \"connectionist\", \"connectionist\", \"constituent\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"contextdependent\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"continuous\", \"contour\", \"contour\", \"convex\", \"convex\", \"convex\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolutional\", \"convolutional\", \"corpus\", \"corpus\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"cortex\", \"cortical\", \"courville\", \"covariance\", \"covariance\", \"covariance\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"crf\", \"cue\", \"cue\", \"cumulative\", \"cumulative\", \"cumulative\", \"cvpr\", \"cvpr\", \"dag\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decode\", \"decode\", \"decode\", \"decoder\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"decomposition\", \"deep\", \"deep\", \"deg\", \"delay\", \"delay\", \"delay\", \"dene\", \"density\", \"density\", \"density\", \"density\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"detection\", \"detection\", \"detection\", \"detection\", \"detector\", \"detector\", \"detector\", \"device\", \"diagnosis\", \"dictionary\", \"dictionary\", \"dictionary\", \"digit\", \"digit\", \"digit\", \"digit\", \"digit\", \"digital\", \"digital\", \"digital\", \"dimensionality\", \"dimensionality\", \"dimensionality\", \"dimensionality\", \"dimensionality\", \"dirichlet\", \"disagreement\", \"discount\", \"discount\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discrete\", \"discriminator\", \"disparity\", \"distortion\", \"distortion\", \"distortion\", \"distribute\", \"distribute\", \"distribute\", \"distribute\", \"distribute\", \"distribute\", \"distribute\", \"distribute\", \"divergence\", \"divergence\", \"divergence\", \"dnn\", \"document\", \"document\", \"document\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"dqn\", \"dropout\", \"dual\", \"dual\", \"duality\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"eccv\", \"edge\", \"edge\", \"edge\", \"edge\", \"eeg\", \"eigenvalue\", \"eigenvalue\", \"eigenvalue\", \"eigenvector\", \"eigenvector\", \"eigenvector\", \"elbo\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"endtoend\", \"endtoend\", \"energy\", \"energy\", \"energy\", \"energy\", \"energy\", \"english\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"entropy\", \"entropy\", \"entropy\", \"entropy\", \"entry\", \"entry\", \"entry\", \"entry\", \"environment\", \"environment\", \"environment\", \"environment\", \"episode\", \"erm\", \"estimator\", \"estimator\", \"estimator\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"excess\", \"excitatory\", \"exp\", \"exp\", \"exp\", \"exp\", \"exp\", \"exp\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expectation\", \"expert\", \"expert\", \"expert\", \"expert\", \"exploration\", \"exploration\", \"exploration\", \"exploration\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"exponential\", \"extract\", \"extract\", \"extract\", \"extract\", \"extract\", \"extract\", \"eye\", \"eye\", \"face\", \"face\", \"face\", \"factorization\", \"factorization\", \"factorization\", \"familiarity\", \"family\", \"family\", \"family\", \"family\", \"family\", \"feedforward\", \"feedforward\", \"feedforward\", \"fidelity\", \"field\", \"field\", \"field\", \"field\", \"field\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"fire\", \"firing\", \"flow\", \"flow\", \"fmri\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"foreground\", \"fragment\", \"fragment\", \"frame\", \"frame\", \"frame\", \"frankwolfe\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"frequency\", \"friend\", \"frobenius\", \"ftl\", \"fuzzy\", \"fuzzy\", \"game\", \"game\", \"gamma\", \"gan\", \"gate\", \"gate\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generalization\", \"generative\", \"generative\", \"generative\", \"generative\", \"generator\", \"generator\", \"generator\", \"generator\", \"geodesic\", \"geometrically\", \"ghahramani\", \"ghahramani\", \"gibbs\", \"gibbs\", \"gin\", \"glm\", \"goodfellow\", \"gpu\", \"grammar\", \"graph\", \"graph\", \"graphical\", \"graphical\", \"graphical\", \"ground\", \"ground\", \"ground\", \"ground\", \"groundtruth\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"haar\", \"ham\", \"ham\", \"hardware\", \"hardware\", \"hash\", \"hedge\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"highdimensional\", \"highdimensional\", \"highdimensional\", \"hinton\", \"hinton\", \"hinton\", \"hinton\", \"hinton\", \"hippocampal\", \"hippocampus\", \"hmm\", \"hmm\", \"hopfield\", \"horizon\", \"hti\", \"human\", \"human\", \"human\", \"hypergraph\", \"hyperparameter\", \"hyperparameter\", \"hyperplane\", \"hypothesis\", \"hypothesis\", \"hypothesis\", \"iccv\", \"iclr\", \"icml\", \"icml\", \"icml\", \"icml\", \"icml\", \"icml\", \"identifiability\", \"illumination\", \"ilya\", \"imagenet\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"inference\", \"inference\", \"inference\", \"inference\", \"inhibition\", \"inhibitory\", \"integral\", \"integral\", \"integral\", \"integral\", \"integral\", \"integrand\", \"invariance\", \"invariance\", \"invariance\", \"invariance\", \"invariant\", \"invariant\", \"invariant\", \"invariant\", \"invariant\", \"invariant\", \"ise\", \"jacobian\", \"jit\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"judgment\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernelbase\", \"kernelbase\", \"kernelbase\", \"kikuchi\", \"kingma\", \"knn\", \"knn\", \"kohonen\", \"krizhevsky\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeler\", \"language\", \"language\", \"language\", \"language\", \"laplace\", \"lasso\", \"latent\", \"latent\", \"latent\", \"latent\", \"lateral\", \"layer\", \"layer\", \"layer\", \"layer\", \"lbp\", \"lda\", \"leaf\", \"learner\", \"learner\", \"lecun\", \"lecun\", \"lecun\", \"lemma\", \"lemma\", \"lemma\", \"lemma\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"length\", \"lift\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linguistic\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"logconcave\", \"logistic\", \"logistic\", \"logistic\", \"logistic\", \"logistic\", \"loglikelihood\", \"loglikelihood\", \"loopy\", \"lowrank\", \"lsh\", \"lstm\", \"magic\", \"mallow\", \"manifold\", \"manifold\", \"manifold\", \"margin\", \"margin\", \"marginal\", \"marginal\", \"marginal\", \"markov\", \"markov\", \"markov\", \"markov\", \"mateo\", \"mcclelland\", \"mcmc\", \"mdp\", \"mdps\", \"mds\", \"membrane\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"message\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"min\", \"min\", \"min\", \"min\", \"min\", \"minibatch\", \"misclassification\", \"mix\", \"mix\", \"mix\", \"mixability\", \"mixedmembership\", \"mixture\", \"mixture\", \"mixture\", \"mmsb\", \"mnist\", \"mnist\", \"mnist\", \"modulation\", \"module\", \"module\", \"monitoring\", \"monte\", \"monte\", \"morgan\", \"morgan\", \"morgan\", \"morgan\", \"morgan\", \"motif\", \"motion\", \"motion\", \"motion\", \"movement\", \"movement\", \"movement\", \"mozer\", \"mozer\", \"mtl\", \"multiarme\", \"multiclass\", \"multilabel\", \"multinomial\", \"music\", \"neal\", \"near\", \"near\", \"near\", \"near\", \"near\", \"near\", \"near\", \"near\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"nesterov\", \"net\", \"net\", \"net\", \"net\", \"neuron\", \"neuron\", \"neuron\", \"neuronal\", \"neuroscience\", \"neuroscience\", \"nips\", \"nips\", \"nips\", \"nips\", \"nips\", \"nips\", \"node\", \"node\", \"nonconvex\", \"nonconvex\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonlinear\", \"nonparametric\", \"nonparametric\", \"nonparametric\", \"nonsmooth\", \"norm\", \"norm\", \"norm\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"normalization\", \"nuclear\", \"object\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"occlusion\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operation\", \"operator\", \"operator\", \"operator\", \"operator\", \"opponent\", \"optimisation\", \"orbit\", \"orientation\", \"orientation\", \"orthonormal\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outcome\", \"outlier\", \"outlier\", \"outlier\", \"pac\", \"pacbayesian\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise\", \"pairwise\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parallel\", \"parent\", \"parse\", \"parse\", \"participant\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"partition\", \"patch\", \"patch\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"pathway\", \"patient\", \"patient\", \"patient\", \"pca\", \"pca\", \"pca\", \"pca\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"permutation\", \"permutation\", \"permutation\", \"permutation\", \"person\", \"person\", \"phase\", \"phase\", \"phase\", \"phase\", \"phase\", \"phone\", \"phoneme\", \"phonetic\", \"pid\", \"pixel\", \"planar\", \"planning\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"player\", \"policy\", \"population\", \"population\", \"population\", \"population\", \"population\", \"pose\", \"pose\", \"pose\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"posterior\", \"posterior\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"predictive\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"preprint\", \"preprint\", \"preprocessor\", \"pretraine\", \"primal\", \"prime\", \"prime\", \"primitive\", \"principal\", \"principal\", \"principal\", \"principal\", \"principal\", \"principal\", \"privacy\", \"private\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"processor\", \"processor\", \"professor\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"programming\", \"projection\", \"projection\", \"projection\", \"projection\", \"pronunciation\", \"propagation\", \"propagation\", \"propagation\", \"propagation\", \"protein\", \"proximal\", \"psd\", \"pulse\", \"pursuit\", \"qij\", \"quadrature\", \"quantization\", \"quantization\", \"quantize\", \"quantizer\", \"query\", \"query\", \"query\", \"query\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"rademacher\", \"rank\", \"rank\", \"rank\", \"rasmussen\", \"rat\", \"rbm\", \"rcnn\", \"realizable\", \"receiver\", \"receptive\", \"receptive\", \"recording\", \"recover\", \"recover\", \"recover\", \"recover\", \"recover\", \"recovery\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"region\", \"registration\", \"regression\", \"regression\", \"regression\", \"regret\", \"regularisation\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"reinforcement\", \"relu\", \"representer\", \"resample\", \"response\", \"response\", \"response\", \"response\", \"response\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"return\", \"reward\", \"riemannian\", \"rigid\", \"risk\", \"risk\", \"rkhs\", \"rkhs\", \"rnn\", \"rnn\", \"rnn\", \"rnp\", \"rod\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"round\", \"round\", \"round\", \"row\", \"row\", \"row\", \"row\", \"row\", \"row\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rule\", \"rumelhart\", \"salakhutdinov\", \"salakhutdinov\", \"salakhutdinov\", \"saliency\", \"sampler\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"scene\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"sdp\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"segment\", \"segment\", \"segment\", \"segment\", \"segment\", \"segmentation\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"semidefinite\", \"semidefinite\", \"semisupervise\", \"semisupervise\", \"sensory\", \"sentence\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"sgd\", \"shape\", \"shape\", \"shape\", \"shape\", \"siam\", \"siam\", \"siam\", \"signature\", \"signature\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"similarity\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"simulation\", \"sin\", \"singular\", \"singular\", \"site\", \"site\", \"site\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"snps\", \"solver\", \"solver\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"spam\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparsity\", \"sparsity\", \"sparsity\", \"spatial\", \"spatial\", \"speaker\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"speech\", \"speech\", \"spike\", \"ssp\", \"stable\", \"stable\", \"stable\", \"stable\", \"stable\", \"stable\", \"stein\", \"stimulus\", \"stock\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"string\", \"string\", \"subexponential\", \"subgaussian\", \"subgradient\", \"subgraph\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"submodular\", \"submodular\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subtree\", \"suffix\", \"sutskever\", \"sutton\", \"svd\", \"svm\", \"svm\", \"svms\", \"syllable\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"symbol\", \"synaptic\", \"synaptic\", \"synchronization\", \"syntactic\", \"synthesize\", \"tagging\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"taxonomy\", \"teacher\", \"teacher\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"tensor\", \"tensor\", \"text\", \"text\", \"text\", \"text\", \"texture\", \"thread\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"threshold\", \"token\", \"topic\", \"topic\", \"touretzky\", \"trajectory\", \"trajectory\", \"transductive\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transformation\", \"transistor\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"tree\", \"tree\", \"tree\", \"tree\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"tsybakov\", \"ucb\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"uncertainty\", \"undirected\", \"unlabeled\", \"unlabeled\", \"user\", \"user\", \"user\", \"utility\", \"utility\", \"utility\", \"utility\", \"utterance\", \"validation\", \"validation\", \"validation\", \"validation\", \"validation\", \"vapnik\", \"variational\", \"variational\", \"variational\", \"vcdimension\", \"vertex\", \"vertex\", \"vertice\", \"vgg\", \"video\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"visual\", \"visual\", \"vlsi\", \"vocabulary\", \"vocabulary\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"vol\", \"voting\", \"vowel\", \"vsi\", \"warp\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"weak\", \"wgan\", \"wire\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"worker\", \"worker\", \"wta\", \"xed\", \"xtl\", \"yoshua\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 5, 2, 4, 1, 8, 9, 7, 6, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el70471342640969149286059718778\", ldavis_el70471342640969149286059718778_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el70471342640969149286059718778\", ldavis_el70471342640969149286059718778_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el70471342640969149286059718778\", ldavis_el70471342640969149286059718778_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run TF-IDF on Bag of Words"
      ],
      "metadata": {
        "id": "XSBbBH0Y-Upy"
      },
      "id": "XSBbBH0Y-Upy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tf-idf model object on ‘bow_corpus’\n",
        "tfidf_model = TfidfModel(bow_corpus)"
      ],
      "metadata": {
        "id": "3B789mYuiz1C"
      },
      "id": "3B789mYuiz1C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply transformation to entire corpus\n",
        "tfidf_corpus = tfidf_model[bow_corpus]"
      ],
      "metadata": {
        "id": "Z6mJbiuY-SWq"
      },
      "id": "Z6mJbiuY-SWq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train LDA Model (TF-IDF Corpus)"
      ],
      "metadata": {
        "id": "g6reEjkj-vF7"
      },
      "id": "g6reEjkj-vF7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with TFIDF corpus\n",
        "lda_model_tfidf = LdaMulticore(corpus=tfidf_corpus, id2word=dictionary, num_topics=10,\n",
        "                               chunksize=10, passes=10, gamma_threshold=0.001,\n",
        "                               per_word_topics=True, workers=multiprocessing.cpu_count(), random_state=42)"
      ],
      "metadata": {
        "id": "cWPAuEfZ-Snp"
      },
      "id": "cWPAuEfZ-Snp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate LDA Model (TF-IDF Corpus)"
      ],
      "metadata": {
        "id": "DbEVAPkI_J0p"
      },
      "id": "DbEVAPkI_J0p"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Compute Coherence Score: c_v\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_text,\n",
        "                                     dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print('LDA TF-IDF Coherence Score (c_v): ', coherence_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHxIXuEI-S_W",
        "outputId": "faca5d6a-7b4c-41d9-f37d-f6cdba2d32f3"
      },
      "id": "JHxIXuEI-S_W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA TF-IDF Coherence Score (c_v):  0.4623956468622895\n",
            "CPU times: user 43.5 s, sys: 194 ms, total: 43.7 s\n",
            "Wall time: 44.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Compute Coherence Score: u_mass\n",
        "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_text,\n",
        "                                     dictionary=dictionary, coherence='u_mass')\n",
        "coherence_score = coherence_model_lda.get_coherence()\n",
        "print('LDA TF-IDF Coherence Score (u_mass): ', coherence_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwnpfWkS-TM_",
        "outputId": "243fd03e-527f-4aa7-805a-f64a3d117b1f"
      },
      "id": "bwnpfWkS-TM_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA TF-IDF Coherence Score (u_mass):  -10.795315632718133\n",
            "CPU times: user 1.78 s, sys: 31.6 ms, total: 1.81 s\n",
            "Wall time: 1.83 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KeyBERT Label Extraction (BOW Corpus)"
      ],
      "metadata": {
        "id": "o1FecDTaWyCi"
      },
      "id": "o1FecDTaWyCi"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "10V2is12XkXt",
        "outputId": "ae6e5db5-ded5-44ff-82f9-1cb502a12944"
      },
      "id": "10V2is12XkXt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.26.4)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (4.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "UEpqBuqq-TfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee58da27-4e92-4b04-95b8-9219d214ddb8"
      },
      "id": "UEpqBuqq-TfE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KeyBERT\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Extracting Labels\n",
        "topic_labels = []\n",
        "for topic_num, topic_str in all_topics:\n",
        "    # Extract words\n",
        "    words = re.findall(r'\"(.*?)\"', topic_str)\n",
        "    joined_words = \" \".join(words)\n",
        "    # Extract keyword with KeyBERT\n",
        "    label = kw_model.extract_keywords(joined_words, keyphrase_ngram_range=(1, 2), top_n=1)\n",
        "\n",
        "    # Decide what to store\n",
        "    if label:\n",
        "        keyword, score = label[0]\n",
        "        topic_labels.append((topic_num, keyword, round(score * 100, 2)))  # percent format\n",
        "    else:\n",
        "        topic_labels.append((topic_num, \"not found\", 0.0))\n",
        "\n",
        "print('Topic Labels:\\n')\n",
        "for topic_num, keyword, percent in topic_labels:\n",
        "    print(f\"Topic {topic_num}: {keyword} ({percent}%)\")"
      ],
      "metadata": {
        "id": "1w7c1UHP-Tx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac059a1-34a0-49e5-97f4-5f1d4975221f"
      },
      "id": "1w7c1UHP-Tx4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic Labels:\n",
            "\n",
            "Topic 0: margin unlabeled (60.44%)\n",
            "Topic 1: detection face (57.34%)\n",
            "Topic 2: topic likelihood (70.87%)\n",
            "Topic 3: spike response (63.29%)\n",
            "Topic 4: convex sparse (70.56%)\n",
            "Topic 5: deep arxiv (56.84%)\n",
            "Topic 6: belief vertex (61.35%)\n",
            "Topic 7: arm strategy (57.55%)\n",
            "Topic 8: architecture trajectory (48.98%)\n",
            "Topic 9: decoder context (58.88%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform BERTopic\n",
        "### Compare BERTopic results to LDA Model (BOW Corpus) results"
      ],
      "metadata": {
        "id": "VHOjgp_twiAL"
      },
      "id": "VHOjgp_twiAL"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQJfaFwzwe6l",
        "outputId": "0cec86e1-fc3f-4af8-e35a-7e4c0c103bb9"
      },
      "id": "uQJfaFwzwe6l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.3)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (3.4.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.1.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.13.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbhTIWCHw5tC",
        "outputId": "c8de32f9-9b31-4446-8502-0d757780be67"
      },
      "id": "JbhTIWCHw5tC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/geopandas/_compat.py:7: DeprecationWarning: The 'shapely.geos' module is deprecated, and will be removed in a future version. All attributes of 'shapely.geos' are available directly from the top-level 'shapely' namespace (since shapely 2.0.0).\n",
            "  import shapely.geos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the BERTopic model\n",
        "topics, probs = topic_model.fit_transform(processed_text.apply(lambda x: ' '.join(x)))"
      ],
      "metadata": {
        "id": "sTxmDtvJxEno"
      },
      "id": "sTxmDtvJxEno",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce topics\n",
        "bertopic_top_N = topic_model.reduce_topics(processed_text.apply(lambda x: ' '.join(x)), nr_topics=6)\n",
        "bertopic_top_N.get_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xwtf65exSdA",
        "outputId": "2bfe33d9-291a-441a-8f9b-33e38c865a95"
      },
      "id": "4xwtf65exSdA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{-1: [('use', 0.0289810236266652),\n",
              "  ('model', 0.027545234542551082),\n",
              "  ('algorithm', 0.0237778795143963),\n",
              "  ('datum', 0.02321750213501457),\n",
              "  ('set', 0.02312467646143305),\n",
              "  ('function', 0.019195629982956444),\n",
              "  ('learn', 0.018672192243484955),\n",
              "  ('problem', 0.018359942301477427),\n",
              "  ('distribution', 0.017414292683146378),\n",
              "  ('result', 0.016928864884554272)],\n",
              " 0: [('use', 0.029237780522055543),\n",
              "  ('algorithm', 0.025822167366303935),\n",
              "  ('function', 0.023374721691471888),\n",
              "  ('model', 0.02319547786811835),\n",
              "  ('set', 0.021707384926221117),\n",
              "  ('learn', 0.020936078592496462),\n",
              "  ('network', 0.020311158500494458),\n",
              "  ('problem', 0.017678243798680555),\n",
              "  ('method', 0.017461537799667214),\n",
              "  ('result', 0.016914761948985294)],\n",
              " 1: [('neuron', 0.04291756943407936),\n",
              "  ('model', 0.03848672360383348),\n",
              "  ('spike', 0.03157382621573059),\n",
              "  ('cell', 0.024182945950691613),\n",
              "  ('neural', 0.022228392117500873),\n",
              "  ('stimulus', 0.022074571456401897),\n",
              "  ('input', 0.021637242669441496),\n",
              "  ('use', 0.021137764301853382),\n",
              "  ('time', 0.020919322126313332),\n",
              "  ('network', 0.020844899819180703)],\n",
              " 2: [('model', 0.047545064376024876),\n",
              "  ('word', 0.03488073429470927),\n",
              "  ('use', 0.033122652435754005),\n",
              "  ('training', 0.022918755806649753),\n",
              "  ('network', 0.020964435960683262),\n",
              "  ('speech', 0.02082608654398642),\n",
              "  ('sentence', 0.019769660853102385),\n",
              "  ('distribution', 0.019169453761735886),\n",
              "  ('learn', 0.018165129434779853),\n",
              "  ('topic', 0.018130877969312283)],\n",
              " 3: [('protein', 0.06262337381024763),\n",
              "  ('model', 0.02888897932196408),\n",
              "  ('site', 0.02721311874622699),\n",
              "  ('sequence', 0.02547413053872153),\n",
              "  ('use', 0.02361535339305253),\n",
              "  ('structure', 0.02294046174612376),\n",
              "  ('datum', 0.022210448214910406),\n",
              "  ('gene', 0.02162726169279104),\n",
              "  ('acid', 0.020273525809237216),\n",
              "  ('motif', 0.02008019854509778)],\n",
              " 4: [('model', 0.03771181437008033),\n",
              "  ('use', 0.03105344059073433),\n",
              "  ('patient', 0.0306188461023225),\n",
              "  ('datum', 0.027769835236204334),\n",
              "  ('variable', 0.026395832357162446),\n",
              "  ('set', 0.02536937559619773),\n",
              "  ('test', 0.023311502753651632),\n",
              "  ('algorithm', 0.022648614543995388),\n",
              "  ('causal', 0.022490919216258714),\n",
              "  ('miss', 0.020598009240166568)]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize word bar chart for reduced topics\n",
        "bertopic_top_N.visualize_barchart(top_n_topics=6, n_words=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "0lSOAMjtzIrO",
        "outputId": "96530e9a-2acc-4f38-ab02-30080b27e1c9"
      },
      "id": "0lSOAMjtzIrO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6ab46737-fa98-4adb-97c7-5e8103780b77\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6ab46737-fa98-4adb-97c7-5e8103780b77\")) {                    Plotly.newPlot(                        \"6ab46737-fa98-4adb-97c7-5e8103780b77\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.021707384926221117,0.02319547786811835,0.023374721691471888,0.025822167366303935,0.029237780522055543],\"y\":[\"set  \",\"model  \",\"function  \",\"algorithm  \",\"use  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.022228392117500873,0.024182945950691613,0.03157382621573059,0.03848672360383348,0.04291756943407936],\"y\":[\"neural  \",\"cell  \",\"spike  \",\"model  \",\"neuron  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.020964435960683262,0.022918755806649753,0.033122652435754005,0.03488073429470927,0.047545064376024876],\"y\":[\"network  \",\"training  \",\"use  \",\"word  \",\"model  \"],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.02361535339305253,0.02547413053872153,0.02721311874622699,0.02888897932196408,0.06262337381024763],\"y\":[\"use  \",\"sequence  \",\"site  \",\"model  \",\"protein  \"],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.026395832357162446,0.027769835236204334,0.0306188461023225,0.03105344059073433,0.03771181437008033],\"y\":[\"variable  \",\"datum  \",\"patient  \",\"use  \",\"model  \"],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.6000000000000001,1.0],\"showgrid\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.0,0.4],\"showgrid\":true},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.0,0.4],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 0\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 1\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 2\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 3\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 4\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"Topic Word Scores\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6ab46737-fa98-4adb-97c7-5e8103780b77');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DHpr9lUwzYu3"
      },
      "id": "DHpr9lUwzYu3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}